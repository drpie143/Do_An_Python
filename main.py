"""
Script chÃ­nh Ä‘á»ƒ cháº¡y toÃ n bá»™ pipeline:
1. Download vÃ  load dá»¯ liá»‡u
2. Tiá»n xá»­ lÃ½ dá»¯ liá»‡u (DataPreprocessor)
3. Training vÃ  tá»‘i Æ°u mÃ´ hÃ¬nh (ModelTrainer)
4. ÄÃ¡nh giÃ¡ vÃ  visualization
5. LÆ°u mÃ´ hÃ¬nh vÃ  káº¿t quáº£

CÃ¡ch cháº¡y:
    python main.py
    
    hoáº·c vá»›i tÃ¹y chá»n:
    python main.py --optimize    # Cháº¡y optimization vá»›i Optuna
    python main.py --no-viz      # KhÃ´ng váº½ biá»ƒu Ä‘á»“
"""

import argparse
import importlib
import json
import logging
import sys
import subprocess
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split

# Import tá»« project
from src.preprocessing.data_preprocessor import DataPreprocessor
from src.modeling.model_trainer import ModelTrainer
import config


# Cáº¥u hÃ¬nh logging (ghi Ä‘Ã¨ file log má»—i láº§n cháº¡y & format rÃµ rÃ ng)
config.LOG_FILE.parent.mkdir(parents=True, exist_ok=True)
LOG_FORMAT = "%(asctime)s | %(levelname)-8s | %(name)s | %(message)s"
CONSOLE_FORMAT = "%(levelname)-8s | %(message)s"
DATE_FORMAT = "%Y-%m-%d %H:%M:%S"

file_handler = logging.FileHandler(config.LOG_FILE, mode='w', encoding='utf-8')
file_handler.setFormatter(logging.Formatter(LOG_FORMAT, DATE_FORMAT))

console_handler = logging.StreamHandler(sys.stdout)
console_handler.setFormatter(logging.Formatter(CONSOLE_FORMAT, DATE_FORMAT))

logging.basicConfig(
    level=getattr(logging, config.LOG_LEVEL),
    handlers=[file_handler, console_handler],
    force=True,
)
logger = logging.getLogger(__name__)


def _log_divider(char: str = "=", width: int = 70) -> None:
    """Log a horizontal divider with consistent width."""
    logger.info(char * width)


def log_section(title: str, icon: str = "ğŸš€") -> None:
    """Render a bold section banner combining the legacy and sample styles."""
    logger.info("\n")
    _log_divider()
    logger.info("%s %s", icon, title.upper())
    _log_divider()
    logger.info("")


def log_step(title: str, icon: str = "ğŸ”¹") -> None:
    """Highlight a sub-step within a section."""
    logger.info("%s %s", icon, title)


def log_key_value(label: str, value) -> None:
    """Align key-value summary rows for faster scanning."""
    logger.info("   %-18s: %s", label, value)


def log_stage_summary(stage_times: List[Tuple[str, float]], total_duration: float) -> None:
    """Pretty print per-stage durations and pipeline total."""
    logger.info("â±ï¸ PIPELINE SUMMARY")
    for stage, duration in stage_times:
        log_key_value(stage.capitalize(), f"{duration:.2f} giÃ¢y")
    log_key_value("Total", f"{total_duration:.2f} giÃ¢y")


def _ensure_gdown() -> Optional[object]:
    """Ensure gdown is importable without forcing installs every run."""
    try:
        return importlib.import_module("gdown")
    except ImportError:
        logger.info("gdown chÆ°a Ä‘Æ°á»£c cÃ i. Äang tiáº¿n hÃ nh cÃ i Ä‘áº·t má»™t láº§n...")
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "gdown"])
        except subprocess.CalledProcessError as exc:
            logger.warning("KhÃ´ng thá»ƒ cÃ i Ä‘áº·t gdown tá»± Ä‘á»™ng: %s", exc)
            return None
        try:
            return importlib.import_module("gdown")
        except ImportError:
            return None


def download_data():
    """Download dá»¯ liá»‡u tá»« Google Drive náº¿u chÆ°a cÃ³."""
    if config.DATA_FILE.exists():
        log_step(f"Dá»¯ liá»‡u Ä‘Ã£ tá»“n táº¡i: {config.DATA_FILE}", icon="âœ…")
        return

    log_step("Äang download dá»¯ liá»‡u tá»« Google Drive (náº¿u kháº£ dá»¥ng)...", icon="ğŸ“¥")
    gdown = _ensure_gdown()
    if gdown is None:
        logger.warning("KhÃ´ng thá»ƒ import gdown. Vui lÃ²ng táº£i thá»§ cÃ´ng file taxi_price.csv vÃ  Ä‘áº·t vÃ o %s", config.DATA_DIR)
        if not config.DATA_FILE.exists():
            raise FileNotFoundError(
                "KhÃ´ng táº£i Ä‘Æ°á»£c dá»¯ liá»‡u tá»± Ä‘á»™ng vÃ¬ thiáº¿u gdown. HÃ£y Ä‘áº·t file taxi_price.csv vÃ o thÆ° má»¥c data vÃ  cháº¡y láº¡i."
            )
        return

    try:
        gdown.download(id=config.GDRIVE_FILE_ID, output=str(config.DATA_FILE), quiet=False)
        log_step(f"ÄÃ£ download dá»¯ liá»‡u vÃ o: {config.DATA_FILE}", icon="âœ…")
    except Exception as exc:
        logger.error("âŒ Lá»—i khi download dá»¯ liá»‡u: %s", exc)
        logger.info("ğŸ’¡ Vui lÃ²ng download thá»§ cÃ´ng vÃ  Ä‘áº·t vÃ o thÆ° má»¥c data/")

    if not config.DATA_FILE.exists():
        raise FileNotFoundError(
            "KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u taxi_price.csv. Download tá»± Ä‘á»™ng tháº¥t báº¡i, vui lÃ²ng táº£i thá»§ cÃ´ng vÃ  cháº¡y láº¡i."
        )


def preprocess_data(
    generate_viz: bool = True,
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, Optional[List[str]], DataPreprocessor]:
    """
    Load raw data, clean it, and return processed train/test splits.
    
    Quy trÃ¬nh chuáº©n:
    1. PRE-SPLIT: Xá»­ lÃ½ nháº¹ (khÃ´ng leak thÃ´ng tin thá»‘ng kÃª)
       - XÃ³a duplicates
       - Unify values (lowercase, strip)  
       - Feature engineering (táº¡o Speed_kmh)
       - Apply constraints cÆ¡ báº£n (dtype, clip)
    
    2. CHIA TRAIN/TEST
    
    3. POST-SPLIT trÃªn TRAIN (fit):
       - Remove outliers (chá»‰ trÃªn train)
       - Fill missing (fit trÃªn train)
       - Encode categorical (fit trÃªn train)
       - Scale features (fit trÃªn train)
       - Interaction features
    
    4. TRANSFORM TEST (dÃ¹ng params tá»« train)
    """
    log_section("BÆ¯á»šC 1: TIá»€N Xá»¬ LÃ Dá»® LIá»†U", icon="ğŸ§¼")

    if not config.DATA_FILE.exists():
        raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file dá»¯ liá»‡u: {config.DATA_FILE}")

    # ========================================================================
    # PHASE 1: LOAD & PRE-SPLIT CLEANING (khÃ´ng leak thÃ´ng tin thá»‘ng kÃª)
    # ========================================================================
    log_step("Äang náº¡p dá»¯ liá»‡u gá»‘c", icon="ğŸ“¥")
    raw_df = pd.read_csv(config.DATA_FILE)
    log_key_value("Raw shape", raw_df.shape)
    
    # 1.1 XÃ³a duplicates
    n_duplicates = int(raw_df.duplicated().sum())
    if n_duplicates > 0:
        raw_df = raw_df.drop_duplicates().reset_index(drop=True)
        log_step(f"ÄÃ£ xÃ³a {n_duplicates} dÃ²ng trÃ¹ng láº·p", icon="ğŸ—‘ï¸")
    log_key_value("After drop duplicates", raw_df.shape)

    # Hiá»ƒn thá»‹ missing values ban Ä‘áº§u
    missing_df = (
        raw_df.isna()
        .sum()
        .reset_index()
        .rename(columns={"index": "column", 0: "missing"})
    )
    if len(raw_df):
        missing_df["missing_pct"] = (missing_df["missing"] / len(raw_df) * 100).round(2)
    missing_df = missing_df[missing_df["missing"] > 0]
    if not missing_df.empty:
        logger.info("âš ï¸  Missing Values:\n%s", missing_df.to_string(index=False))

    # 1.2 Unify values (lowercase, strip) - khÃ´ng leak thÃ´ng tin
    log_step("Thá»‘ng nháº¥t text (lowercase, strip)", icon="ğŸ”¤")
    cat_cols = raw_df.select_dtypes(include=['object']).columns.tolist()
    for col in cat_cols:
        raw_df[col] = raw_df[col].astype(str).str.lower().str.strip()
    
    # 1.3 Feature engineering (táº¡o Speed_kmh) - khÃ´ng leak thÃ´ng tin
    if config.SPEED_FEATURE.get("enabled", False):
        log_step(f"Táº¡o feature {config.SPEED_FEATURE['name']}", icon="ğŸï¸")
        if {"Trip_Distance_km", "Trip_Duration_Minutes"}.issubset(raw_df.columns):
            duration_hours = raw_df["Trip_Duration_Minutes"] / 60
            raw_df["Speed_kmh"] = np.where(
                duration_hours > 0, 
                raw_df["Trip_Distance_km"] / duration_hours, 
                0
            ).round(2)
            # Loáº¡i bá» cÃ¡c dÃ²ng cÃ³ tá»‘c Ä‘á»™ báº¥t há»£p lÃ½
            speed_rule = config.CONSTRAINT_RULES.get("Speed_kmh", {})
            min_speed = speed_rule.get("min", 0.0)
            max_speed = speed_rule.get("max", 160.0)
            mask_invalid = (raw_df["Speed_kmh"] < min_speed) | (raw_df["Speed_kmh"] > max_speed)
            if mask_invalid.any():
                raw_df = raw_df.loc[~mask_invalid].reset_index(drop=True)
                log_step(f"ÄÃ£ xÃ³a {int(mask_invalid.sum())} dÃ²ng cÃ³ Speed_kmh ngoÃ i [{min_speed}, {max_speed}]", icon="âš ï¸")

    # 1.4 Apply constraints cÆ¡ báº£n (chá»‰ clip, khÃ´ng dÃ¹ng thá»‘ng kÃª)
    log_step("Ãp dá»¥ng rÃ ng buá»™c cÆ¡ báº£n (clip giÃ¡ trá»‹)", icon="ğŸ“")
    for col, rule in config.CONSTRAINT_RULES.items():
        if col not in raw_df.columns:
            continue
        if not isinstance(rule, dict):
            continue
        
        # Chá»‰ Ã¡p dá»¥ng clip (khÃ´ng dÃ¹ng mean/median vÃ¬ sáº½ leak)
        action = rule.get("action", "drop")
        min_val = rule.get("min", None)
        max_val = rule.get("max", None)
        
        if action == "clip" and (min_val is not None or max_val is not None):
            raw_df[col] = raw_df[col].clip(lower=min_val, upper=max_val)
            logger.info("   â€¢ %s: clip to [%s, %s]", col, min_val, max_val)
        elif action == "drop":
            # XÃ³a dÃ²ng vi pháº¡m (khÃ´ng dÃ¹ng thá»‘ng kÃª)
            mask = pd.Series(True, index=raw_df.index)
            if min_val is not None:
                mask &= raw_df[col] >= min_val
            if max_val is not None:
                mask &= raw_df[col] <= max_val
            n_dropped = int((~mask).sum())
            if n_dropped > 0:
                raw_df = raw_df.loc[mask].reset_index(drop=True)
                logger.info("   â€¢ %s: dropped %s rows outside [%s, %s]", col, n_dropped, min_val, max_val)

    log_key_value("After pre-split cleaning", raw_df.shape)

    # EDA trÃªn dá»¯ liá»‡u Ä‘Ã£ lÃ m sáº¡ch nháº¹
    if generate_viz:
        log_step("Äang táº¡o cÃ¡c biá»ƒu Ä‘á»“ EDA (lÆ°u táº¡i results/eda)", icon="ğŸ–¼ï¸")
        overview_preprocessor = DataPreprocessor(data=raw_df.copy())
        overview_preprocessor.generate_eda_report(target_col=config.TARGET_COLUMN)

    # ========================================================================
    # PHASE 2: CHIA TRAIN/TEST
    # ========================================================================
    log_step("Chia train/test", icon="âœ‚ï¸")
    train_df, test_df = train_test_split(
        raw_df,
        test_size=config.TEST_SIZE,
        random_state=config.RANDOM_SEED,
        shuffle=True,
    )
    train_df = train_df.reset_index(drop=True)
    test_df = test_df.reset_index(drop=True)
    log_key_value("Train shape", train_df.shape)
    log_key_value("Test shape", test_df.shape)

    # ========================================================================
    # PHASE 3: POST-SPLIT PROCESSING trÃªn TRAIN (fit)
    # Thá»© tá»±: Outliers â†’ Fill Missing â†’ Encode â†’ Scale â†’ Interaction
    # ========================================================================
    log_step("Báº¯t Ä‘áº§u xá»­ lÃ½ TRAIN set (fit)", icon="ğŸ”§")
    
    # Khá»Ÿi táº¡o preprocessor cho train
    train_preprocessor = DataPreprocessor(
        data=train_df.copy(),
        missing_strategy=config.MISSING_STRATEGY["numeric"],
        categorical_missing_strategy=config.MISSING_STRATEGY["categorical"],
        scaler_type=config.SCALING_METHOD,
        encoder_type=config.ENCODING_METHOD,
    )

    # 3.1 Remove outliers (CHá»ˆ trÃªn train, TRÆ¯á»šC fill missing)
    if config.OUTLIER_DETECTION:
        log_step("Loáº¡i bá» outliers (chá»‰ trÃªn train)", icon="ğŸ“Š")
        train_preprocessor.remove_outliers(
            method=config.OUTLIER_METHOD,
            threshold=config.OUTLIER_THRESHOLD
        )
        log_key_value("Train after outliers", train_preprocessor.data.shape)

    # 3.2 Fill missing values (fit trÃªn train)
    log_step("Xá»­ lÃ½ missing values (fit trÃªn train)", icon="ğŸ©¹")
    train_preprocessor.handle_missing(
        strategy='auto',
        numeric_strategy=config.MISSING_STRATEGY['numeric'],
        categorical_strategy=config.MISSING_STRATEGY['categorical']
    )

    # 3.3 Encode categorical (fit trÃªn train)
    log_step("MÃ£ hÃ³a biáº¿n phÃ¢n loáº¡i (fit trÃªn train)", icon="ğŸ”¤")
    train_preprocessor.encode_categorical(
        method=config.ENCODING_METHOD,
        drop_first=config.DROP_FIRST_ONEHOT
    )

    # 3.4 Scale features (fit trÃªn train)
    # KHÃ”NG scale cÃ¡c cá»™t OneHot (binary 0/1) - chá»‰ scale numeric gá»‘c
    log_step("Chuáº©n hÃ³a features (fit trÃªn train)", icon="ğŸ“")
    
    # Láº¥y danh sÃ¡ch cá»™t OneHot Ä‘á»ƒ exclude
    onehot_cols = [col for col in train_preprocessor.data.columns 
                   if any(cat in col for cat in config.CATEGORICAL_COLS)]
    exclude_from_scale = [config.TARGET_COLUMN] + onehot_cols
    
    train_preprocessor.scale_features(
        method=config.SCALING_METHOD,
        exclude_columns=exclude_from_scale
    )

    # 3.5 Táº¡o interaction features
    if config.CREATE_INTERACTION_FEATURES:
        log_step("Táº¡o interaction features", icon="â•")
        train_preprocessor.create_interaction_features(
            col_pairs=config.INTERACTION_PAIRS,
            operations=['multiply']
        )

    # Váº½ heatmap tÆ°Æ¡ng quan trÃªn train Ä‘Ã£ xá»­ lÃ½
    heatmap_path = config.EDA_RESULTS_DIR / 'correlation_heatmap_train.png'
    corr_df = train_preprocessor.plot_correlation_heatmap(
        target_col=config.TARGET_COLUMN,
        method='spearman',
        save_path=heatmap_path,
        annot=True,
        show=False
    )
    log_step(f"Heatmap tÆ°Æ¡ng quan (train) Ä‘Ã£ lÆ°u táº¡i: {heatmap_path}", icon="ğŸ“Œ")

    # Chá»n features cho Polynomial Regression
    poly_feature_subset: Optional[List[str]] = None
    if corr_df is not None and config.TARGET_COLUMN in corr_df.columns:
        corr_series = corr_df[config.TARGET_COLUMN].drop(labels=[config.TARGET_COLUMN])
        selected = corr_series[abs(corr_series) >= config.POLY_CORRELATION_THRESHOLD]
        if not selected.empty:
            poly_feature_subset = selected.index.tolist()
            log_step(
                f"{len(poly_feature_subset)} feature cÃ³ |corr| >= {config.POLY_CORRELATION_THRESHOLD}: {poly_feature_subset}",
                icon="ğŸ¯"
            )
        else:
            logger.warning(
                "âš ï¸  KhÃ´ng cÃ³ feature nÃ o Ä‘áº¡t ngÆ°á»¡ng |corr| >= %s. Sá»­ dá»¥ng toÃ n bá»™ features cho Polynomial.",
                config.POLY_CORRELATION_THRESHOLD,
            )

    train_preprocessor.print_summary()
    train_preprocessor.mark_as_fitted()

    # ========================================================================
    # PHASE 4: TRANSFORM TEST (dÃ¹ng params tá»« train)
    # ========================================================================
    log_step("Transform TEST set (dÃ¹ng params tá»« train)", icon="ğŸ”„")
    train_processed = train_preprocessor.get_processed_data()
    test_processed = train_preprocessor.transform_new_data(test_df)
    log_key_value("Train processed shape", train_processed.shape)
    log_key_value("Test processed shape", test_processed.shape)

    # ========================================================================
    # Táº O DATA UNSCALED CHO TREE-BASED MODELS (RF, XGBoost)
    # Tree-based models khÃ´ng cáº§n scale, scale cÃ³ thá»ƒ lÃ m giáº£m performance
    # ========================================================================
    log_step("Táº¡o data unscaled cho Tree-based models", icon="ğŸŒ²")
    
    # Táº¡o preprocessor má»›i KHÃ”NG scale
    train_preprocessor_unscaled = DataPreprocessor(
        data=train_df.copy(),
        missing_strategy=config.MISSING_STRATEGY["numeric"],
        categorical_missing_strategy=config.MISSING_STRATEGY["categorical"],
        scaler_type=config.SCALING_METHOD,
        encoder_type=config.ENCODING_METHOD,
    )
    # Fill missing
    train_preprocessor_unscaled.handle_missing(
        strategy='auto',
        numeric_strategy=config.MISSING_STRATEGY['numeric'],
        categorical_strategy=config.MISSING_STRATEGY['categorical']
    )
    # Encode categorical
    train_preprocessor_unscaled.encode_categorical(
        method=config.ENCODING_METHOD,
        drop_first=config.DROP_FIRST_ONEHOT
    )
    # KHÃ”NG SCALE - giá»¯ nguyÃªn giÃ¡ trá»‹ gá»‘c
    train_preprocessor_unscaled.mark_as_fitted()
    
    train_unscaled = train_preprocessor_unscaled.get_processed_data()
    test_unscaled = train_preprocessor_unscaled.transform_new_data(test_df)
    
    X_train_unscaled = train_unscaled.drop(columns=[config.TARGET_COLUMN])
    y_train_unscaled = train_unscaled[config.TARGET_COLUMN]
    X_test_unscaled = test_unscaled.drop(columns=[config.TARGET_COLUMN])
    y_test_unscaled = test_unscaled[config.TARGET_COLUMN]
    log_key_value("Unscaled train shape", X_train_unscaled.shape)

    # LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
    combined = pd.concat(
        [
            train_processed.assign(split='train'),
            test_processed.assign(split='test')
        ],
        ignore_index=True,
    )
    combined.to_csv(config.PROCESSED_DATA_FILE, index=False)
    log_step(f"ÄÃ£ lÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ táº¡i: {config.PROCESSED_DATA_FILE}", icon="ğŸ’¾")

    # TÃ¡ch X vÃ  y (scaled - cho Polynomial)
    X_train = train_processed.drop(columns=[config.TARGET_COLUMN])
    y_train = train_processed[config.TARGET_COLUMN]
    X_test = test_processed.drop(columns=[config.TARGET_COLUMN])
    y_test = test_processed[config.TARGET_COLUMN]

    return (X_train, X_test, y_train, y_test, 
            X_train_unscaled, X_test_unscaled, y_train_unscaled, y_test_unscaled,
            poly_feature_subset, train_preprocessor)


def train_models(
    X_train: pd.DataFrame,
    X_test: pd.DataFrame,
    y_train: pd.Series,
    y_test: pd.Series,
    X_train_unscaled: pd.DataFrame,
    X_test_unscaled: pd.DataFrame,
    y_train_unscaled: pd.Series,
    y_test_unscaled: pd.Series,
    optimize: bool = False,
    poly_feature_subset: Optional[List[str]] = None,
):
    """Huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh há»c mÃ¡y vá»›i dá»¯ liá»‡u Ä‘Ã£ split."""
    log_section("BÆ¯á»šC 2: HUáº¤N LUYá»†N MÃ” HÃŒNH", icon="ğŸ¤–")
    
    # Khá»Ÿi táº¡o trainer vá»›i data SCALED (cho Polynomial)
    trainer = ModelTrainer(
        X_train=X_train,
        X_test=X_test,
        y_train=y_train,
        y_test=y_test,
        output_dir=str(config.MODELS_DIR)
    )
    
    # Khá»Ÿi táº¡o trainer vá»›i data UNSCALED (cho RF, XGBoost)
    trainer_unscaled = ModelTrainer(
        X_train=X_train_unscaled,
        X_test=X_test_unscaled,
        y_train=y_train_unscaled,
        y_test=y_test_unscaled,
        output_dir=str(config.MODELS_DIR)
    )
    
    log_step(f"Data scaled: {trainer.data_info}", icon="ğŸ“Š")
    log_step(f"Data unscaled: {trainer_unscaled.data_info}", icon="ğŸŒ²")
    
    # ========== POLYNOMIAL REGRESSION (dÃ¹ng data SCALED) ==========
    if optimize:
        log_step("Tá»‘i Æ°u Polynomial Regression", icon="ğŸ”")
        best_poly_params = trainer.optimize_polynomial(
            n_trials=config.OPTUNA_N_TRIALS['polynomial'],
            timeout=config.OPTUNA_TIMEOUT['polynomial']
        )
        trainer.train_polynomial(
            degree=best_poly_params.get('degree', config.DEFAULT_HYPERPARAMS['polynomial']['degree']),
            alpha=best_poly_params.get('alpha', config.DEFAULT_HYPERPARAMS['polynomial']['alpha']),
            feature_subset=poly_feature_subset
        )
    else:
        default_poly = config.DEFAULT_HYPERPARAMS['polynomial']
        trainer.train_polynomial(
            degree=default_poly['degree'],
            alpha=default_poly['alpha'],
            feature_subset=poly_feature_subset
        )
    
    # ========== RANDOM FOREST (dÃ¹ng data UNSCALED) ==========
    if optimize:
        log_step("Tá»‘i Æ°u Random Forest (unscaled data)", icon="ğŸ”")
        best_rf_params = trainer_unscaled.optimize_rf(
            n_trials=config.OPTUNA_N_TRIALS['random_forest'],
            timeout=config.OPTUNA_TIMEOUT['random_forest']
        )
        trainer_unscaled.train_rf(**best_rf_params)
    else:
        trainer_unscaled.train_rf(**config.DEFAULT_HYPERPARAMS['random_forest'])
    
    # ========== EXTRA TREES (dÃ¹ng data UNSCALED) ==========
    if optimize:
        log_step("Tá»‘i Æ°u Extra Trees (unscaled data)", icon="ğŸ”")
        best_et_params = trainer_unscaled.optimize_extra_trees(
            n_trials=config.OPTUNA_N_TRIALS['extra_trees'],
            timeout=config.OPTUNA_TIMEOUT['extra_trees']
        )
        trainer_unscaled.train_extra_trees(**best_et_params)
    else:
        trainer_unscaled.train_extra_trees(**config.DEFAULT_HYPERPARAMS['extra_trees'])
    
    # ========== XGBOOST (dÃ¹ng data UNSCALED) ==========
    if optimize:
        log_step("Tá»‘i Æ°u XGBoost (unscaled data)", icon="ğŸ”")
        best_xgb_params = trainer_unscaled.optimize_xgb(
            n_trials=config.OPTUNA_N_TRIALS['xgboost'],
            timeout=config.OPTUNA_TIMEOUT['xgboost']
        )
        trainer_unscaled.train_xgb(**best_xgb_params)
    else:
        trainer_unscaled.train_xgb(**config.DEFAULT_HYPERPARAMS['xgboost'])
    
    # Merge results tá»« cáº£ 2 trainers
    trainer.models.update(trainer_unscaled.models)
    trainer.results.update(trainer_unscaled.results)
    
    return trainer


def evaluate_and_visualize(trainer: ModelTrainer, preprocessor: DataPreprocessor, visualize: bool = True) -> None:
    """
    ÄÃ¡nh giÃ¡ vÃ  visualization káº¿t quáº£.
    
    Args:
        trainer: ModelTrainer instance
        visualize: CÃ³ váº½ biá»ƒu Ä‘á»“ khÃ´ng
    """
    log_section("BÆ¯á»šC 3: ÄÃNH GIÃ & VISUALIZATION", icon="ğŸ“Š")
    
    # In tÃ³m táº¯t káº¿t quáº£
    trainer.summary()
    
    # LÆ°u káº¿t quáº£
    trainer.save_results(config.RESULTS_FILE)
    
    # LÆ°u toÃ n bá»™ mÃ´ hÃ¬nh vÃ  cáº¥u hÃ¬nh tiá»n xá»­ lÃ½
    saved_model_paths = trainer.save_all_models(format=config.MODEL_FORMAT)
    preprocessor_filename = f"data_preprocessor_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib"
    preprocessor_path = config.MODELS_DIR / preprocessor_filename
    preprocessor.save_state(preprocessor_path)
    save_pipeline_metadata(preprocessor_path, saved_model_paths, trainer)
    
    if visualize:
        # Váº½ biá»ƒu Ä‘á»“ so sÃ¡nh
        log_step("Váº½ biá»ƒu Ä‘á»“ so sÃ¡nh", icon="ğŸ“ˆ")
        trainer.plot_comparison(metric='test_r2', save=True)
        trainer.plot_comparison(metric='test_rmse', save=True)
        trainer.plot_comparison(metric='test_mae', save=True)
        
        # Váº½ biá»ƒu Ä‘á»“ predictions
        log_step("Váº½ biá»ƒu Ä‘á»“ predictions", icon="ğŸ“ˆ")
        trainer.plot_all_predictions(save=True)
        
        # Váº½ feature importance
        log_step("Váº½ biá»ƒu Ä‘á»“ feature importance", icon="ğŸ“ˆ")
        trainer.plot_all_feature_importance(top_n=15, save=True)
        
        # So sÃ¡nh feature importance
        log_step("So sÃ¡nh feature importance", icon="ğŸ“ˆ")
        trainer.compare_feature_importance(top_n=10, save=True)
    
    # TÃ¬m mÃ´ hÃ¬nh tá»‘t nháº¥t
    best_name, best_result = trainer.get_best_model()
    
    log_section("MÃ” HÃŒNH Tá»T NHáº¤T", icon="âœ¨")
    log_key_value("Model", best_name.upper())
    log_key_value("Test RÂ²", f"{best_result['test_r2']:.6f}")
    log_key_value("Test RMSE", f"{best_result['test_rmse']:.6f}")
    log_key_value("Test MAE", f"{best_result['test_mae']:.6f}")


def save_pipeline_metadata(preprocessor_path: Path, model_paths: Dict[str, str], trainer: ModelTrainer) -> Path:
    """Ghi láº¡i tráº¡ng thÃ¡i pipeline Ä‘á»ƒ phá»¥c vá»¥ inference sau nÃ y."""
    best_name, best_result = trainer.get_best_model()
    metadata = {
        "generated_at": datetime.utcnow().isoformat() + "Z",
        "preprocessor": {
            "path": str(preprocessor_path.resolve()),
            "target_column": config.TARGET_COLUMN,
            "n_features": trainer.data_info["n_features"],
        },
        "models": {},
        "best_model": best_name,
        "best_model_path": model_paths.get(best_name),
    }
    for model_name, result in trainer.results.items():
        metadata["models"][model_name] = {
            "path": model_paths.get(model_name),
            "metrics": {
                "train_rmse": float(result["train_rmse"]),
                "test_rmse": float(result["test_rmse"]),
                "test_mae": float(result["test_mae"]),
                "test_r2": float(result["test_r2"]),
            },
            "hyperparams": result["hyperparams"],
        }
    if best_name and best_result:
        metadata["best_metrics"] = {
            "test_rmse": float(best_result["test_rmse"]),
            "test_mae": float(best_result["test_mae"]),
            "test_r2": float(best_result["test_r2"]),
        }
    path = config.RESULTS_DIR / "pipeline_state.json"
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as fp:
        json.dump(metadata, fp, indent=4, ensure_ascii=False)
    logger.info("âœ… ÄÃ£ lÆ°u pipeline metadata: %s", path)
    return path


def main():
    """Main function Ä‘á»ƒ cháº¡y toÃ n bá»™ pipeline."""
    # Parse arguments
    parser = argparse.ArgumentParser(description='Taxi Price Prediction Pipeline')
    parser.add_argument('--optimize', action='store_true', 
                       help='Cháº¡y optimization vá»›i Optuna')
    parser.add_argument('--no-viz', action='store_true',
                       help='KhÃ´ng váº½ biá»ƒu Ä‘á»“ visualization')
    parser.add_argument('--skip-download', action='store_true',
                       help='Bá» qua bÆ°á»›c download dá»¯ liá»‡u')
    
    args = parser.parse_args()
    
    log_section("Báº®T Äáº¦U PIPELINE Dá»° ÄOÃN GIÃ TAXI", icon="ğŸš€")
    
    stage_times = []
    pipeline_start = time.perf_counter()

    try:
        # BÆ°á»›c 0: Download dá»¯ liá»‡u
        if not args.skip_download:
            step_start = time.perf_counter()
            download_data()
            stage_times.append(("download", time.perf_counter() - step_start))
        
        # BÆ°á»›c 1: Tiá»n xá»­ lÃ½
        step_start = time.perf_counter()
        (X_train, X_test, y_train, y_test, 
         X_train_unscaled, X_test_unscaled, y_train_unscaled, y_test_unscaled,
         poly_feature_subset, preprocessor) = preprocess_data(generate_viz=not args.no_viz)
        stage_times.append(("preprocess", time.perf_counter() - step_start))
        
        # BÆ°á»›c 2: Training
        step_start = time.perf_counter()
        trainer = train_models(
            X_train,
            X_test,
            y_train,
            y_test,
            X_train_unscaled,
            X_test_unscaled,
            y_train_unscaled,
            y_test_unscaled,
            optimize=args.optimize,
            poly_feature_subset=poly_feature_subset
        )
        stage_times.append(("train", time.perf_counter() - step_start))
        
        # BÆ°á»›c 3: ÄÃ¡nh giÃ¡ vÃ  visualization
        step_start = time.perf_counter()
        evaluate_and_visualize(trainer, preprocessor, visualize=not args.no_viz)
        stage_times.append(("evaluate", time.perf_counter() - step_start))
        
        log_section("HOÃ€N Táº¤T PIPELINE", icon="âœ…")
        log_step(f"MÃ´ hÃ¬nh Ä‘Ã£ lÆ°u táº¡i: {config.MODELS_DIR}", icon="ğŸ“")
        log_step(f"Káº¿t quáº£ Ä‘Ã£ lÆ°u táº¡i: {config.RESULTS_DIR}", icon="ğŸ“")

        total_duration = time.perf_counter() - pipeline_start
        logger.info("")
        log_stage_summary(stage_times, total_duration)
        
    except Exception as e:
        logger.error(f"\nâŒ Lá»–I: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
