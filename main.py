"""
Script ch√≠nh ƒë·ªÉ ch·∫°y to√†n b·ªô pipeline:
1. Download v√† load d·ªØ li·ªáu
2. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu (DataPreprocessor)
3. Training v√† t·ªëi ∆∞u m√¥ h√¨nh (ModelTrainer)
4. ƒê√°nh gi√° v√† visualization
5. L∆∞u m√¥ h√¨nh v√† k·∫øt qu·∫£

C√°ch ch·∫°y:
    python main.py
    
    ho·∫∑c v·ªõi t√πy ch·ªçn:
    python main.py --optimize    # Ch·∫°y optimization v·ªõi Optuna
    python main.py --no-viz      # Kh√¥ng v·∫Ω bi·ªÉu ƒë·ªì
"""

import argparse
import logging
import sys
import subprocess
from pathlib import Path
from typing import List, Optional, Tuple

import pandas as pd

# Import t·ª´ project
from src.preprocessing.data_preprocessor import DataPreprocessor
from src.modeling.model_trainer import ModelTrainer
import config


# C·∫•u h√¨nh logging (force=True ƒë·ªÉ ghi r√µ r√†ng v√†o training.log)
config.LOG_FILE.parent.mkdir(parents=True, exist_ok=True)
logging.basicConfig(
    level=getattr(logging, config.LOG_LEVEL),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(config.LOG_FILE, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ],
    force=True
)
logger = logging.getLogger(__name__)


def download_data():
    """Download d·ªØ li·ªáu t·ª´ Google Drive n·∫øu ch∆∞a c√≥."""
    if config.DATA_FILE.exists():
        logger.info(f"‚úÖ D·ªØ li·ªáu ƒë√£ t·ªìn t·∫°i: {config.DATA_FILE}")
        return
    
    logger.info("üì• ƒêang download d·ªØ li·ªáu t·ª´ Google Drive...")
    
    try:
        # C√†i ƒë·∫∑t gdown n·∫øu ch∆∞a c√≥
        subprocess.check_call([sys.executable, "-m", "pip", "install", "gdown", "-q"])
        
        # Download file
        subprocess.run([
            "gdown", 
            config.GDRIVE_FILE_ID, 
            "-O", 
            str(config.DATA_FILE)
        ], check=True)
        
        logger.info(f"‚úÖ ƒê√£ download d·ªØ li·ªáu v√†o: {config.DATA_FILE}")
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi download d·ªØ li·ªáu: {e}")
        logger.info("üí° Vui l√≤ng download th·ªß c√¥ng v√† ƒë·∫∑t v√†o th∆∞ m·ª•c data/")
        sys.exit(1)


def preprocess_data(generate_viz: bool = True) -> Tuple[pd.DataFrame, Optional[List[str]]]:
    """Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu v√† tr·∫£ v·ªÅ danh s√°ch feature c√≥ t∆∞∆°ng quan cao v·ªõi target."""
    logger.info("\n" + "="*70)
    logger.info("üìä B∆Ø·ªöC 1: TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU")
    logger.info("="*70 + "\n")
    
    # Kh·ªüi t·∫°o preprocessor v√† load data
    preprocessor = DataPreprocessor()
    preprocessor.load(str(config.DATA_FILE))
    
    logger.info(f"D·ªØ li·ªáu g·ªëc: {preprocessor.data.shape}")
    
    # Ki·ªÉm tra missing values
    missing_df = preprocessor.check_missing()
    if len(missing_df) > 0:
        print("\n‚ö†Ô∏è  Missing Values:")
        print(missing_df.to_string(index=False))

    if generate_viz:
        logger.info("\nüñºÔ∏è  ƒêang t·∫°o c√°c bi·ªÉu ƒë·ªì EDA (t·ª± ƒë·ªông l∆∞u t·∫°i results/eda)...")
        preprocessor.generate_eda_report(target_col=config.TARGET_COLUMN)
    
    # X·ª≠ l√Ω missing values
    preprocessor.handle_missing(
        strategy='auto',
        numeric_strategy=config.MISSING_STRATEGY['numeric'],
        categorical_strategy=config.MISSING_STRATEGY['categorical']
    )
    
    # X√≥a outliers n·∫øu ƒë∆∞·ª£c c·∫•u h√¨nh
    if config.OUTLIER_DETECTION:
        preprocessor.remove_outliers(
            method=config.OUTLIER_METHOD,
            threshold=config.OUTLIER_THRESHOLD
        )
    
    # Encoding bi·∫øn ph√¢n lo·∫°i
    preprocessor.encode_categorical(
        method=config.ENCODING_METHOD,
        drop_first=config.DROP_FIRST_ONEHOT
    )
    
    # Scale features (chu·∫©n h√≥a d·ªØ li·ªáu) - QUAN TR·ªåNG
    logger.info("\nüìè Chu·∫©n h√≥a features (kh√¥ng ƒë·ª•ng t·ªõi target)...")
    preprocessor.scale_features(
        method='standard',
        exclude_columns=[config.TARGET_COLUMN]
    )
    heatmap_path = config.EDA_RESULTS_DIR / 'correlation_heatmap.png'
    corr_df = preprocessor.plot_correlation_heatmap(
        target_col=config.TARGET_COLUMN,
        method='spearman',
        save_path=heatmap_path,
        annot=True,
        show=False
    )
    logger.info(f"üìå Heatmap t∆∞∆°ng quan ƒë√£ l∆∞u t·∫°i: {heatmap_path}")
    poly_feature_subset: Optional[List[str]] = None
    if corr_df is not None and config.TARGET_COLUMN in corr_df.columns:
        corr_series = corr_df[config.TARGET_COLUMN].drop(labels=[config.TARGET_COLUMN])
        selected = corr_series[abs(corr_series) >= config.POLY_CORRELATION_THRESHOLD]
        if not selected.empty:
            poly_feature_subset = selected.index.tolist()
            logger.info(
                f"üéØ {len(poly_feature_subset)} feature c√≥ |corr| >= {config.POLY_CORRELATION_THRESHOLD}: {poly_feature_subset}"
            )
        else:
            logger.warning(
                f"‚ö†Ô∏è  Kh√¥ng c√≥ feature n√†o ƒë·∫°t ng∆∞·ª°ng |corr| >= {config.POLY_CORRELATION_THRESHOLD}. S·ª≠ d·ª•ng to√†n b·ªô features cho Polynomial."
            )
    
    # T·∫°o interaction features n·∫øu c·∫ßn
    if config.CREATE_INTERACTION_FEATURES:
        preprocessor.create_interaction_features(
            col_pairs=config.INTERACTION_PAIRS,
            operations=['multiply']
        )
    
    # In t√≥m t·∫Øt
    preprocessor.print_summary()
    
    # L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
    preprocessor.save_data(str(config.PROCESSED_DATA_FILE))
    
    return preprocessor.get_processed_data(), poly_feature_subset


def train_models(df: pd.DataFrame, optimize: bool = False,
                 poly_feature_subset: Optional[List[str]] = None):
    """
    Hu·∫•n luy·ªán c√°c m√¥ h√¨nh h·ªçc m√°y.
    
    Args:
        df: DataFrame ƒë√£ x·ª≠ l√Ω
        optimize: C√≥ ch·∫°y optimization kh√¥ng
        poly_feature_subset: Danh s√°ch feature d√πng ri√™ng cho Polynomial Regression
        
    Returns:
        ModelTrainer instance
    """
    logger.info("\n" + "="*70)
    logger.info("ü§ñ B∆Ø·ªöC 2: HU·∫§N LUY·ªÜN M√î H√åNH")
    logger.info("="*70 + "\n")
    
    # Chu·∫©n b·ªã d·ªØ li·ªáu
    X_train, X_test, y_train, y_test = ModelTrainer.prepare_data(
        df=df,
        target_col=config.TARGET_COLUMN,
        test_size=config.TEST_SIZE,
        random_state=config.RANDOM_SEED,
        scale=False  # KH√îNG scale - m·ªói model t·ª± x·ª≠ l√Ω
    )
    
    # Kh·ªüi t·∫°o trainer
    trainer = ModelTrainer(
        X_train=X_train,
        X_test=X_test,
        y_train=y_train,
        y_test=y_test,
        output_dir=str(config.MODELS_DIR)
    )
    
    logger.info(f"Data info: {trainer.data_info}\n")
    
    # ========== POLYNOMIAL REGRESSION ==========
    if optimize:
        logger.info("üîç T·ªëi ∆∞u Polynomial Regression...")
        best_poly_params = trainer.optimize_polynomial(
            n_trials=config.OPTUNA_N_TRIALS['polynomial'],
            timeout=config.OPTUNA_TIMEOUT['polynomial']
        )
        trainer.train_polynomial(
            degree=best_poly_params.get('degree', config.DEFAULT_HYPERPARAMS['polynomial']['degree']),
            alpha=best_poly_params.get('alpha', config.DEFAULT_HYPERPARAMS['polynomial']['alpha']),
            feature_subset=poly_feature_subset
        )
    else:
        default_poly = config.DEFAULT_HYPERPARAMS['polynomial']
        trainer.train_polynomial(
            degree=default_poly['degree'],
            alpha=default_poly['alpha'],
            feature_subset=poly_feature_subset
        )
    
    # ========== RANDOM FOREST ==========
    if optimize:
        logger.info("üîç T·ªëi ∆∞u Random Forest...")
        best_rf_params = trainer.optimize_rf(
            n_trials=config.OPTUNA_N_TRIALS['random_forest'],
            timeout=config.OPTUNA_TIMEOUT['random_forest']
        )
        trainer.train_rf(**best_rf_params)
    else:
        trainer.train_rf(**config.DEFAULT_HYPERPARAMS['random_forest'])
    
    # ========== XGBOOST ==========
    if optimize:
        logger.info("üîç T·ªëi ∆∞u XGBoost...")
        best_xgb_params = trainer.optimize_xgb(
            n_trials=config.OPTUNA_N_TRIALS['xgboost'],
            timeout=config.OPTUNA_TIMEOUT['xgboost']
        )
        trainer.train_xgb(**best_xgb_params)
    else:
        trainer.train_xgb(**config.DEFAULT_HYPERPARAMS['xgboost'])
    
    return trainer


def evaluate_and_visualize(trainer: ModelTrainer, visualize: bool = True):
    """
    ƒê√°nh gi√° v√† visualization k·∫øt qu·∫£.
    
    Args:
        trainer: ModelTrainer instance
        visualize: C√≥ v·∫Ω bi·ªÉu ƒë·ªì kh√¥ng
    """
    logger.info("\n" + "="*70)
    logger.info("üìä B∆Ø·ªöC 3: ƒê√ÅNH GI√Å V√Ä VISUALIZATION")
    logger.info("="*70 + "\n")
    
    # In t√≥m t·∫Øt k·∫øt qu·∫£
    trainer.summary()
    
    # L∆∞u k·∫øt qu·∫£
    trainer.save_results(config.RESULTS_FILE)
    
    # L∆∞u t·∫•t c·∫£ m√¥ h√¨nh
    trainer.save_all_models(format=config.MODEL_FORMAT)
    
    if visualize:
        # V·∫Ω bi·ªÉu ƒë·ªì so s√°nh
        logger.info("üìà V·∫Ω bi·ªÉu ƒë·ªì so s√°nh...")
        trainer.plot_comparison(metric='test_r2', save=True)
        trainer.plot_comparison(metric='test_rmse', save=True)
        trainer.plot_comparison(metric='test_mae', save=True)
        
        # V·∫Ω bi·ªÉu ƒë·ªì predictions
        logger.info("üìà V·∫Ω bi·ªÉu ƒë·ªì predictions...")
        trainer.plot_all_predictions(save=True)
        
        # V·∫Ω feature importance
        logger.info("üìà V·∫Ω bi·ªÉu ƒë·ªì feature importance...")
        trainer.plot_all_feature_importance(top_n=15, save=True)
        
        # So s√°nh feature importance
        logger.info("üìà So s√°nh feature importance...")
        trainer.compare_feature_importance(top_n=10, save=True)
    
    # T√¨m m√¥ h√¨nh t·ªët nh·∫•t
    best_name, best_result = trainer.get_best_model()
    
    logger.info("\n" + "="*70)
    logger.info(f"‚ú® M√î H√åNH T·ªêT NH·∫§T: {best_name.upper()}")
    logger.info(f"   Test R¬≤: {best_result['test_r2']:.6f}")
    logger.info(f"   Test RMSE: {best_result['test_rmse']:.6f}")
    logger.info(f"   Test MAE: {best_result['test_mae']:.6f}")
    logger.info("="*70 + "\n")


def main():
    """Main function ƒë·ªÉ ch·∫°y to√†n b·ªô pipeline."""
    # Parse arguments
    parser = argparse.ArgumentParser(description='Taxi Price Prediction Pipeline')
    parser.add_argument('--optimize', action='store_true', 
                       help='Ch·∫°y optimization v·ªõi Optuna')
    parser.add_argument('--no-viz', action='store_true',
                       help='Kh√¥ng v·∫Ω bi·ªÉu ƒë·ªì visualization')
    parser.add_argument('--skip-download', action='store_true',
                       help='B·ªè qua b∆∞·ªõc download d·ªØ li·ªáu')
    
    args = parser.parse_args()
    
    logger.info("\n" + "="*70)
    logger.info("üöÄ B·∫ÆT ƒê·∫¶U TAXI PRICE PREDICTION PIPELINE")
    logger.info("="*70 + "\n")
    
    try:
        # B∆∞·ªõc 0: Download d·ªØ li·ªáu
        if not args.skip_download:
            download_data()
        
        # B∆∞·ªõc 1: Ti·ªÅn x·ª≠ l√Ω
        df_processed, poly_feature_subset = preprocess_data(generate_viz=not args.no_viz)
        
        # B∆∞·ªõc 2: Training
        trainer = train_models(
            df_processed,
            optimize=args.optimize,
            poly_feature_subset=poly_feature_subset
        )
        
        # B∆∞·ªõc 3: ƒê√°nh gi√° v√† visualization
        evaluate_and_visualize(trainer, visualize=not args.no_viz)
        
        logger.info("\n" + "="*70)
        logger.info("‚úÖ HO√ÄN T·∫§T PIPELINE TH√ÄNH C√îNG!")
        logger.info("="*70 + "\n")
        logger.info(f"üìÅ M√¥ h√¨nh ƒë√£ l∆∞u t·∫°i: {config.MODELS_DIR}")
        logger.info(f"üìÅ K·∫øt qu·∫£ ƒë√£ l∆∞u t·∫°i: {config.RESULTS_DIR}")
        
    except Exception as e:
        logger.error(f"\n‚ùå L·ªñI: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
