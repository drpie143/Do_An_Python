"""
Script ch√≠nh ƒë·ªÉ ch·∫°y to√†n b·ªô pipeline:
1. Download v√† load d·ªØ li·ªáu
2. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu (DataPreprocessor)
3. Training v√† t·ªëi ∆∞u m√¥ h√¨nh (ModelTrainer)
4. ƒê√°nh gi√° v√† visualization
5. L∆∞u m√¥ h√¨nh v√† k·∫øt qu·∫£

C√°ch ch·∫°y:
    python main.py
    
    ho·∫∑c v·ªõi t√πy ch·ªçn:
    python main.py --optimize    # Ch·∫°y optimization v·ªõi Optuna
    python main.py --no-viz      # Kh√¥ng v·∫Ω bi·ªÉu ƒë·ªì
"""

import argparse
import logging
import sys
import subprocess
from pathlib import Path
from typing import List, Optional, Tuple

import pandas as pd
from sklearn.model_selection import train_test_split

# Import t·ª´ project
from src.preprocessing.data_preprocessor import DataPreprocessor
from src.modeling.model_trainer import ModelTrainer
import config


# C·∫•u h√¨nh logging (force=True ƒë·ªÉ ghi r√µ r√†ng v√†o training.log)
config.LOG_FILE.parent.mkdir(parents=True, exist_ok=True)
logging.basicConfig(
    level=getattr(logging, config.LOG_LEVEL),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(config.LOG_FILE, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ],
    force=True
)
logger = logging.getLogger(__name__)


def download_data():
    """Download d·ªØ li·ªáu t·ª´ Google Drive n·∫øu ch∆∞a c√≥."""
    if config.DATA_FILE.exists():
        logger.info(f"‚úÖ D·ªØ li·ªáu ƒë√£ t·ªìn t·∫°i: {config.DATA_FILE}")
        return
    
    logger.info("üì• ƒêang download d·ªØ li·ªáu t·ª´ Google Drive...")
    
    try:
        # C√†i ƒë·∫∑t gdown n·∫øu ch∆∞a c√≥
        subprocess.check_call([sys.executable, "-m", "pip", "install", "gdown", "-q"])
        
        # Download file
        subprocess.run([
            "gdown", 
            config.GDRIVE_FILE_ID, 
            "-O", 
            str(config.DATA_FILE)
        ], check=True)
        
        logger.info(f"‚úÖ ƒê√£ download d·ªØ li·ªáu v√†o: {config.DATA_FILE}")
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi download d·ªØ li·ªáu: {e}")
        logger.info("üí° Vui l√≤ng download th·ªß c√¥ng v√† ƒë·∫∑t v√†o th∆∞ m·ª•c data/")
        sys.exit(1)

def preprocess_data(
    generate_viz: bool = True,
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, Optional[List[str]]]:
    """Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu, t√°ch train/test v√† tr·∫£ v·ªÅ b·ªô d·ªØ li·ªáu ƒë√£ s·∫°ch."""
    logger.info("\n" + "="*70)
    logger.info("üìä B∆Ø·ªöC 1: TI·ªÄN X·ª¨ L√ù D·ªÆ LI·ªÜU")
    logger.info("="*70 + "\n")
    
    # Kh·ªüi t·∫°o preprocessor v√† load data
    preprocessor = DataPreprocessor()
    preprocessor.load(str(config.DATA_FILE))
    preprocessor.feature_engineering(settings=getattr(config, "SPEED_FEATURE", None))
    if getattr(config, "CONSTRAINT_RULES", None):
        preprocessor.constraint_rules = config.CONSTRAINT_RULES.copy()
    
    logger.info(f"D·ªØ li·ªáu g·ªëc: {preprocessor.data.shape}")
    
    # T·∫°o b√°o c√°o t·ªïng quan nhanh ƒë·ªÉ log c√°c c·∫£nh b√°o EDA
    logger.info("\nüßæ ƒêang ch·∫°y EDA overview...")
    overview = preprocessor.eda_overview(top_n=5)

    missing_df = None
    if overview and isinstance(overview.get("column_profile"), pd.DataFrame):
        column_profile = overview["column_profile"]
        missing_df = (
            column_profile[column_profile["missing"] > 0]
            .reset_index()
            .rename(columns={"index": "Column", "missing": "Missing_Count", "missing_pct": "Missing_Percent"})
        )
    if missing_df is not None and not missing_df.empty:
        print("\n‚ö†Ô∏è  Missing Values:")
        print(missing_df.to_string(index=False))

    if generate_viz:
        logger.info("\nüñºÔ∏è  ƒêang t·∫°o c√°c bi·ªÉu ƒë·ªì EDA (t·ª± ƒë·ªông l∆∞u t·∫°i results/eda)...")
        preprocessor.generate_eda_report(target_col=config.TARGET_COLUMN)

    # √Åp d·ª•ng r√†ng bu·ªôc d·ªØ li·ªáu (kh√¥ng ph·ª• thu·ªôc train/test)
    if preprocessor.constraint_rules:
        preprocessor.apply_constraints()

    # T·∫°o interaction features n·∫øu c·∫ßn (deterministic, kh√¥ng c·∫ßn fit)
    if config.CREATE_INTERACTION_FEATURES:
        preprocessor.create_interaction_features(
            col_pairs=config.INTERACTION_PAIRS,
            operations=['multiply']
        )

    base_df = preprocessor.get_processed_data()
    logger.info("üìÇ D·ªØ li·ªáu sau b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω c∆° b·∫£n: %s", base_df.shape)

    logger.info("\nüîÄ Chia train/test tr∆∞·ªõc khi fit encoder/scaler ƒë·ªÉ tr√°nh data leakage...")
    train_df, test_df = train_test_split(
        base_df,
        test_size=config.TEST_SIZE,
        random_state=config.RANDOM_SEED,
        shuffle=True
    )
    train_df = train_df.reset_index(drop=True)
    test_df = test_df.reset_index(drop=True)
    logger.info("Train: %s | Test: %s", train_df.shape, test_df.shape)

    train_preprocessor = DataPreprocessor(train_df)
    train_preprocessor.handle_missing(
        strategy='auto',
        numeric_strategy=config.MISSING_STRATEGY['numeric'],
        categorical_strategy=config.MISSING_STRATEGY['categorical']
    )
    if config.OUTLIER_DETECTION:
        train_preprocessor.remove_outliers(
            method=config.OUTLIER_METHOD,
            threshold=config.OUTLIER_THRESHOLD
        )
    train_preprocessor.encode_categorical(
        method=config.ENCODING_METHOD,
        drop_first=config.DROP_FIRST_ONEHOT
    )
    logger.info("\nüìè Chu·∫©n h√≥a features (fit tr√™n train, transform cho test)...")
    train_preprocessor.scale_features(
        method='standard',
        exclude_columns=[config.TARGET_COLUMN]
    )

    train_processed = train_preprocessor.get_processed_data()
    test_processed = train_preprocessor.transform_dataset(test_df)

    # Heatmap/Correlation d·ª±a tr√™n train ƒë√£ x·ª≠ l√Ω
    heatmap_path = config.EDA_RESULTS_DIR / 'correlation_heatmap.png'
    corr_df = train_preprocessor.plot_correlation_heatmap(
        target_col=config.TARGET_COLUMN,
        method='spearman',
        save_path=heatmap_path,
        annot=True,
        show=False
    )
    logger.info(f"üìå Heatmap t∆∞∆°ng quan (train) ƒë√£ l∆∞u t·∫°i: {heatmap_path}")
    poly_feature_subset: Optional[List[str]] = None
    if corr_df is not None and config.TARGET_COLUMN in corr_df.columns:
        corr_series = corr_df[config.TARGET_COLUMN].drop(labels=[config.TARGET_COLUMN])
        selected = corr_series[abs(corr_series) >= config.POLY_CORRELATION_THRESHOLD]
        if not selected.empty:
            poly_feature_subset = selected.index.tolist()
            logger.info(
                f"üéØ {len(poly_feature_subset)} feature c√≥ |corr| >= {config.POLY_CORRELATION_THRESHOLD}: {poly_feature_subset}"
            )
        else:
            logger.warning(
                f"‚ö†Ô∏è  Kh√¥ng c√≥ feature n√†o ƒë·∫°t ng∆∞·ª°ng |corr| >= {config.POLY_CORRELATION_THRESHOLD}. S·ª≠ d·ª•ng to√†n b·ªô features cho Polynomial."
            )

    train_preprocessor.print_summary()

    combined_processed = pd.concat([train_processed, test_processed], axis=0).reset_index(drop=True)
    combined_processed.to_csv(config.PROCESSED_DATA_FILE, index=False)
    logger.info("üíæ ƒê√£ l∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω (train+test) t·∫°i %s", config.PROCESSED_DATA_FILE)

    X_train = train_processed.drop(columns=[config.TARGET_COLUMN])
    y_train = train_processed[config.TARGET_COLUMN]
    X_test = test_processed.drop(columns=[config.TARGET_COLUMN])
    y_test = test_processed[config.TARGET_COLUMN]

    return X_train, X_test, y_train, y_test, poly_feature_subset

def train_models(
    X_train: pd.DataFrame,
    X_test: pd.DataFrame,
    y_train: pd.Series,
    y_test: pd.Series,
    optimize: bool = False,
    poly_feature_subset: Optional[List[str]] = None,
):
    """
    Hu·∫•n luy·ªán c√°c m√¥ h√¨nh h·ªçc m√°y tr√™n b·ªô d·ªØ li·ªáu ƒë√£ t√°ch train/test.
    
    Args:
        X_train, X_test, y_train, y_test: d·ªØ li·ªáu sau preprocessing (kh√¥ng leakage)
        optimize: C√≥ ch·∫°y optimization kh√¥ng
        poly_feature_subset: Danh s√°ch feature d√πng ri√™ng cho Polynomial Regression
        
    Returns:
        ModelTrainer instance
    """
    logger.info("\n" + "="*70)
    logger.info("ü§ñ B∆Ø·ªöC 2: HU·∫§N LUY·ªÜN M√î H√åNH")
    logger.info("="*70 + "\n")
    
    # Kh·ªüi t·∫°o trainer
    trainer = ModelTrainer(
        X_train=X_train,
        X_test=X_test,
        y_train=y_train,
        y_test=y_test,
        output_dir=str(config.MODELS_DIR)
    )
    
    logger.info(f"Data info: {trainer.data_info}\n")
    
    # ========== POLYNOMIAL REGRESSION ==========
    if optimize:
        logger.info("üîç T·ªëi ∆∞u Polynomial Regression...")
        best_poly_params = trainer.optimize_polynomial(
            n_trials=config.OPTUNA_N_TRIALS['polynomial'],
            timeout=config.OPTUNA_TIMEOUT['polynomial']
        )
        trainer.train_polynomial(
            degree=best_poly_params.get('degree', config.DEFAULT_HYPERPARAMS['polynomial']['degree']),
            alpha=best_poly_params.get('alpha', config.DEFAULT_HYPERPARAMS['polynomial']['alpha']),
            feature_subset=poly_feature_subset
        )
    else:
        default_poly = config.DEFAULT_HYPERPARAMS['polynomial']
        trainer.train_polynomial(
            degree=default_poly['degree'],
            alpha=default_poly['alpha'],
            feature_subset=poly_feature_subset
        )
    
    # ========== RANDOM FOREST ==========
    if optimize:
        logger.info("üîç T·ªëi ∆∞u Random Forest...")
        best_rf_params = trainer.optimize_rf(
            n_trials=config.OPTUNA_N_TRIALS['random_forest'],
            timeout=config.OPTUNA_TIMEOUT['random_forest']
        )
        trainer.train_rf(**best_rf_params)
    else:
        trainer.train_rf(**config.DEFAULT_HYPERPARAMS['random_forest'])
    
    # ========== XGBOOST ==========
    if optimize:
        logger.info("üîç T·ªëi ∆∞u XGBoost...")
        best_xgb_params = trainer.optimize_xgb(
            n_trials=config.OPTUNA_N_TRIALS['xgboost'],
            timeout=config.OPTUNA_TIMEOUT['xgboost']
        )
        trainer.train_xgb(**best_xgb_params)
    else:
        trainer.train_xgb(**config.DEFAULT_HYPERPARAMS['xgboost'])
    
    return trainer


def evaluate_and_visualize(trainer: ModelTrainer, visualize: bool = True):
    """
    ƒê√°nh gi√° v√† visualization k·∫øt qu·∫£.
    
    Args:
        trainer: ModelTrainer instance
        visualize: C√≥ v·∫Ω bi·ªÉu ƒë·ªì kh√¥ng
    """
    logger.info("\n" + "="*70)
    logger.info("üìä B∆Ø·ªöC 3: ƒê√ÅNH GI√Å V√Ä VISUALIZATION")
    logger.info("="*70 + "\n")
    
    # In t√≥m t·∫Øt k·∫øt qu·∫£
    trainer.summary()
    
    # L∆∞u k·∫øt qu·∫£
    trainer.save_results(config.RESULTS_FILE)
    
    # L∆∞u t·∫•t c·∫£ m√¥ h√¨nh
    trainer.save_all_models(format=config.MODEL_FORMAT)
    
    if visualize:
        # V·∫Ω bi·ªÉu ƒë·ªì so s√°nh
        logger.info("üìà V·∫Ω bi·ªÉu ƒë·ªì so s√°nh...")
        trainer.plot_comparison(metric='test_r2', save=True)
        trainer.plot_comparison(metric='test_rmse', save=True)
        trainer.plot_comparison(metric='test_mae', save=True)
        
        # V·∫Ω bi·ªÉu ƒë·ªì predictions
        logger.info("üìà V·∫Ω bi·ªÉu ƒë·ªì predictions...")
        trainer.plot_all_predictions(save=True)
        
        # V·∫Ω feature importance
        logger.info("üìà V·∫Ω bi·ªÉu ƒë·ªì feature importance...")
        trainer.plot_all_feature_importance(top_n=15, save=True)
        
        # So s√°nh feature importance
        logger.info("üìà So s√°nh feature importance...")
        trainer.compare_feature_importance(top_n=10, save=True)
    
    # T√¨m m√¥ h√¨nh t·ªët nh·∫•t
    best_name, best_result = trainer.get_best_model()
    
    logger.info("\n" + "="*70)
    logger.info(f"‚ú® M√î H√åNH T·ªêT NH·∫§T: {best_name.upper()}")
    logger.info(f"   Test R¬≤: {best_result['test_r2']:.6f}")
    logger.info(f"   Test RMSE: {best_result['test_rmse']:.6f}")
    logger.info(f"   Test MAE: {best_result['test_mae']:.6f}")
    logger.info("="*70 + "\n")


def main():
    """Main function ƒë·ªÉ ch·∫°y to√†n b·ªô pipeline."""
    # Parse arguments
    parser = argparse.ArgumentParser(description='Taxi Price Prediction Pipeline')
    parser.add_argument('--optimize', action='store_true', 
                       help='Ch·∫°y optimization v·ªõi Optuna')
    parser.add_argument('--no-viz', action='store_true',
                       help='Kh√¥ng v·∫Ω bi·ªÉu ƒë·ªì visualization')
    parser.add_argument('--skip-download', action='store_true',
                       help='B·ªè qua b∆∞·ªõc download d·ªØ li·ªáu')
    
    args = parser.parse_args()
    
    logger.info("\n" + "="*70)
    logger.info("üöÄ B·∫ÆT ƒê·∫¶U TAXI PRICE PREDICTION PIPELINE")
    logger.info("="*70 + "\n")
    
    try:
        # B∆∞·ªõc 0: Download d·ªØ li·ªáu
        if not args.skip_download:
            download_data()
        
        # B∆∞·ªõc 1: Ti·ªÅn x·ª≠ l√Ω (t√°ch train/test tr∆∞·ªõc khi train)
        X_train, X_test, y_train, y_test, poly_feature_subset = preprocess_data(
            generate_viz=not args.no_viz
        )

        # B∆∞·ªõc 2: Training
        trainer = train_models(
            X_train=X_train,
            X_test=X_test,
            y_train=y_train,
            y_test=y_test,
            optimize=args.optimize,
            poly_feature_subset=poly_feature_subset
        )
        
        # B∆∞·ªõc 3: ƒê√°nh gi√° v√† visualization
        evaluate_and_visualize(trainer, visualize=not args.no_viz)
        
        logger.info("\n" + "="*70)
        logger.info("‚úÖ HO√ÄN T·∫§T PIPELINE TH√ÄNH C√îNG!")
        logger.info("="*70 + "\n")
        logger.info(f"üìÅ M√¥ h√¨nh ƒë√£ l∆∞u t·∫°i: {config.MODELS_DIR}")
        logger.info(f"üìÅ K·∫øt qu·∫£ ƒë√£ l∆∞u t·∫°i: {config.RESULTS_DIR}")
        
    except Exception as e:
        logger.error(f"\n‚ùå L·ªñI: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
