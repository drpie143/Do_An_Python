"""
Script chÃ­nh Ä‘á»ƒ cháº¡y toÃ n bá»™ pipeline:
1. Download vÃ  load dá»¯ liá»‡u
2. Tiá»n xá»­ lÃ½ dá»¯ liá»‡u (DataPreprocessor)
3. Training vÃ  tá»‘i Æ°u mÃ´ hÃ¬nh (ModelTrainer)
4. ÄÃ¡nh giÃ¡ vÃ  visualization
5. LÆ°u mÃ´ hÃ¬nh vÃ  káº¿t quáº£

CÃ¡ch cháº¡y:
    python main.py
    
    hoáº·c vá»›i tÃ¹y chá»n:
    python main.py --optimize    # Cháº¡y optimization vá»›i Optuna
    python main.py --no-viz      # KhÃ´ng váº½ biá»ƒu Ä‘á»“
"""

import argparse
import importlib
import json
import logging
import sys
import subprocess
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import pandas as pd
from sklearn.model_selection import train_test_split

# Import tá»« project
from src.preprocessing.data_preprocessor import DataPreprocessor
from src.modeling.model_trainer import ModelTrainer
import config


# Cáº¥u hÃ¬nh logging (ghi Ä‘Ã¨ file log má»—i láº§n cháº¡y & format rÃµ rÃ ng)
config.LOG_FILE.parent.mkdir(parents=True, exist_ok=True)
LOG_FORMAT = "%(asctime)s | %(levelname)-8s | %(name)s | %(message)s"
CONSOLE_FORMAT = "%(levelname)-8s | %(message)s"
DATE_FORMAT = "%Y-%m-%d %H:%M:%S"

file_handler = logging.FileHandler(config.LOG_FILE, mode='w', encoding='utf-8')
file_handler.setFormatter(logging.Formatter(LOG_FORMAT, DATE_FORMAT))

console_handler = logging.StreamHandler(sys.stdout)
console_handler.setFormatter(logging.Formatter(CONSOLE_FORMAT, DATE_FORMAT))

logging.basicConfig(
    level=getattr(logging, config.LOG_LEVEL),
    handlers=[file_handler, console_handler],
    force=True,
)
logger = logging.getLogger(__name__)


def _log_divider(char: str = "=", width: int = 70) -> None:
    """Log a horizontal divider with consistent width."""
    logger.info(char * width)


def log_section(title: str, icon: str = "ğŸš€") -> None:
    """Render a bold section banner combining the legacy and sample styles."""
    logger.info("\n")
    _log_divider()
    logger.info("%s %s", icon, title.upper())
    _log_divider()
    logger.info("")


def log_step(title: str, icon: str = "ğŸ”¹") -> None:
    """Highlight a sub-step within a section."""
    logger.info("%s %s", icon, title)


def log_key_value(label: str, value) -> None:
    """Align key-value summary rows for faster scanning."""
    logger.info("   %-18s: %s", label, value)


def log_stage_summary(stage_times: List[Tuple[str, float]], total_duration: float) -> None:
    """Pretty print per-stage durations and pipeline total."""
    logger.info("â±ï¸ PIPELINE SUMMARY")
    for stage, duration in stage_times:
        log_key_value(stage.capitalize(), f"{duration:.2f} giÃ¢y")
    log_key_value("Total", f"{total_duration:.2f} giÃ¢y")


def _ensure_gdown() -> Optional[object]:
    """Ensure gdown is importable without forcing installs every run."""
    try:
        return importlib.import_module("gdown")
    except ImportError:
        logger.info("gdown chÆ°a Ä‘Æ°á»£c cÃ i. Äang tiáº¿n hÃ nh cÃ i Ä‘áº·t má»™t láº§n...")
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "gdown"])
        except subprocess.CalledProcessError as exc:
            logger.warning("KhÃ´ng thá»ƒ cÃ i Ä‘áº·t gdown tá»± Ä‘á»™ng: %s", exc)
            return None
        try:
            return importlib.import_module("gdown")
        except ImportError:
            return None


def download_data():
    """Download dá»¯ liá»‡u tá»« Google Drive náº¿u chÆ°a cÃ³."""
    if config.DATA_FILE.exists():
        log_step(f"Dá»¯ liá»‡u Ä‘Ã£ tá»“n táº¡i: {config.DATA_FILE}", icon="âœ…")
        return

    log_step("Äang download dá»¯ liá»‡u tá»« Google Drive (náº¿u kháº£ dá»¥ng)...", icon="ğŸ“¥")
    gdown = _ensure_gdown()
    if gdown is None:
        logger.warning("KhÃ´ng thá»ƒ import gdown. Vui lÃ²ng táº£i thá»§ cÃ´ng file taxi_price.csv vÃ  Ä‘áº·t vÃ o %s", config.DATA_DIR)
        if not config.DATA_FILE.exists():
            raise FileNotFoundError(
                "KhÃ´ng táº£i Ä‘Æ°á»£c dá»¯ liá»‡u tá»± Ä‘á»™ng vÃ¬ thiáº¿u gdown. HÃ£y Ä‘áº·t file taxi_price.csv vÃ o thÆ° má»¥c data vÃ  cháº¡y láº¡i."
            )
        return

    try:
        gdown.download(id=config.GDRIVE_FILE_ID, output=str(config.DATA_FILE), quiet=False)
        log_step(f"ÄÃ£ download dá»¯ liá»‡u vÃ o: {config.DATA_FILE}", icon="âœ…")
    except Exception as exc:
        logger.error("âŒ Lá»—i khi download dá»¯ liá»‡u: %s", exc)
        logger.info("ğŸ’¡ Vui lÃ²ng download thá»§ cÃ´ng vÃ  Ä‘áº·t vÃ o thÆ° má»¥c data/")

    if not config.DATA_FILE.exists():
        raise FileNotFoundError(
            "KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u taxi_price.csv. Download tá»± Ä‘á»™ng tháº¥t báº¡i, vui lÃ²ng táº£i thá»§ cÃ´ng vÃ  cháº¡y láº¡i."
        )



def preprocess_data(generate_viz: bool = True) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, Optional[List[str]], DataPreprocessor]:
    """Tiá»n xá»­ lÃ½ dá»¯ liá»‡u vá»›i train/test split trÆ°á»›c khi fit scaler/encoder."""
    log_section("BÆ¯á»šC 1: TIá»€N Xá»¬ LÃ Dá»® LIá»†U", icon="ğŸ§¹")
    
    preprocessor = DataPreprocessor()
    log_step("Äang náº¡p dá»¯ liá»‡u gá»‘c", icon="ğŸ“¥")
    preprocessor.load(str(config.DATA_FILE))
    log_step(f"Dá»¯ liá»‡u gá»‘c: {preprocessor.data.shape}", icon="ğŸ“¦")

    missing_df = preprocessor.check_missing()
    if len(missing_df) > 0:
        print("\nâš ï¸  Missing Values:")
        print(missing_df.to_string(index=False))

    if generate_viz:
        log_step("Äang táº¡o cÃ¡c biá»ƒu Ä‘á»“ EDA (lÆ°u táº¡i results/eda)", icon="ğŸ–¼ï¸")
        preprocessor.generate_eda_report(target_col=config.TARGET_COLUMN)

    log_step("Chuáº©n hÃ³a dá»¯ liá»‡u trÆ°á»›c khi split", icon="ğŸ§½")
    preprocessor.apply_constraints()
    preprocessor.unify_values()
    preprocessor.feature_engineering()
    base_clean_df = preprocessor.get_processed_data()
    log_step(f"Dá»¯ liá»‡u sáº¡ch (pre-split): {base_clean_df.shape}", icon="ğŸ“")

    train_df, test_df = train_test_split(
        base_clean_df,
        test_size=config.TEST_SIZE,
        random_state=config.RANDOM_SEED,
    )
    log_step(f"Split dá»¯ liá»‡u -> Train: {train_df.shape}, Test: {test_df.shape}", icon="ğŸ”€")

    train_preprocessor = DataPreprocessor(train_df)
    train_preprocessor.apply_constraints()
    train_preprocessor.unify_values()
    train_preprocessor.feature_engineering()
    train_preprocessor.handle_missing(
        strategy='auto',
        numeric_strategy=config.MISSING_STRATEGY['numeric'],
        categorical_strategy=config.MISSING_STRATEGY['categorical']
    )
    if config.OUTLIER_DETECTION:
        train_preprocessor.remove_outliers(
            method=config.OUTLIER_METHOD,
            threshold=config.OUTLIER_THRESHOLD
        )
    train_preprocessor.encode_categorical(
        method=config.ENCODING_METHOD,
        drop_first=config.DROP_FIRST_ONEHOT
    )
    log_step("Chuáº©n hÃ³a features (dá»±a trÃªn train set)", icon="ğŸ“")
    train_preprocessor.scale_features(
        method=config.SCALING_METHOD,
        exclude_columns=[config.TARGET_COLUMN]
    )
    if config.CREATE_INTERACTION_FEATURES:
        train_preprocessor.create_interaction_features(
            col_pairs=config.INTERACTION_PAIRS,
            operations=['multiply']
        )

    heatmap_path = config.EDA_RESULTS_DIR / 'correlation_heatmap_train.png'
    corr_df = train_preprocessor.plot_correlation_heatmap(
        target_col=config.TARGET_COLUMN,
        method='spearman',
        save_path=heatmap_path,
        annot=True,
        show=False
    )
    log_step(f"Heatmap tÆ°Æ¡ng quan (train) Ä‘Ã£ lÆ°u táº¡i: {heatmap_path}", icon="ğŸ“Œ")
    poly_feature_subset: Optional[List[str]] = None
    if corr_df is not None and config.TARGET_COLUMN in corr_df.columns:
        corr_series = corr_df[config.TARGET_COLUMN].drop(labels=[config.TARGET_COLUMN])
        selected = corr_series[abs(corr_series) >= config.POLY_CORRELATION_THRESHOLD]
        if not selected.empty:
            poly_feature_subset = selected.index.tolist()
            log_step(
                f"{len(poly_feature_subset)} feature cÃ³ |corr| >= {config.POLY_CORRELATION_THRESHOLD}: {poly_feature_subset}",
                icon="ğŸ¯"
            )
        else:
            logger.warning(
                f"âš ï¸  KhÃ´ng cÃ³ feature nÃ o Ä‘áº¡t ngÆ°á»¡ng |corr| >= {config.POLY_CORRELATION_THRESHOLD}. Sá»­ dá»¥ng toÃ n bá»™ features cho Polynomial."
            )

    train_preprocessor.print_summary()
    train_preprocessor.mark_as_fitted()

    train_processed = train_preprocessor.get_processed_data()
    test_processed = train_preprocessor.transform_new_data(test_df)

    combined = pd.concat(
        [
            train_processed.assign(split='train'),
            test_processed.assign(split='test')
        ],
        ignore_index=True,
    )
    combined.to_csv(config.PROCESSED_DATA_FILE, index=False)

    X_train = train_processed.drop(columns=[config.TARGET_COLUMN])
    y_train = train_processed[config.TARGET_COLUMN]
    X_test = test_processed.drop(columns=[config.TARGET_COLUMN])
    y_test = test_processed[config.TARGET_COLUMN]

    return X_train, X_test, y_train, y_test, poly_feature_subset, train_preprocessor


def train_models(
    X_train: pd.DataFrame,
    X_test: pd.DataFrame,
    y_train: pd.Series,
    y_test: pd.Series,
    optimize: bool = False,
    poly_feature_subset: Optional[List[str]] = None,
):
    """Huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh há»c mÃ¡y vá»›i dá»¯ liá»‡u Ä‘Ã£ split."""
    log_section("BÆ¯á»šC 2: HUáº¤N LUYá»†N MÃ” HÃŒNH", icon="ğŸ¤–")
    
    # Khá»Ÿi táº¡o trainer
    trainer = ModelTrainer(
        X_train=X_train,
        X_test=X_test,
        y_train=y_train,
        y_test=y_test,
        output_dir=str(config.MODELS_DIR)
    )
    
    log_step(f"Data info: {trainer.data_info}", icon="ğŸ“Š")
    
    # ========== POLYNOMIAL REGRESSION ==========
    if optimize:
        log_step("Tá»‘i Æ°u Polynomial Regression", icon="ğŸ”")
        best_poly_params = trainer.optimize_polynomial(
            n_trials=config.OPTUNA_N_TRIALS['polynomial'],
            timeout=config.OPTUNA_TIMEOUT['polynomial']
        )
        trainer.train_polynomial(
            degree=best_poly_params.get('degree', config.DEFAULT_HYPERPARAMS['polynomial']['degree']),
            alpha=best_poly_params.get('alpha', config.DEFAULT_HYPERPARAMS['polynomial']['alpha']),
            feature_subset=poly_feature_subset
        )
    else:
        default_poly = config.DEFAULT_HYPERPARAMS['polynomial']
        trainer.train_polynomial(
            degree=default_poly['degree'],
            alpha=default_poly['alpha'],
            feature_subset=poly_feature_subset
        )
    
    # ========== RANDOM FOREST ==========
    if optimize:
        log_step("Tá»‘i Æ°u Random Forest", icon="ğŸ”")
        best_rf_params = trainer.optimize_rf(
            n_trials=config.OPTUNA_N_TRIALS['random_forest'],
            timeout=config.OPTUNA_TIMEOUT['random_forest']
        )
        trainer.train_rf(**best_rf_params)
    else:
        trainer.train_rf(**config.DEFAULT_HYPERPARAMS['random_forest'])
    
    # ========== XGBOOST ==========
    if optimize:
        log_step("Tá»‘i Æ°u XGBoost", icon="ğŸ”")
        best_xgb_params = trainer.optimize_xgb(
            n_trials=config.OPTUNA_N_TRIALS['xgboost'],
            timeout=config.OPTUNA_TIMEOUT['xgboost']
        )
        trainer.train_xgb(**best_xgb_params)
    else:
        trainer.train_xgb(**config.DEFAULT_HYPERPARAMS['xgboost'])
    
    return trainer


def evaluate_and_visualize(trainer: ModelTrainer, preprocessor: DataPreprocessor, visualize: bool = True) -> None:
    """
    ÄÃ¡nh giÃ¡ vÃ  visualization káº¿t quáº£.
    
    Args:
        trainer: ModelTrainer instance
        visualize: CÃ³ váº½ biá»ƒu Ä‘á»“ khÃ´ng
    """
    log_section("BÆ¯á»šC 3: ÄÃNH GIÃ & VISUALIZATION", icon="ğŸ“Š")
    
    # In tÃ³m táº¯t káº¿t quáº£
    trainer.summary()
    
    # LÆ°u káº¿t quáº£
    trainer.save_results(config.RESULTS_FILE)
    
    # LÆ°u toÃ n bá»™ mÃ´ hÃ¬nh vÃ  cáº¥u hÃ¬nh tiá»n xá»­ lÃ½
    saved_model_paths = trainer.save_all_models(format=config.MODEL_FORMAT)
    preprocessor_filename = f"data_preprocessor_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib"
    preprocessor_path = config.MODELS_DIR / preprocessor_filename
    preprocessor.save_state(preprocessor_path)
    save_pipeline_metadata(preprocessor_path, saved_model_paths, trainer)
    
    if visualize:
        # Váº½ biá»ƒu Ä‘á»“ so sÃ¡nh
        log_step("Váº½ biá»ƒu Ä‘á»“ so sÃ¡nh", icon="ğŸ“ˆ")
        trainer.plot_comparison(metric='test_r2', save=True)
        trainer.plot_comparison(metric='test_rmse', save=True)
        trainer.plot_comparison(metric='test_mae', save=True)
        
        # Váº½ biá»ƒu Ä‘á»“ predictions
        log_step("Váº½ biá»ƒu Ä‘á»“ predictions", icon="ğŸ“ˆ")
        trainer.plot_all_predictions(save=True)
        
        # Váº½ feature importance
        log_step("Váº½ biá»ƒu Ä‘á»“ feature importance", icon="ğŸ“ˆ")
        trainer.plot_all_feature_importance(top_n=15, save=True)
        
        # So sÃ¡nh feature importance
        log_step("So sÃ¡nh feature importance", icon="ğŸ“ˆ")
        trainer.compare_feature_importance(top_n=10, save=True)
    
    # TÃ¬m mÃ´ hÃ¬nh tá»‘t nháº¥t
    best_name, best_result = trainer.get_best_model()
    
    log_section("MÃ” HÃŒNH Tá»T NHáº¤T", icon="âœ¨")
    log_key_value("Model", best_name.upper())
    log_key_value("Test RÂ²", f"{best_result['test_r2']:.6f}")
    log_key_value("Test RMSE", f"{best_result['test_rmse']:.6f}")
    log_key_value("Test MAE", f"{best_result['test_mae']:.6f}")


def save_pipeline_metadata(preprocessor_path: Path, model_paths: Dict[str, str], trainer: ModelTrainer) -> Path:
    """Ghi láº¡i tráº¡ng thÃ¡i pipeline Ä‘á»ƒ phá»¥c vá»¥ inference sau nÃ y."""
    best_name, best_result = trainer.get_best_model()
    metadata = {
        "generated_at": datetime.utcnow().isoformat() + "Z",
        "preprocessor": {
            "path": str(preprocessor_path.resolve()),
            "target_column": config.TARGET_COLUMN,
            "n_features": trainer.data_info["n_features"],
        },
        "models": {},
        "best_model": best_name,
        "best_model_path": model_paths.get(best_name),
    }
    for model_name, result in trainer.results.items():
        metadata["models"][model_name] = {
            "path": model_paths.get(model_name),
            "metrics": {
                "train_rmse": float(result["train_rmse"]),
                "test_rmse": float(result["test_rmse"]),
                "test_mae": float(result["test_mae"]),
                "test_r2": float(result["test_r2"]),
            },
            "hyperparams": result["hyperparams"],
        }
    if best_name and best_result:
        metadata["best_metrics"] = {
            "test_rmse": float(best_result["test_rmse"]),
            "test_mae": float(best_result["test_mae"]),
            "test_r2": float(best_result["test_r2"]),
        }
    path = config.RESULTS_DIR / "pipeline_state.json"
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as fp:
        json.dump(metadata, fp, indent=4, ensure_ascii=False)
    logger.info("âœ… ÄÃ£ lÆ°u pipeline metadata: %s", path)
    return path


def main():
    """Main function Ä‘á»ƒ cháº¡y toÃ n bá»™ pipeline."""
    # Parse arguments
    parser = argparse.ArgumentParser(description='Taxi Price Prediction Pipeline')
    parser.add_argument('--optimize', action='store_true', 
                       help='Cháº¡y optimization vá»›i Optuna')
    parser.add_argument('--no-viz', action='store_true',
                       help='KhÃ´ng váº½ biá»ƒu Ä‘á»“ visualization')
    parser.add_argument('--skip-download', action='store_true',
                       help='Bá» qua bÆ°á»›c download dá»¯ liá»‡u')
    
    args = parser.parse_args()
    
    log_section("Báº®T Äáº¦U PIPELINE Dá»° ÄOÃN GIÃ TAXI", icon="ğŸš€")
    
    stage_times = []
    pipeline_start = time.perf_counter()

    try:
        # BÆ°á»›c 0: Download dá»¯ liá»‡u
        if not args.skip_download:
            step_start = time.perf_counter()
            download_data()
            stage_times.append(("download", time.perf_counter() - step_start))
        
        # BÆ°á»›c 1: Tiá»n xá»­ lÃ½
        step_start = time.perf_counter()
        X_train, X_test, y_train, y_test, poly_feature_subset, preprocessor = preprocess_data(generate_viz=not args.no_viz)
        stage_times.append(("preprocess", time.perf_counter() - step_start))
        
        # BÆ°á»›c 2: Training
        step_start = time.perf_counter()
        trainer = train_models(
            X_train,
            X_test,
            y_train,
            y_test,
            optimize=args.optimize,
            poly_feature_subset=poly_feature_subset
        )
        stage_times.append(("train", time.perf_counter() - step_start))
        
        # BÆ°á»›c 3: ÄÃ¡nh giÃ¡ vÃ  visualization
        step_start = time.perf_counter()
        evaluate_and_visualize(trainer, preprocessor, visualize=not args.no_viz)
        stage_times.append(("evaluate", time.perf_counter() - step_start))
        
        log_section("HOÃ€N Táº¤T PIPELINE", icon="âœ…")
        log_step(f"MÃ´ hÃ¬nh Ä‘Ã£ lÆ°u táº¡i: {config.MODELS_DIR}", icon="ğŸ“")
        log_step(f"Káº¿t quáº£ Ä‘Ã£ lÆ°u táº¡i: {config.RESULTS_DIR}", icon="ğŸ“")

        total_duration = time.perf_counter() - pipeline_start
        logger.info("")
        log_stage_summary(stage_times, total_duration)
        
    except Exception as e:
        logger.error(f"\nâŒ Lá»–I: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
