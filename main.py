"""
Script chÃ­nh Ä‘á»ƒ cháº¡y toÃ n bá»™ pipeline:
1. Download vÃ  load dá»¯ liá»‡u
2. Tiá»n xá»­ lÃ½ dá»¯ liá»‡u (DataPreprocessor)
3. Training vÃ  tá»‘i Æ°u mÃ´ hÃ¬nh (ModelTrainer)
4. ÄÃ¡nh giÃ¡ vÃ  visualization
5. LÆ°u mÃ´ hÃ¬nh vÃ  káº¿t quáº£

CÃ¡ch cháº¡y:
    python main.py
    
    hoáº·c vá»›i tÃ¹y chá»n:
    python main.py --optimize    # Cháº¡y optimization vá»›i Optuna
    python main.py --no-viz      # KhÃ´ng váº½ biá»ƒu Ä‘á»“
"""

import argparse
import importlib
import json
import logging
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import pandas as pd
from sklearn.model_selection import train_test_split

# Import tá»« project
from src.preprocessing import DataLoader, DataTransformer
from src.modeling import ModelTrainer
from src.modeling.base_trainer import log_section, log_step
import config


# Cáº¥u hÃ¬nh logging (ghi Ä‘Ã¨ file log má»—i láº§n cháº¡y & format rÃµ rÃ ng)
config.LOG_FILE.parent.mkdir(parents=True, exist_ok=True)
LOG_FORMAT = "%(asctime)s | %(levelname)-8s | %(name)s | %(message)s"
CONSOLE_FORMAT = "%(levelname)-8s | %(message)s"
DATE_FORMAT = "%Y-%m-%d %H:%M:%S"

file_handler = logging.FileHandler(config.LOG_FILE, mode='w', encoding='utf-8')
file_handler.setFormatter(logging.Formatter(LOG_FORMAT, DATE_FORMAT))

# Console handler vá»›i encoding UTF-8 Ä‘á»ƒ há»— trá»£ emoji trÃªn Windows
console_handler = logging.StreamHandler(
    open(sys.stdout.fileno(), mode='w', encoding='utf-8', buffering=1)
)
console_handler.setFormatter(logging.Formatter(CONSOLE_FORMAT, DATE_FORMAT))

logging.basicConfig(
    level=getattr(logging, config.LOG_LEVEL),
    handlers=[file_handler, console_handler],
    force=True,
)
logger = logging.getLogger(__name__)


def log_key_value(label: str, value) -> None:
    """Align key-value summary rows for faster scanning."""
    logger.info("   %-18s: %s", label, value)


def log_stage_summary(stage_times: List[Tuple[str, float]], total_duration: float) -> None:
    """Pretty print per-stage durations and pipeline total."""
    logger.info("â±ï¸ PIPELINE SUMMARY")
    for stage, duration in stage_times:
        log_key_value(stage.capitalize(), f"{duration:.2f} giÃ¢y")
    log_key_value("Total", f"{total_duration:.2f} giÃ¢y")


def _ensure_gdown() -> Optional[object]:
    """Ensure gdown is importable without forcing installs every run."""
    try:
        return importlib.import_module("gdown")
    except ImportError:
        logger.info("gdown chÆ°a Ä‘Æ°á»£c cÃ i. Äang tiáº¿n hÃ nh cÃ i Ä‘áº·t má»™t láº§n...")
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", "gdown"])
        except subprocess.CalledProcessError as exc:
            logger.warning("KhÃ´ng thá»ƒ cÃ i Ä‘áº·t gdown tá»± Ä‘á»™ng: %s", exc)
            return None
        try:
            return importlib.import_module("gdown")
        except ImportError:
            return None


def download_data():
    """Download dá»¯ liá»‡u tá»« Google Drive náº¿u chÆ°a cÃ³."""
    if config.DATA_FILE.exists():
        log_step(f"Dá»¯ liá»‡u Ä‘Ã£ tá»“n táº¡i: {config.DATA_FILE}", icon="âœ…")
        return

    log_step("Äang download dá»¯ liá»‡u tá»« Google Drive (náº¿u kháº£ dá»¥ng)...", icon="ğŸ“¥")
    gdown = _ensure_gdown()
    if gdown is None:
        logger.warning("KhÃ´ng thá»ƒ import gdown. Vui lÃ²ng táº£i thá»§ cÃ´ng file taxi_price.csv vÃ  Ä‘áº·t vÃ o %s", config.DATA_DIR)
        if not config.DATA_FILE.exists():
            raise FileNotFoundError(
                "KhÃ´ng táº£i Ä‘Æ°á»£c dá»¯ liá»‡u tá»± Ä‘á»™ng vÃ¬ thiáº¿u gdown. HÃ£y Ä‘áº·t file taxi_price.csv vÃ o thÆ° má»¥c data vÃ  cháº¡y láº¡i."
            )
        return

    try:
        gdown.download(id=config.GDRIVE_FILE_ID, output=str(config.DATA_FILE), quiet=False)
        log_step(f"ÄÃ£ download dá»¯ liá»‡u vÃ o: {config.DATA_FILE}", icon="âœ…")
    except Exception as exc:
        logger.error("âŒ Lá»—i khi download dá»¯ liá»‡u: %s", exc)
        logger.info("ğŸ’¡ Vui lÃ²ng download thá»§ cÃ´ng vÃ  Ä‘áº·t vÃ o thÆ° má»¥c data/")

    if not config.DATA_FILE.exists():
        raise FileNotFoundError(
            "KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u taxi_price.csv. Download tá»± Ä‘á»™ng tháº¥t báº¡i, vui lÃ²ng táº£i thá»§ cÃ´ng vÃ  cháº¡y láº¡i."
        )


def preprocess_data(
    generate_viz: bool = True,
) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series, Optional[List[str]], DataTransformer]:
    """
    Load raw data, clean it, and return processed train/test splits.
    
    Quy trÃ¬nh:
    1. PRE-SPLIT: Sá»­ dá»¥ng DataLoader (xÃ³a duplicates, unify text, apply constraints)
    2. CHIA TRAIN/TEST
    3. POST-SPLIT: Sá»­ dá»¥ng DataTransformer (fit_transform trÃªn train, transform trÃªn test)
    """
    log_section("BÆ¯á»šC 1: TIá»€N Xá»¬ LÃ Dá»® LIá»†U", icon="ğŸ§¼")

    if not config.DATA_FILE.exists():
        raise FileNotFoundError(f"KhÃ´ng tÃ¬m tháº¥y file dá»¯ liá»‡u: {config.DATA_FILE}")

    # ========================================================================
    # PHASE 1: LOAD & PRE-SPLIT CLEANING (sá»­ dá»¥ng DataLoader)
    # ========================================================================
    log_step("Äang náº¡p vÃ  xá»­ lÃ½ dá»¯ liá»‡u gá»‘c (DataLoader)", icon="ğŸ“¥")
    
    loader = DataLoader.from_file(config.DATA_FILE)
    loader.drop_duplicates()
    loader.unify_values()
    loader.apply_constraints(constraint_rules=config.CONSTRAINT_RULES)
    
    raw_df = loader.get_data()
    log_key_value("After pre-split cleaning", raw_df.shape)

    # EDA trÃªn dá»¯ liá»‡u gá»‘c
    if generate_viz:
        log_step("Äang táº¡o cÃ¡c biá»ƒu Ä‘á»“ EDA", icon="ğŸ–¼ï¸")
        loader.generate_eda_report(target_col=config.TARGET_COLUMN)

    # ========================================================================
    # PHASE 2: CHIA TRAIN/TEST
    # ========================================================================
    log_step("Chia train/test", icon="âœ‚ï¸")
    train_df, test_df = train_test_split(
        raw_df,
        test_size=config.TEST_SIZE,
        random_state=config.RANDOM_SEED,
        shuffle=True,
    )
    train_df = train_df.reset_index(drop=True)
    test_df = test_df.reset_index(drop=True)
    log_key_value("Train shape", train_df.shape)
    log_key_value("Test shape", test_df.shape)

    # ========================================================================
    # PHASE 3: FIT_TRANSFORM trÃªn TRAIN (sá»­ dá»¥ng DataTransformer)
    # ========================================================================
    log_step("Xá»­ lÃ½ TRAIN set (DataTransformer.fit_transform)", icon="ğŸ”§")
    
    transformer = DataTransformer(
        data=train_df.copy(),
        missing_strategy=config.MISSING_STRATEGY["numeric"],
        categorical_missing_strategy=config.MISSING_STRATEGY["categorical"],
        scaler_type=config.SCALING_METHOD,
        encoder_type=config.ENCODING_METHOD,
    )

    train_processed = transformer.fit_transform(
        target_col=config.TARGET_COLUMN,
        remove_outliers=config.OUTLIER_DETECTION,
        outlier_method=config.OUTLIER_METHOD,
        outlier_threshold=config.OUTLIER_THRESHOLD,
        encoding_method=config.ENCODING_METHOD,
        drop_first_onehot=config.DROP_FIRST_ONEHOT,
        scaling_method=config.SCALING_METHOD,
        interaction_pairs=config.INTERACTION_PAIRS if config.CREATE_INTERACTION_FEATURES else None,
    )

    # Chá»n features cho Polynomial Regression (dÃ¹ng method cÃ³ sáºµn)
    poly_feature_subset = transformer.get_correlated_features(
        target_col=config.TARGET_COLUMN,
        threshold=config.POLY_CORRELATION_THRESHOLD,
        method='spearman'
    ) or None

    transformer.print_summary()

    # ========================================================================
    # PHASE 4: TRANSFORM TEST (sá»­ dá»¥ng transformer Ä‘Ã£ fit)
    # ========================================================================
    log_step("Transform TEST set (DataTransformer.transform_new_data)", icon="ğŸ”„")
    test_processed = transformer.transform_new_data(test_df)
    log_key_value("Train processed shape", train_processed.shape)
    log_key_value("Test processed shape", test_processed.shape)

    # LÆ°u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
    combined = pd.concat([
        train_processed.assign(split='train'),
        test_processed.assign(split='test')
    ], ignore_index=True)
    combined.to_csv(config.PROCESSED_DATA_FILE, index=False)
    log_step(f"ÄÃ£ lÆ°u dá»¯ liá»‡u táº¡i: {config.PROCESSED_DATA_FILE}", icon="ğŸ’¾")

    # TÃ¡ch X vÃ  y (dÃ¹ng method cÃ³ sáºµn cho train, pandas cho test)
    X_train, y_train = transformer.split_features_target(config.TARGET_COLUMN)
    X_test = test_processed.drop(columns=[config.TARGET_COLUMN])
    y_test = test_processed[config.TARGET_COLUMN]

    return (X_train, X_test, y_train, y_test, poly_feature_subset, transformer)


def train_models(
    X_train: pd.DataFrame,
    X_test: pd.DataFrame,
    y_train: pd.Series,
    y_test: pd.Series,
    optimize: bool = False,
    poly_feature_subset: Optional[List[str]] = None,
) -> ModelTrainer:
    """Huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh há»c mÃ¡y."""
    log_section("BÆ¯á»šC 2: HUáº¤N LUYá»†N MÃ” HÃŒNH", icon="ğŸ¤–")
    
    trainer = ModelTrainer(
        X_train=X_train,
        X_test=X_test,
        y_train=y_train,
        y_test=y_test,
        output_dir=str(config.MODELS_DIR)
    )
    
    # Sá»­ dá»¥ng train_all() Ä‘á»ƒ huáº¥n luyá»‡n táº¥t cáº£ models
    trainer.train_all(
        optimize=optimize,
        poly_feature_subset=poly_feature_subset,
        hyperparams=config.DEFAULT_HYPERPARAMS,
        optuna_config={
            'n_trials': config.OPTUNA_N_TRIALS,
            'timeout': config.OPTUNA_TIMEOUT,
        }
    )
    
    return trainer


def evaluate_and_visualize(trainer: ModelTrainer, transformer: DataTransformer, visualize: bool = True) -> None:
    """
    ÄÃ¡nh giÃ¡ vÃ  visualization káº¿t quáº£.
    
    Args:
        trainer: ModelTrainer instance
        transformer: DataTransformer instance (Ä‘Ã£ fit)
        visualize: CÃ³ váº½ biá»ƒu Ä‘á»“ khÃ´ng
    """
    log_section("BÆ¯á»šC 3: ÄÃNH GIÃ & VISUALIZATION", icon="ğŸ“Š")
    
    # In tÃ³m táº¯t káº¿t quáº£
    trainer.summary()
    
    # LÆ°u káº¿t quáº£
    trainer.save_results(config.RESULTS_FILE)
    
    # LÆ°u toÃ n bá»™ mÃ´ hÃ¬nh vÃ  cáº¥u hÃ¬nh tiá»n xá»­ lÃ½
    saved_model_paths = trainer.save_all_models(format=config.MODEL_FORMAT)
    transformer_path = config.MODELS_DIR / "data_transformer.joblib"
    transformer.save_state(transformer_path)
    save_pipeline_metadata(transformer_path, saved_model_paths, trainer)
    
    if visualize:
        # Váº½ biá»ƒu Ä‘á»“ metrics tá»•ng há»£p (RÂ², RMSE, MAE trong 1 hÃ¬nh)
        log_step("Váº½ biá»ƒu Ä‘á»“ metrics tá»•ng há»£p", icon="ğŸ“ˆ")
        trainer.plot_metrics_summary(save=True)
        
        # Váº½ biá»ƒu Ä‘á»“ predictions tá»•ng há»£p (táº¥t cáº£ models trong 1 hÃ¬nh)
        log_step("Váº½ biá»ƒu Ä‘á»“ predictions tá»•ng há»£p", icon="ğŸ“ˆ")
        trainer.plot_combined_predictions(save=True)
        
        # Váº½ feature importance so sÃ¡nh (1 hÃ¬nh cho táº¥t cáº£ tree-based models)
        log_step("Váº½ biá»ƒu Ä‘á»“ feature importance so sÃ¡nh", icon="ğŸ“ˆ")
        trainer.compare_feature_importance(top_n=10, save=True)
    
    # TÃ¬m mÃ´ hÃ¬nh tá»‘t nháº¥t
    best_name, best_result = trainer.get_best_model()
    
    log_section("MÃ” HÃŒNH Tá»T NHáº¤T", icon="âœ¨")
    log_key_value("Model", best_name.upper())
    log_key_value("Test RÂ²", f"{best_result['test_r2']:.6f}")
    log_key_value("Test RMSE", f"{best_result['test_rmse']:.6f}")
    log_key_value("Test MAE", f"{best_result['test_mae']:.6f}")


def save_pipeline_metadata(transformer_path: Path, model_paths: Dict[str, str], trainer: ModelTrainer) -> Path:
    """Ghi láº¡i tráº¡ng thÃ¡i pipeline Ä‘á»ƒ phá»¥c vá»¥ inference sau nÃ y."""
    best_name, best_result = trainer.get_best_model()
    metadata = {
        "generated_at": datetime.now().astimezone().isoformat(),
        "transformer": {
            "path": str(transformer_path.resolve()),
            "target_column": config.TARGET_COLUMN,
            "n_features": trainer.data_info["n_features"],
        },
        "models": {},
        "best_model": best_name,
        "best_model_path": model_paths.get(best_name),
    }
    for model_name, result in trainer.results.items():
        metadata["models"][model_name] = {
            "path": model_paths.get(model_name),
            "metrics": {
                "train_rmse": float(result["train_rmse"]),
                "test_rmse": float(result["test_rmse"]),
                "test_mae": float(result["test_mae"]),
                "test_r2": float(result["test_r2"]),
            },
            "hyperparams": result["hyperparams"],
        }
    if best_name and best_result:
        metadata["best_metrics"] = {
            "test_rmse": float(best_result["test_rmse"]),
            "test_mae": float(best_result["test_mae"]),
            "test_r2": float(best_result["test_r2"]),
        }
    path = config.RESULTS_DIR / "pipeline_state.json"
    path.parent.mkdir(parents=True, exist_ok=True)
    with open(path, "w", encoding="utf-8") as fp:
        json.dump(metadata, fp, indent=4, ensure_ascii=False)
    logger.info("âœ… ÄÃ£ lÆ°u pipeline metadata: %s", path)
    return path


def main():
    """Main function Ä‘á»ƒ cháº¡y toÃ n bá»™ pipeline."""
    # Parse arguments
    parser = argparse.ArgumentParser(description='Taxi Price Prediction Pipeline')
    parser.add_argument('--optimize', action='store_true', 
                       help='Cháº¡y optimization vá»›i Optuna')
    parser.add_argument('--no-viz', action='store_true',
                       help='KhÃ´ng váº½ biá»ƒu Ä‘á»“ visualization')
    parser.add_argument('--skip-download', action='store_true',
                       help='Bá» qua bÆ°á»›c download dá»¯ liá»‡u')
    
    args = parser.parse_args()
    
    log_section("Báº®T Äáº¦U PIPELINE Dá»° ÄOÃN GIÃ TAXI", icon="ğŸš€")
    
    stage_times = []
    pipeline_start = time.perf_counter()

    try:
        # BÆ°á»›c 0: Download dá»¯ liá»‡u
        if not args.skip_download:
            step_start = time.perf_counter()
            download_data()
            stage_times.append(("download", time.perf_counter() - step_start))
        
        # BÆ°á»›c 1: Tiá»n xá»­ lÃ½
        step_start = time.perf_counter()
        (X_train, X_test, y_train, y_test, 
         poly_feature_subset, transformer) = preprocess_data(generate_viz=not args.no_viz)
        stage_times.append(("preprocess", time.perf_counter() - step_start))
        
        # BÆ°á»›c 2: Training
        step_start = time.perf_counter()
        trainer = train_models(
            X_train,
            X_test,
            y_train,
            y_test,
            optimize=args.optimize,
            poly_feature_subset=poly_feature_subset
        )
        stage_times.append(("train", time.perf_counter() - step_start))
        
        # BÆ°á»›c 3: ÄÃ¡nh giÃ¡ vÃ  visualization
        step_start = time.perf_counter()
        evaluate_and_visualize(trainer, transformer, visualize=not args.no_viz)
        stage_times.append(("evaluate", time.perf_counter() - step_start))
        
        log_section("HOÃ€N Táº¤T PIPELINE", icon="âœ…")
        log_step(f"MÃ´ hÃ¬nh Ä‘Ã£ lÆ°u táº¡i: {config.MODELS_DIR}", icon="ğŸ“")
        log_step(f"Káº¿t quáº£ Ä‘Ã£ lÆ°u táº¡i: {config.RESULTS_DIR}", icon="ğŸ“")

        total_duration = time.perf_counter() - pipeline_start
        logger.info("")
        log_stage_summary(stage_times, total_duration)
        
    except Exception as e:
        logger.error(f"\nâŒ Lá»–I: {e}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()
