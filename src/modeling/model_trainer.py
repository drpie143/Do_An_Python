"""
Model Trainer - Orchestrator cho viá»‡c huáº¥n luyá»‡n nhiá»u mÃ´ hÃ¬nh.

ÄÃ¢y lÃ  lá»›p chÃ­nh Ä‘á»ƒ sá»­ dá»¥ng, cung cáº¥p interface giá»‘ng nhÆ° file gá»‘c nhÆ°ng
delegate cÃ´ng viá»‡c sang cÃ¡c trainer cá»¥ thá»ƒ trong model_registry.

Há»— trá»£:
- train_all(): Huáº¥n luyá»‡n táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh
- Tá»‘i Æ°u hyperparameters vá»›i Optuna
- Save/Load mÃ´ hÃ¬nh
- Visualization vÃ  comparison
"""

import json
import pickle
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any

import numpy as np
import pandas as pd
import joblib

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from src.modeling.base_trainer import log_section, log_step, log_metrics, _divider
from src.modeling.model_registry import (
    PolynomialTrainer, RandomForestTrainer, ExtraTreesTrainer, XGBoostTrainer,
    get_trainer, TRAINER_REGISTRY
)
from src.visualization import DataVisualizer
from config import MODEL_RESULTS_DIR, MODELS_DIR, PLOT_DPI, PLOT_STYLE, FIGURE_SIZE


logger = logging.getLogger(__name__)


class ModelTrainer:
    """
    Orchestrator cho viá»‡c huáº¥n luyá»‡n vÃ  quáº£n lÃ½ nhiá»u mÃ´ hÃ¬nh há»c mÃ¡y.
    
    Há»— trá»£:
    - 4 mÃ´ hÃ¬nh: Polynomial Regression, Random Forest, Extra Trees, XGBoost
    - Tá»‘i Æ°u hyperparameters báº±ng Optuna
    - Logging quÃ¡ trÃ¬nh huáº¥n luyá»‡n
    - LÆ°u/táº£i mÃ´ hÃ¬nh
    - ÄÃ¡nh giÃ¡ vÃ  so sÃ¡nh káº¿t quáº£
    - Visualization
    
    Attributes:
        X_train, X_test: Features cá»§a train/test
        y_train, y_test: Target cá»§a train/test
        models: Dictionary lÆ°u cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ train
        best_model: MÃ´ hÃ¬nh tá»‘t nháº¥t
        results: LÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡
    """
    
    RANDOM_SEED = 42
    
    def __init__(self, 
                 X_train: pd.DataFrame, 
                 X_test: pd.DataFrame,
                 y_train: pd.Series,
                 y_test: pd.Series,
                 output_dir: str = "./models"):
        """
        Khá»Ÿi táº¡o ModelTrainer.
        
        Args:
            X_train, X_test: Features
            y_train, y_test: Target
            output_dir: ThÆ° má»¥c lÆ°u káº¿t quáº£
        """
        self.X_train = X_train.copy()
        self.X_test = X_test.copy()
        self.y_train = y_train.copy()
        self.y_test = y_test.copy()
        
        self.output_dir = Path(output_dir) if output_dir else MODELS_DIR
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Khá»Ÿi táº¡o cÃ¡c trainers
        self._trainers: Dict[str, Any] = {}
        
        # Káº¿t quáº£ tá»•ng há»£p
        self.models = {}
        self.X_train_transformed = {}
        self.X_test_transformed = {}
        self.best_model = None
        self.best_model_name = None
        self.results = {}
        self.optimization_history = {}
        
        # Visualizer
        self.visualizer = DataVisualizer(
            output_dir=MODEL_RESULTS_DIR,
            auto_save=True,
            auto_show=False,
            dpi=PLOT_DPI,
            style=PLOT_STYLE,
            figure_size=FIGURE_SIZE,
        )
        
        np.random.seed(self.RANDOM_SEED)
        
        log_section("MODELTRAINER KHá»I Táº O", icon="âš™ï¸")
        log_step("Khá»Ÿi táº¡o thÃ nh cÃ´ng", icon="âœ…")
        log_step(f"Train: {self.X_train.shape}, Test: {self.X_test.shape}", icon="ğŸ“Š")
    
    @property
    def data_info(self) -> Dict[str, Any]:
        """Tráº£ vá» thÃ´ng tin dá»¯ liá»‡u."""
        return {
            'train_shape': self.X_train.shape,
            'test_shape': self.X_test.shape,
            'n_features': self.X_train.shape[1],
            'n_samples_train': self.X_train.shape[0],
            'n_samples_test': self.X_test.shape[0]
        }
    
    def _get_trainer(self, name: str):
        """Láº¥y hoáº·c táº¡o trainer theo tÃªn."""
        if name not in self._trainers:
            self._trainers[name] = get_trainer(
                name, self.X_train, self.X_test, self.y_train, self.y_test
            )
        return self._trainers[name]
    
    def _sync_trainer_results(self, name: str, trainer) -> None:
        """Äá»“ng bá»™ káº¿t quáº£ tá»« trainer vá» ModelTrainer."""
        self.models[name] = trainer.get_model_data()
        self.results[name] = trainer.get_result()
        self.optimization_history[name] = trainer.optimization_history
        
        # Xá»­ lÃ½ Ä‘áº·c biá»‡t cho Polynomial
        if name == 'polynomial' and hasattr(trainer, 'X_train_transformed'):
            self.X_train_transformed[name] = trainer.X_train_transformed
            self.X_test_transformed[name] = trainer.X_test_transformed
    
    # ========== POLYNOMIAL REGRESSION ==========
    def optimize_polynomial(self, n_trials: int = 10, timeout: int = 300) -> Dict:
        """Tá»‘i Æ°u hyperparameters cho Polynomial Regression."""
        trainer = self._get_trainer('polynomial')
        return trainer.optimize(n_trials=n_trials, timeout=timeout)
    
    def train_polynomial(self, degree: int = 3, alpha: float = 1.0,
                         feature_subset: Optional[List[str]] = None) -> None:
        """Huáº¥n luyá»‡n Polynomial Regression."""
        trainer = self._get_trainer('polynomial')
        trainer.train(degree=degree, alpha=alpha, feature_subset=feature_subset)
        self._sync_trainer_results('polynomial', trainer)
    
    # ========== RANDOM FOREST ==========
    def optimize_rf(self, n_trials: int = 20, timeout: int = 600) -> Dict:
        """Tá»‘i Æ°u hyperparameters cho Random Forest."""
        trainer = self._get_trainer('random_forest')
        return trainer.optimize(n_trials=n_trials, timeout=timeout)
    
    def train_rf(self, n_estimators: int = 100, max_depth: int = 10,
                 min_samples_split: int = 5, min_samples_leaf: int = 2) -> None:
        """Huáº¥n luyá»‡n Random Forest."""
        trainer = self._get_trainer('random_forest')
        trainer.train(
            n_estimators=n_estimators,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf
        )
        self._sync_trainer_results('random_forest', trainer)
    
    # ========== EXTRA TREES ==========
    def optimize_extra_trees(self, n_trials: int = 20, timeout: int = 600) -> Dict:
        """Tá»‘i Æ°u hyperparameters cho Extra Trees."""
        trainer = self._get_trainer('extra_trees')
        return trainer.optimize(n_trials=n_trials, timeout=timeout)
    
    def train_extra_trees(self, n_estimators: int = 200, max_depth: int = 12,
                          min_samples_split: int = 2, min_samples_leaf: int = 1) -> None:
        """Huáº¥n luyá»‡n Extra Trees."""
        trainer = self._get_trainer('extra_trees')
        trainer.train(
            n_estimators=n_estimators,
            max_depth=max_depth,
            min_samples_split=min_samples_split,
            min_samples_leaf=min_samples_leaf
        )
        self._sync_trainer_results('extra_trees', trainer)
    
    # ========== XGBOOST ==========
    def optimize_xgb(self, n_trials: int = 30, timeout: int = 900) -> Dict:
        """Tá»‘i Æ°u hyperparameters cho XGBoost."""
        trainer = self._get_trainer('xgboost')
        return trainer.optimize(n_trials=n_trials, timeout=timeout)
    
    def train_xgb(self, **xgb_params) -> None:
        """Huáº¥n luyá»‡n XGBoost."""
        trainer = self._get_trainer('xgboost')
        trainer.train(**xgb_params)
        self._sync_trainer_results('xgboost', trainer)
    
    # ========== TRAIN ALL ==========
    def train_all(
        self,
        optimize: bool = False,
        poly_feature_subset: Optional[List[str]] = None,
        hyperparams: Optional[Dict[str, Dict]] = None,
        optuna_config: Optional[Dict[str, Dict]] = None,
    ) -> "ModelTrainer":
        """
        Huáº¥n luyá»‡n táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh.
        
        Args:
            optimize: CÃ³ tá»‘i Æ°u hyperparameters vá»›i Optuna khÃ´ng
            poly_feature_subset: Danh sÃ¡ch features cho Polynomial Regression
            hyperparams: Dict chá»©a hyperparameters máº·c Ä‘á»‹nh cho tá»«ng model
            optuna_config: Dict chá»©a cáº¥u hÃ¬nh Optuna (n_trials, timeout)
            
        Returns:
            self Ä‘á»ƒ cÃ³ thá»ƒ chain methods
        """
        log_section("HUáº¤N LUYá»†N Táº¤T Cáº¢ MÃ” HÃŒNH", icon="ğŸ¤–")
        
        # Default hyperparams
        default_hyperparams = {
            'polynomial': {'degree': 3, 'alpha': 1.0},
            'random_forest': {'n_estimators': 100, 'max_depth': 10, 'min_samples_split': 5, 'min_samples_leaf': 2},
            'extra_trees': {'n_estimators': 200, 'max_depth': 12, 'min_samples_split': 2, 'min_samples_leaf': 1},
            'xgboost': {'max_depth': 4, 'learning_rate': 0.05, 'n_estimators': 150, 'subsample': 0.7},
        }
        if hyperparams:
            for key in hyperparams:
                default_hyperparams[key] = hyperparams[key]
        
        # Default Optuna config
        default_optuna = {
            'n_trials': {'polynomial': 10, 'random_forest': 20, 'extra_trees': 20, 'xgboost': 30},
            'timeout': {'polynomial': 300, 'random_forest': 600, 'extra_trees': 600, 'xgboost': 900},
        }
        if optuna_config:
            for key in optuna_config:
                default_optuna[key] = optuna_config[key]
        
        # 1. Polynomial Regression
        if optimize:
            log_step("Tá»‘i Æ°u Polynomial Regression", icon="ğŸ”")
            best_poly = self.optimize_polynomial(
                n_trials=default_optuna['n_trials']['polynomial'],
                timeout=default_optuna['timeout']['polynomial']
            )
            self.train_polynomial(
                degree=best_poly.get('degree', default_hyperparams['polynomial']['degree']),
                alpha=best_poly.get('alpha', default_hyperparams['polynomial']['alpha']),
                feature_subset=poly_feature_subset
            )
        else:
            self.train_polynomial(
                degree=default_hyperparams['polynomial']['degree'],
                alpha=default_hyperparams['polynomial']['alpha'],
                feature_subset=poly_feature_subset
            )
        
        # 2. Random Forest
        if optimize:
            log_step("Tá»‘i Æ°u Random Forest", icon="ğŸ”")
            best_rf = self.optimize_rf(
                n_trials=default_optuna['n_trials']['random_forest'],
                timeout=default_optuna['timeout']['random_forest']
            )
            self.train_rf(**best_rf)
        else:
            self.train_rf(**default_hyperparams['random_forest'])
        
        # 3. Extra Trees
        if optimize:
            log_step("Tá»‘i Æ°u Extra Trees", icon="ğŸ”")
            best_et = self.optimize_extra_trees(
                n_trials=default_optuna['n_trials']['extra_trees'],
                timeout=default_optuna['timeout']['extra_trees']
            )
            self.train_extra_trees(**best_et)
        else:
            self.train_extra_trees(**default_hyperparams['extra_trees'])
        
        # 4. XGBoost
        if optimize:
            log_step("Tá»‘i Æ°u XGBoost", icon="ğŸ”")
            best_xgb = self.optimize_xgb(
                n_trials=default_optuna['n_trials']['xgboost'],
                timeout=default_optuna['timeout']['xgboost']
            )
            self.train_xgb(**best_xgb)
        else:
            self.train_xgb(**default_hyperparams['xgboost'])
        
        log_step("ÄÃ£ hoÃ n táº¥t huáº¥n luyá»‡n táº¥t cáº£ mÃ´ hÃ¬nh!", icon="âœ…")
        return self
    
    # ========== SAVE & LOAD ==========
    def save_model(self, model_name: str, format: str = 'joblib', 
                   use_timestamp: bool = False) -> str:
        """
        LÆ°u mÃ´ hÃ¬nh vÃ o file.
        
        Args:
            model_name: TÃªn mÃ´ hÃ¬nh
            format: Äá»‹nh dáº¡ng ('joblib' hoáº·c 'pickle')
            use_timestamp: Náº¿u True thÃ¬ thÃªm timestamp vÃ o tÃªn file,
                          Náº¿u False thÃ¬ ghi Ä‘Ã¨ file cÅ© (máº·c Ä‘á»‹nh)
        """
        if model_name not in self.models:
            logger.error(f"âŒ KhÃ´ng tÃ¬m tháº¥y mÃ´ hÃ¬nh: {model_name}")
            return None
        
        # Chá»‰ thÃªm timestamp náº¿u Ä‘Æ°á»£c yÃªu cáº§u
        if use_timestamp:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = self.output_dir / f"{model_name}_{timestamp}.{format}"
        else:
            filename = self.output_dir / f"{model_name}.{format}"
        
        model_data = self.models[model_name]
        
        if format == 'joblib':
            joblib.dump(model_data, filename)
        elif format == 'pickle':
            with open(filename, 'wb') as f:
                pickle.dump(model_data, f)
        
        log_step(f"ÄÃ£ lÆ°u mÃ´ hÃ¬nh: {filename}", icon="ğŸ’¾")
        return str(filename)
    
    def load_model(self, filepath: str, model_name: str) -> None:
        """Táº£i mÃ´ hÃ¬nh tá»« file."""
        filepath = Path(filepath)
        
        if filepath.suffix == '.joblib':
            model_data = joblib.load(filepath)
        else:
            with open(filepath, 'rb') as f:
                model_data = pickle.load(f)
        
        self.models[model_name] = model_data
        log_step(f"ÄÃ£ táº£i mÃ´ hÃ¬nh: {filepath}", icon="ğŸ“‚")
    
    def save_all_models(self, format: str = 'joblib', 
                        use_timestamp: bool = False) -> Dict[str, str]:
        """
        LÆ°u táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh Ä‘Ã£ train.
        
        Args:
            format: Äá»‹nh dáº¡ng ('joblib' hoáº·c 'pickle')
            use_timestamp: Náº¿u True thÃ¬ thÃªm timestamp, False thÃ¬ ghi Ä‘Ã¨ (máº·c Ä‘á»‹nh)
        """
        log_section("LÆ¯U Táº¤T Cáº¢ CÃC MÃ” HÃŒNH", icon="ğŸ’¾")
        
        saved_paths: Dict[str, str] = {}
        for model_name in self.models.keys():
            path = self.save_model(model_name, format=format, use_timestamp=use_timestamp)
            if path:
                saved_paths[model_name] = path
        
        log_step(f"HoÃ n táº¥t lÆ°u {len(saved_paths)} mÃ´ hÃ¬nh!", icon="âœ…")
        return saved_paths
    
    # ========== EVALUATION ==========
    def get_best_model(self) -> Tuple[str, Dict]:
        """Láº¥y mÃ´ hÃ¬nh tá»‘t nháº¥t dá»±a trÃªn test RÂ²."""
        best_r2 = -np.inf
        best_name = None
        
        for name, result in self.results.items():
            if result['test_r2'] > best_r2:
                best_r2 = result['test_r2']
                best_name = name
        
        self.best_model_name = best_name
        if best_name:
            self.best_model = self.models[best_name]['model']
        
        return best_name, self.results[best_name] if best_name else None
    
    def save_results(self, filename: str = 'model_results.json') -> None:
        """LÆ°u káº¿t quáº£ Ä‘Ã¡nh giÃ¡ ra file JSON."""
        filepath = MODEL_RESULTS_DIR / filename
        filepath.parent.mkdir(parents=True, exist_ok=True)
        
        results_serializable = {}
        for model_name, result in self.results.items():
            results_serializable[model_name] = {
                'train_rmse': float(result['train_rmse']),
                'test_rmse': float(result['test_rmse']),
                'test_mae': float(result['test_mae']),
                'test_r2': float(result['test_r2']),
                'hyperparams': result['hyperparams']
            }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results_serializable, f, indent=4, ensure_ascii=False)
        
        log_step(f"ÄÃ£ lÆ°u káº¿t quáº£: {filepath}", icon="ğŸ’¾")
    
    def summary(self) -> None:
        """In ra tÃ³m táº¯t káº¿t quáº£ cÃ¡c mÃ´ hÃ¬nh."""
        log_section("TÃ“M Táº®T Káº¾T QUáº¢ TRAINING", icon="ğŸ“Š")
        
        summary_data = []
        for model_name, result in self.results.items():
            summary_data.append({
                'Model': model_name.upper(),
                'Train RMSE': f"{result['train_rmse']:.6f}",
                'Test RMSE': f"{result['test_rmse']:.6f}",
                'Test MAE': f"{result['test_mae']:.6f}",
                'Test RÂ²': f"{result['test_r2']:.6f}"
            })
        
        summary_df = pd.DataFrame(summary_data)
        print(summary_df.to_string(index=False))
        
        best_name, best_result = self.get_best_model()
        if best_name:
            log_section("MÃ” HÃŒNH Tá»T NHáº¤T", icon="âœ¨")
            log_step(f"Model: {best_name.upper()}", icon="ğŸ†")
            log_metrics({
                "Test RÂ²": best_result['test_r2'],
                "Test RMSE": best_result['test_rmse'],
                "Test MAE": best_result['test_mae']
            })
        
        logger.info("%s\n", _divider())
    
    # ========== PREDICTIONS ==========
    def predict(self, X: pd.DataFrame, model_name: Optional[str] = None) -> np.ndarray:
        """Dá»± Ä‘oÃ¡n vá»›i mÃ´ hÃ¬nh Ä‘Ã£ train."""
        if model_name is None:
            if self.best_model_name is None:
                self.get_best_model()
            model_name = self.best_model_name
        
        if model_name not in self.models:
            raise ValueError(f"MÃ´ hÃ¬nh {model_name} chÆ°a Ä‘Æ°á»£c train")
        
        model_obj = self.models[model_name]['model']
        
        # Xá»­ lÃ½ cho Polynomial
        if model_name == 'polynomial':
            poly = self.models[model_name]['poly']
            feature_subset = self.models[model_name].get('feature_subset')
            X_input = X[feature_subset] if feature_subset else X
            X_poly = poly.transform(X_input)
            return model_obj.predict(X_poly)
        
        return model_obj.predict(X)
    
    # ========== VISUALIZATION ==========
    def plot_comparison(self, metric: str = 'test_r2', save: bool = True) -> None:
        """Váº½ biá»ƒu Ä‘á»“ so sÃ¡nh cÃ¡c mÃ´ hÃ¬nh."""
        if not self.results:
            logger.warning("âŒ ChÆ°a cÃ³ káº¿t quáº£ Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“")
            return
        
        metric_values = {model: self.results[model][metric] for model in self.results}
        if not metric_values:
            logger.warning("âŒ KhÃ´ng cÃ³ káº¿t quáº£ Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“")
            return

        save_path = None
        if save:
            save_path = MODEL_RESULTS_DIR / f'comparison_{metric}.png'
        self.visualizer.plot_model_comparison(metric_values, metric, save_path=save_path, show=not save)
    
    def plot_predictions(self, model_name: str, save: bool = True) -> None:
        """Váº½ biá»ƒu Ä‘á»“ actual vs predicted."""
        if model_name not in self.models:
            logger.error(f"âŒ KhÃ´ng tÃ¬m tháº¥y mÃ´ hÃ¬nh: {model_name}")
            return
        
        model_obj = self.models[model_name]['model']
        
        if model_name == 'polynomial':
            X_test_pred = self.X_test_transformed['polynomial']
        else:
            X_test_pred = self.X_test
        
        y_pred = model_obj.predict(X_test_pred)
        
        save_path = None
        if save:
            save_path = MODEL_RESULTS_DIR / f'predictions_{model_name}.png'
        self.visualizer.plot_regression_diagnostics(
            y_true=self.y_test,
            y_pred=y_pred,
            model_name=model_name,
            save_path=save_path,
            show=not save,
        )
    
    def plot_all_predictions(self, save: bool = True) -> None:
        """Váº½ biá»ƒu Ä‘á»“ predictions cho táº¥t cáº£ cÃ¡c mÃ´ hÃ¬nh (riÃªng láº» - deprecated)."""
        log_section("Váº¼ BIá»‚U Äá»’ PREDICTIONS CHO Táº¤T Cáº¢ MÃ” HÃŒNH", icon="ğŸ“ˆ")
        
        for model_name in self.models.keys():
            self.plot_predictions(model_name, save=save)
    
    def plot_combined_predictions(self, save: bool = True) -> None:
        """Váº½ táº¥t cáº£ predictions trong 1 figure Ä‘á»ƒ so sÃ¡nh."""
        log_section("Váº¼ BIá»‚U Äá»’ PREDICTIONS Tá»”NG Há»¢P", icon="ğŸ“ˆ")
        
        predictions = {}
        for model_name in self.models.keys():
            model_obj = self.models[model_name]['model']
            
            if model_name == 'polynomial':
                X_test_pred = self.X_test_transformed.get('polynomial', self.X_test)
            else:
                X_test_pred = self.X_test
            
            y_pred = model_obj.predict(X_test_pred)
            predictions[model_name] = (self.y_test.values, y_pred)
        
        save_path = None
        if save:
            save_path = MODEL_RESULTS_DIR / 'predictions_combined.png'
        
        self.visualizer.plot_combined_predictions(
            predictions=predictions,
            save_path=save_path,
            show=not save,
        )
    
    def plot_metrics_summary(self, save: bool = True) -> None:
        """Váº½ biá»ƒu Ä‘á»“ tá»•ng há»£p táº¥t cáº£ metrics."""
        log_section("Váº¼ BIá»‚U Äá»’ METRICS Tá»”NG Há»¢P", icon="ğŸ“Š")
        
        if not self.results:
            logger.warning("âŒ ChÆ°a cÃ³ káº¿t quáº£ Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“")
            return
        
        save_path = None
        if save:
            save_path = MODEL_RESULTS_DIR / 'metrics_summary.png'
        
        self.visualizer.plot_metrics_summary(
            results=self.results,
            save_path=save_path,
            show=not save,
        )
    
    # ========== FEATURE IMPORTANCE ==========
    def get_feature_importance(self, model_name: str, top_n: int = 10) -> pd.DataFrame:
        """Láº¥y feature importance cá»§a mÃ´ hÃ¬nh."""
        if model_name not in self.models:
            logger.error(f"âŒ MÃ´ hÃ¬nh {model_name} chÆ°a Ä‘Æ°á»£c train")
            return None
        
        if model_name == 'polynomial':
            logger.warning("âš ï¸  Polynomial Regression khÃ´ng há»— trá»£ feature importance")
            return None
        
        model_obj = self.models[model_name]['model']
        
        if hasattr(model_obj, 'feature_importances_'):
            importances = model_obj.feature_importances_
            feature_names = self.X_train.columns
            
            importance_df = pd.DataFrame({
                'feature': feature_names,
                'importance': importances
            }).sort_values('importance', ascending=False).head(top_n)
            
            return importance_df
        else:
            logger.warning(f"âš ï¸  MÃ´ hÃ¬nh {model_name} khÃ´ng há»— trá»£ feature importance")
            return None
    
    def plot_feature_importance(self, model_name: str, top_n: int = 15, save: bool = True) -> None:
        """Váº½ biá»ƒu Ä‘á»“ feature importance."""
        importance_df = self.get_feature_importance(model_name, top_n=top_n)
        
        if importance_df is None:
            return
        
        logger.info(f"\nğŸ“Š Feature Importance - {model_name.upper()}")
        logger.info(f"{'='*70}")
        print(importance_df.to_string(index=False))
        
        save_path = None
        if save:
            save_path = MODEL_RESULTS_DIR / f'feature_importance_{model_name}.png'
        self.visualizer.plot_feature_importance(
            importance_df=importance_df,
            model_name=model_name,
            save_path=save_path,
            top_n=top_n,
            show=not save,
        )
    
    def plot_all_feature_importance(self, top_n: int = 15, save: bool = True) -> None:
        """Váº½ feature importance cho táº¥t cáº£ mÃ´ hÃ¬nh há»— trá»£."""
        log_section("Váº¼ FEATURE IMPORTANCE CHO Táº¤T Cáº¢ MÃ” HÃŒNH", icon="ğŸ“Š")
        
        supported_models = [m for m in self.models.keys() if m != 'polynomial']
        
        if not supported_models:
            logger.warning("âš ï¸  KhÃ´ng cÃ³ mÃ´ hÃ¬nh nÃ o há»— trá»£ feature importance")
            return
        
        for model_name in supported_models:
            self.plot_feature_importance(model_name, top_n=top_n, save=save)
    
    def compare_feature_importance(self, top_n: int = 10, save: bool = True) -> None:
        """So sÃ¡nh feature importance giá»¯a cÃ¡c mÃ´ hÃ¬nh."""
        log_section("SO SÃNH FEATURE IMPORTANCE GIá»®A CÃC MÃ” HÃŒNH", icon="ğŸ“Š")
        
        importances = {}
        for model_name in ['random_forest', 'extra_trees', 'xgboost']:
            if model_name in self.models:
                imp_df = self.get_feature_importance(model_name, top_n=top_n)
                if imp_df is not None:
                    importances[model_name] = imp_df
        
        if len(importances) == 0:
            logger.warning("âš ï¸  KhÃ´ng cÃ³ mÃ´ hÃ¬nh nÃ o há»— trá»£ feature importance Ä‘á»ƒ so sÃ¡nh")
            return
        
        if len(importances) == 1:
            logger.warning(f"âš ï¸  Chá»‰ cÃ³ 1 mÃ´ hÃ¬nh ({list(importances.keys())[0]}), cáº§n Ã­t nháº¥t 2 Ä‘á»ƒ so sÃ¡nh")
            return
        
        save_path = None
        if save:
            save_path = MODEL_RESULTS_DIR / 'feature_importance_comparison.png'
        self.visualizer.plot_feature_importance_comparison(
            importances=importances,
            save_path=save_path,
            top_n=top_n,
            show=not save,
        )
    
    # ========== DATA PREPARATION (Static) ==========
    @staticmethod
    def prepare_data(df: pd.DataFrame, 
                     target_col: str = 'Trip_Price',
                     test_size: float = 0.2,
                     random_state: int = 42,
                     scale: bool = False) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
        """
        Chia dá»¯ liá»‡u (KHÃ”NG scale - scale Ä‘Æ°á»£c thá»±c hiá»‡n á»Ÿ preprocessing).
        
        Args:
            df: DataFrame chá»©a dá»¯ liá»‡u
            target_col: TÃªn cá»™t target
            test_size: Tá»· lá»‡ test set
            random_state: Random seed
            scale: DEPRECATED - KhÃ´ng nÃªn scale á»Ÿ Ä‘Ã¢y
            
        Returns:
            (X_train, X_test, y_train, y_test)
        """
        log_section("CHUáº¨N Bá»Š Dá»® LIá»†U CHO TRAINING", icon="ğŸ”„")
        
        X = df.drop(target_col, axis=1)
        y = df[target_col]
        
        log_step(f"Total samples: {len(df)}, Features: {X.shape[1]}", icon="ğŸ“¦")
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state
        )
        
        log_step(f"Train set: {X_train.shape[0]}, Test set: {X_test.shape[0]}", icon="ğŸ”€")
        
        if scale:
            logger.warning("âš ï¸  scale=True khÃ´ng khuyáº¿n nghá»‹ - NÃªn scale á»Ÿ preprocessing!")
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_test_scaled = scaler.transform(X_test)
            
            X_train = pd.DataFrame(X_train_scaled, columns=X.columns)
            X_test = pd.DataFrame(X_test_scaled, columns=X.columns)
            
            log_step("Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a (StandardScaler)", icon="âœ…")
        
        log_step(f"X_train shape: {X_train.shape}", icon="ğŸ“Š")
        log_step(f"X_test shape: {X_test.shape}", icon="ğŸ“Š")
        
        return X_train, X_test, y_train, y_test
