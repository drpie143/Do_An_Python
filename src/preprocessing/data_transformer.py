"""
Data transformation utilities cho post-split processing.

Module n√†y x·ª≠ l√Ω c√°c b∆∞·ªõc SAU khi chia train/test:
- Handle missing values (fit tr√™n train, transform tr√™n test)
- Remove outliers (ch·ªâ tr√™n train)
- Encode categorical (fit/transform)
- Scale features (fit/transform)
- Interaction features
- fit_transform() pipeline
- transform_new_data() cho test/inference
- Save/load state
"""

from __future__ import annotations

import logging
from pathlib import Path
from typing import Any, Dict, List, Optional, Sequence, Tuple, Union, Set

import numpy as np
import pandas as pd
import joblib
from sklearn.base import clone
from sklearn.ensemble import IsolationForest
from sklearn.preprocessing import (
	LabelEncoder,
	MaxAbsScaler,
	MinMaxScaler,
	Normalizer,
	OneHotEncoder,
	OrdinalEncoder,
	RobustScaler,
	StandardScaler,
)

from src.visualization import DataVisualizer
from config import EDA_RESULTS_DIR, PLOT_DPI, PLOT_STYLE, FIGURE_SIZE


logger = logging.getLogger(__name__)

MissingRule = Union[str, Tuple[str, Any]]


class DataTransformer:
	"""Transform d·ªØ li·ªáu sau khi chia train/test (fit tr√™n train, transform tr√™n test)."""

	def __init__(
		self,
		data: Optional[pd.DataFrame] = None,
		missing_strategy: str = "median",
		categorical_missing_strategy: str = "mode",
		scaler_type: str = "standard",
		encoder_type: str = "onehot",
	) -> None:
		"""
		Kh·ªüi t·∫°o DataTransformer.
		
		Args:
			data: DataFrame (th∆∞·ªùng l√† train data)
			missing_strategy: Chi·∫øn l∆∞·ª£c fill missing cho numeric ('mean', 'median', 'mode')
			categorical_missing_strategy: Chi·∫øn l∆∞·ª£c fill missing cho categorical ('mode', 'constant')
			scaler_type: Lo·∫°i scaler ('standard', 'minmax', 'robust')
			encoder_type: Lo·∫°i encoder ('onehot', 'label', 'ordinal')
		"""
		self.data = data.copy() if data is not None else None
		self.numeric_cols: List[str] = []
		self.categorical_cols: List[str] = []
		self.types_: Dict[str, List[str]] = {}
		self.preprocessing_steps: List[str] = []

		# Missing value strategies
		self.missing_strategy = missing_strategy
		self.categorical_missing_strategy = categorical_missing_strategy
		self.missing_rules: Dict[str, MissingRule] = {}
		self.impute_values: Dict[str, Any] = {}

		# Constraint rules (for transform_new_data)
		self.constraint_rules: Dict[str, Dict[str, Any]] = {}

		# Outlier rules
		self.outlier_rules: Dict[str, str] = {}

		# Scaler
		self.scaler_rules: Dict[str, str] = {}
		self.scalers: Dict[str, Any] = {}
		self.default_scaler = self._init_default_scaler(scaler_type)
		self._scale_exclude: List[str] = []
		self._scaling_method: Optional[str] = None

		# Encoder
		self.encoder_rules: Dict[str, str] = {}
		self.encoders: Dict[str, Any] = {}
		self.default_encoder = self._init_default_encoder(encoder_type)
		self._onehot_drop_first = True
		self._encoding_method: Optional[str] = None
		self.onehot_feature_names: Dict[str, List[str]] = {}

		# Fitted state
		self.feature_columns: List[str] = []
		self._is_fitted: bool = False
		self._capture_config: bool = True

		# Config capture for transform_new_data
		self._unify_applied: bool = False
		self._text_unify_rules: Optional[Dict[str, Dict[str, str]]] = None
		self._feature_engineering_config: Optional[Dict[str, Any]] = None
		self._datetime_feature_configs: List[Dict[str, Any]] = []
		self._datetime_config_keys: Set[Tuple[str, Tuple[str, ...], bool]] = set()
		self._interaction_configs: List[Dict[str, Any]] = []
		self._interaction_config_keys: Set[Tuple[Tuple[Tuple[str, str], ...], Tuple[str, ...]]] = set()

		# Logging
		self.log: Dict[str, List[str]] = {}

		# Visualizer
		self._visualizer_init_args = {
			"target_col": None,
			"output_dir": EDA_RESULTS_DIR,
			"auto_save": True,
			"auto_show": False,
			"dpi": PLOT_DPI,
			"style": PLOT_STYLE,
			"figure_size": FIGURE_SIZE,
		}
		self.visualizer = DataVisualizer(data=self.data, **self._visualizer_init_args)

		if self.data is not None:
			self.detect_types()
			logger.info("‚úÖ DataTransformer kh·ªüi t·∫°o th√†nh c√¥ng - Shape: %s", self.data.shape)

	# ------------------------------------------------------------------
	# Internal helpers
	# ------------------------------------------------------------------
	def _init_default_scaler(self, scaler_type: str) -> Optional[Any]:
		mapping = {
			"standard": StandardScaler,
			"minmax": MinMaxScaler,
			"robust": RobustScaler,
			"maxabs": MaxAbsScaler,
			"normalize": lambda: Normalizer(norm="l2"),
		}
		constructor = mapping.get(scaler_type.lower()) if isinstance(scaler_type, str) else None
		return constructor() if constructor else None

	def _init_default_encoder(self, encoder_type: str) -> Optional[Any]:
		mapping = {
			"label": LabelEncoder,
			"onehot": lambda: OneHotEncoder(sparse_output=False, handle_unknown="ignore", dtype=int),
			"ordinal": lambda: OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1),
		}
		constructor = mapping.get(encoder_type.lower()) if isinstance(encoder_type, str) else None
		return constructor() if constructor else None

	def _update_visualizer(self) -> None:
		self.visualizer.set_data(self.data)

	def _log(self, key: str, message: str) -> None:
		self.log.setdefault(key, []).append(message)

	def detect_types(self) -> Dict[str, List[str]]:
		"""Ph√¢n lo·∫°i c√°c c·ªôt theo ki·ªÉu d·ªØ li·ªáu."""
		if self.data is None:
			raise ValueError("Ch∆∞a c√≥ d·ªØ li·ªáu ƒë·ªÉ ph√¢n lo·∫°i ki·ªÉu.")

		df = self.data
		numeric_cols = df.select_dtypes(include=["number"]).columns.tolist()
		datetime_cols = df.select_dtypes(include=["datetime", "datetime64[ns]"]).columns.tolist()
		bool_cols = df.select_dtypes(include=["bool"]).columns.tolist()
		categorical_cols = [col for col in df.columns if col not in set(numeric_cols + datetime_cols + bool_cols)]

		self.types_ = {
			"numeric_data": numeric_cols,
			"categorical_data": categorical_cols,
			"datetime_data": datetime_cols,
			"boolean_data": bool_cols,
		}
		self.numeric_cols = numeric_cols
		self.categorical_cols = categorical_cols
		return self.types_

	# ------------------------------------------------------------------
	# Missing value handling
	# ------------------------------------------------------------------
	def fill_missing(self, missing_rules: Optional[Dict[str, MissingRule]] = None, *, fit: bool = True) -> "DataTransformer":
		"""X·ª≠ l√Ω missing values."""
		if self.data is None:
			raise ValueError("Ch∆∞a c√≥ d·ªØ li·ªáu")
		if missing_rules:
			self.missing_rules.update(missing_rules)

		df = self.data
		cols_with_nan = [col for col in df.columns if df[col].isna().any()]
		if not cols_with_nan:
			logger.info("Kh√¥ng c√≥ missing value c·∫ßn x·ª≠ l√Ω")
			return self

		self.detect_types()
		for col in cols_with_nan:
			rule = self.missing_rules.get(col)
			strategy = None
			custom_value = None
			if isinstance(rule, (list, tuple)):
				strategy, custom_value = rule[0], rule[1]
			elif isinstance(rule, str):
				strategy = rule

			if strategy is None:
				strategy = self.missing_strategy if pd.api.types.is_numeric_dtype(df[col]) else self.categorical_missing_strategy

			try:
				if strategy == "drop":
					if fit:
						before = len(df)
						df = df.dropna(subset=[col])
						logger.info("%s: Drop %s d√≤ng", col, before - len(df))
					self.impute_values[col] = "__drop__"
				elif strategy == "mean" and pd.api.types.is_numeric_dtype(df[col]):
					value = df[col].mean() if fit else self.impute_values.get(col)
					if value is None:
						value = df[col].mean()
					df[col] = df[col].fillna(value)
					if fit:
						self.impute_values[col] = value
				elif strategy == "median" and pd.api.types.is_numeric_dtype(df[col]):
					value = df[col].median() if fit else self.impute_values.get(col)
					if value is None:
						value = df[col].median()
					df[col] = df[col].fillna(value)
					if fit:
						self.impute_values[col] = value
				elif strategy == "mode":
					if fit:
						mode_val = df[col].mode().iloc[0] if not df[col].mode().empty else custom_value
						self.impute_values[col] = mode_val
					else:
						mode_val = self.impute_values.get(col)
						if mode_val is None and not df[col].mode().empty:
							mode_val = df[col].mode().iloc[0]
					df[col] = df[col].fillna(mode_val)
				elif strategy == "constant":
					if fit:
						fill_val = custom_value
						if fill_val is None:
							fill_val = 0 if pd.api.types.is_numeric_dtype(df[col]) else "Unknown"
						self.impute_values[col] = fill_val
					else:
						fill_val = self.impute_values.get(col, custom_value)
						if fill_val is None:
							fill_val = 0 if pd.api.types.is_numeric_dtype(df[col]) else "Unknown"
					df[col] = df[col].fillna(fill_val)
				elif strategy == "ffill":
					df[col] = df[col].ffill()
				elif strategy == "bfill":
					df[col] = df[col].bfill()
				else:
					if fit:
						default_value = df[col].median() if pd.api.types.is_numeric_dtype(df[col]) else df[col].mode().iloc[0]
						self.impute_values[col] = default_value
					else:
						default_value = self.impute_values.get(col)
						if default_value is None:
							default_value = df[col].median() if pd.api.types.is_numeric_dtype(df[col]) else df[col].mode().iloc[0]
					df[col] = df[col].fillna(default_value)
			except Exception as exc:
				self._log("missing_error", f"{col}: {exc}")
				logger.error("L·ªói x·ª≠ l√Ω missing cho %s: %s", col, exc)

		self.data = df.reset_index(drop=True)
		if fit:
			self.preprocessing_steps.append("fill_missing")
		self._update_visualizer()
		return self

	def handle_missing(
		self,
		strategy: str = "auto",
		numeric_strategy: str = "median",
		categorical_strategy: str = "mode",
		fill_value: Optional[Any] = None,
	) -> "DataTransformer":
		"""X·ª≠ l√Ω missing values v·ªõi chi·∫øn l∆∞·ª£c cho numeric v√† categorical."""
		if strategy == "drop":
			self.missing_strategy = "drop"
			self.categorical_missing_strategy = "drop"
			return self.fill_missing()

		self.missing_strategy = numeric_strategy
		self.categorical_missing_strategy = categorical_strategy
		if fill_value is not None:
			for col in self.numeric_cols:
				self.missing_rules.setdefault(col, ("constant", fill_value))
		return self.fill_missing()

	# ------------------------------------------------------------------
	# Outlier handling
	# ------------------------------------------------------------------
	def handle_outliers(self, outlier_rules: Optional[Dict[str, str]] = None) -> "DataTransformer":
		"""X·ª≠ l√Ω outliers theo t·ª´ng c·ªôt."""
		if self.data is None:
			return self
		rules = outlier_rules or self.outlier_rules
		if not rules:
			return self

		df = self.data
		for col, method in rules.items():
			if col not in df.columns or not pd.api.types.is_numeric_dtype(df[col]):
				continue
			method = method.lower()
			mask_outlier = None

			try:
				if method == "iqr":
					q1 = df[col].quantile(0.25)
					q3 = df[col].quantile(0.75)
					iqr = q3 - q1
					lower = q1 - 1.5 * iqr
					upper = q3 + 1.5 * iqr
					mask_outlier = (df[col] < lower) | (df[col] > upper)
				elif method == "zscore":
					z_scores = np.abs((df[col] - df[col].mean()) / df[col].std(ddof=0))
					mask_outlier = z_scores > 3
				elif method == "iforest":
					clf = IsolationForest(random_state=42, contamination="auto")
					preds = clf.fit_predict(df[[col]].fillna(df[col].mean()))
					mask_outlier = preds == -1
				else:
					logger.warning("Method %s kh√¥ng h·ª£p l·ªá cho c·ªôt %s", method, col)
					continue

				if mask_outlier is not None and mask_outlier.any():
					logger.info("%s: Drop %s outliers (%s)", col, int(mask_outlier.sum()), method)
					df = df.loc[~mask_outlier]
			except Exception as exc:
				self._log("outlier_error", f"{col}: {exc}")

		self.data = df.reset_index(drop=True)
		self._update_visualizer()
		self.preprocessing_steps.append("handle_outliers")
		return self

	def remove_outliers(self, method: str = "iqr", threshold: float = 1.5) -> "DataTransformer":
		"""Lo·∫°i b·ªè outliers t·ª´ t·∫•t c·∫£ c·ªôt s·ªë."""
		self.outlier_rules = {col: method for col in self.numeric_cols}
		return self.handle_outliers(outlier_rules=self.outlier_rules)

	# ------------------------------------------------------------------
	# Scaling
	# ------------------------------------------------------------------
	def scale(self, scaler_rules: Optional[Dict[str, str]] = None, *, fit: bool = True) -> "DataTransformer":
		"""Scale c√°c c·ªôt s·ªë."""
		if self.data is None:
			return self
		self.detect_types()
		num_cols = self.types_.get("numeric_data", [])
		if not num_cols:
			return self

		if scaler_rules:
			self.scaler_rules.update(scaler_rules)

		df = self.data
		logger.info(
			"‚öôÔ∏è  SCALE FEATURES | T·ªïng c·ªôt s·ªë: %s | C√≥ rule ri√™ng: %s",
			len(num_cols),
			len(self.scaler_rules),
		)
		scaled_cols = set()
		for col, method in self.scaler_rules.items():
			if col not in num_cols:
				continue
			if fit:
				scaler = self._init_default_scaler(method)
				if scaler is None:
					continue
				df[[col]] = scaler.fit_transform(df[[col]])
				self.scalers[col] = scaler
			else:
				scaler = self.scalers.get(col)
				if scaler is None:
					continue
				df[[col]] = scaler.transform(df[[col]])
			scaled_cols.add(col)
			logger.info("   ‚Ä¢ %s: d√πng scaler %s", col, scaler.__class__.__name__)

		remaining_cols = [col for col in num_cols if col not in scaled_cols]
		if self.default_scaler and remaining_cols:
			logger.info(
				"   ‚Ä¢ %s c·ªôt c√≤n l·∫°i d√πng scaler m·∫∑c ƒë·ªãnh %s",
				len(remaining_cols),
				self.default_scaler.__class__.__name__,
			)
			for col in remaining_cols:
				if fit:
					scaler = clone(self.default_scaler)
					df[[col]] = scaler.fit_transform(df[[col]])
					self.scalers[col] = scaler
				else:
					scaler = self.scalers.get(col)
					if scaler is None:
						continue
					df[[col]] = scaler.transform(df[[col]])

		self.data = df
		return self

	def scale_features(
		self,
		method: str = "standard",
		columns: Optional[List[str]] = None,
		exclude_columns: Optional[List[str]] = None,
		*,
		fit: bool = True,
	) -> "DataTransformer":
		"""Scale features v·ªõi ph∆∞∆°ng ph√°p ch·ªâ ƒë·ªãnh."""
		if not fit and self._scaling_method is not None:
			method = self._scaling_method
		self.detect_types()
		target_cols = columns or self.types_.get("numeric_data", [])
		if exclude_columns:
			target_cols = [col for col in target_cols if col not in exclude_columns]
		if fit:
			self._scaling_method = method
			self._scale_exclude = exclude_columns or []
			self.scaler_rules = {col: method for col in target_cols}
			self.default_scaler = None
		return self.scale(fit=fit)

	# ------------------------------------------------------------------
	# Encoding
	# ------------------------------------------------------------------
	def encode(self, encoder_rules: Optional[Dict[str, str]] = None, *, fit: bool = True) -> "DataTransformer":
		"""Encode c√°c c·ªôt categorical."""
		if self.data is None:
			return self
		self.detect_types()
		cat_cols = self.types_.get("categorical_data", [])
		if not cat_cols:
			return self
		if encoder_rules:
			self.encoder_rules.update(encoder_rules)

		df = self.data
		logger.info(
			"üß© ENCODING FEATURES | T·ªïng c·ªôt ph√¢n lo·∫°i: %s | Rule ri√™ng: %s",
			len(cat_cols),
			len(self.encoder_rules),
		)
		onehot_frames = []
		cols_to_drop: List[str] = []

		for col in cat_cols:
			method = self.encoder_rules.get(col)
			if method is None and self.default_encoder is not None:
				if isinstance(self.default_encoder, OneHotEncoder):
					method = "onehot"
				elif isinstance(self.default_encoder, OrdinalEncoder):
					method = "ordinal"
				else:
					method = "label"

			try:
				if method == "label":
					if fit:
						le = LabelEncoder()
						df[col] = le.fit_transform(df[col].astype(str))
						self.encoders[col] = le
					else:
						le = self.encoders.get(col)
						if le is not None:
							df[col] = df[col].astype(str).apply(
								lambda x: le.transform([x])[0] if x in le.classes_ else -1
							)
					logger.info("   ‚Ä¢ %s: LabelEncoder", col)
				elif method == "onehot":
					if fit:
						ohe = OneHotEncoder(sparse_output=False, handle_unknown="ignore", dtype=int, drop="first" if self._onehot_drop_first else None)
						encoded = ohe.fit_transform(df[[col]])
						self.encoders[col] = ohe
						feature_names = [f"{col}_{cat}" for cat in ohe.categories_[0][1:]] if self._onehot_drop_first else [f"{col}_{cat}" for cat in ohe.categories_[0]]
						self.onehot_feature_names[col] = feature_names
					else:
						ohe = self.encoders.get(col)
						if ohe is not None:
							encoded = ohe.transform(df[[col]])
							feature_names = self.onehot_feature_names.get(col, [])
						else:
							continue
					ohe_df = pd.DataFrame(encoded, columns=feature_names, index=df.index)
					onehot_frames.append(ohe_df)
					cols_to_drop.append(col)
					logger.info("   ‚Ä¢ %s: OneHotEncoder -> %s c·ªôt m·ªõi", col, len(feature_names))
				elif method == "ordinal":
					if fit:
						oe = OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1)
						df[[col]] = oe.fit_transform(df[[col]])
						self.encoders[col] = oe
					else:
						oe = self.encoders.get(col)
						if oe is not None:
							df[[col]] = oe.transform(df[[col]])
					logger.info("   ‚Ä¢ %s: OrdinalEncoder", col)
			except Exception as exc:
				self._log("encode_error", f"{col}: {exc}")

		if onehot_frames:
			df = pd.concat([df] + onehot_frames, axis=1)
			df.drop(columns=cols_to_drop, inplace=True)
			logger.info("   ‚Ä¢ T·ªïng c·ªôt one-hot m·ªõi: %s", sum(frame.shape[1] for frame in onehot_frames))

		self.data = df
		return self

	def encode_categorical(
		self,
		method: str = "onehot",
		columns: Optional[List[str]] = None,
		drop_first: bool = True,
		*,
		fit: bool = True,
	) -> "DataTransformer":
		"""Encode categorical v·ªõi ph∆∞∆°ng ph√°p ch·ªâ ƒë·ªãnh."""
		self._onehot_drop_first = drop_first
		if fit:
			self._encoding_method = method
			if method == "onehot" and columns:
				self.encoder_rules = {col: "onehot" for col in columns}
			elif method == "label" and columns:
				self.encoder_rules = {col: "label" for col in columns}
		if fit:
			self.default_encoder = self._init_default_encoder(method)
		return self.encode(fit=fit)

	# ------------------------------------------------------------------
	# Feature engineering (post-split)
	# ------------------------------------------------------------------
	def create_datetime_features(
		self,
		datetime_col: str,
		features: Sequence[str] = ("hour", "day", "month", "dayofweek"),
		drop_original: bool = False,
	) -> "DataTransformer":
		"""T·∫°o features t·ª´ c·ªôt datetime."""
		if self.data is None or datetime_col not in self.data.columns:
			return self
		df = self.data
		if not pd.api.types.is_datetime64_any_dtype(df[datetime_col]):
			df[datetime_col] = pd.to_datetime(df[datetime_col], errors="coerce")
		if self._capture_config:
			config_key = (datetime_col, tuple(features), drop_original)
			if config_key not in self._datetime_config_keys:
				self._datetime_config_keys.add(config_key)
				self._datetime_feature_configs.append(
					{"datetime_col": datetime_col, "features": tuple(features), "drop_original": drop_original}
				)

		mapping = {
			"hour": df[datetime_col].dt.hour,
			"day": df[datetime_col].dt.day,
			"month": df[datetime_col].dt.month,
			"year": df[datetime_col].dt.year,
			"dayofweek": df[datetime_col].dt.dayofweek,
			"quarter": df[datetime_col].dt.quarter,
		}
		for feat in features:
			if feat in mapping:
				df[f"{datetime_col}_{feat}"] = mapping[feat]
		if drop_original:
			df.drop(columns=[datetime_col], inplace=True)
		self.data = df
		self.detect_types()
		if self._capture_config:
			self.preprocessing_steps.append("create_datetime_features")
		return self

	def create_interaction_features(
		self,
		col_pairs: List[Tuple[str, str]],
		operations: Sequence[str] = ("multiply",),
	) -> "DataTransformer":
		"""T·∫°o interaction features t·ª´ c√°c c·∫∑p c·ªôt."""
		if self.data is None:
			return self
		df = self.data
		if self._capture_config:
			normalized_pairs = [tuple(pair) for pair in col_pairs]
			config_key = (tuple(normalized_pairs), tuple(operations))
			if config_key not in self._interaction_config_keys:
				self._interaction_config_keys.add(config_key)
				self._interaction_configs.append(
					{"col_pairs": normalized_pairs, "operations": tuple(operations)}
				)
		for col1, col2 in col_pairs:
			if col1 not in df.columns or col2 not in df.columns:
				continue
			for op in operations:
				if op == "multiply":
					df[f"{col1}_x_{col2}"] = df[col1] * df[col2]
				elif op == "add":
					df[f"{col1}_plus_{col2}"] = df[col1] + df[col2]
				elif op == "subtract":
					df[f"{col1}_minus_{col2}"] = df[col1] - df[col2]
				elif op == "divide":
					df[f"{col1}_div_{col2}"] = df[col1] / (df[col2] + 1e-6)
		self.data = df
		self.detect_types()
		if self._capture_config:
			self.preprocessing_steps.append("create_interaction_features")
		return self

	# ------------------------------------------------------------------
	# Visualization bridge
	# ------------------------------------------------------------------
	def plot_correlation_heatmap(
		self,
		target_col: Optional[str] = None,
		method: str = "pearson",
		save_path: Optional[Union[str, Path]] = None,
		figsize: Tuple[int, int] = (12, 10),
		annot: bool = False,
		show: bool = False,
	) -> Optional[pd.DataFrame]:
		"""V·∫Ω heatmap t∆∞∆°ng quan."""
		if self.data is None:
			logger.warning("‚ùå Ch∆∞a c√≥ d·ªØ li·ªáu ƒë·ªÉ v·∫Ω heatmap")
			return None
		self.visualizer.set_data(self.data)
		self.visualizer.set_target(target_col)
		return self.visualizer.plot_correlation_heatmap(
			method=method,
			annot=annot,
			figsize=figsize,
			save_path=Path(save_path) if save_path else None,
			show=show,
		)

	def generate_eda_report(
		self,
		target_col: Optional[str] = None,
		numeric_cols: Optional[Sequence[str]] = None,
		categorical_cols: Optional[Sequence[str]] = None,
	) -> None:
		"""
		T·∫°o b√°o c√°o EDA t·ªëi ∆∞u v·ªõi 5 bi·ªÉu ƒë·ªì ch√≠nh:
		1. Data Overview - T·ªïng quan dataset
		2. Numeric Distributions - Ph√¢n ph·ªëi c√°c features s·ªë
		3. Categorical Distributions - Ph√¢n ph·ªëi c√°c features ph√¢n lo·∫°i
		4. Correlation Heatmap - Ma tr·∫≠n t∆∞∆°ng quan
		5. Target Analysis - Ph√¢n t√≠ch bi·∫øn m·ª•c ti√™u
		"""
		if self.data is None:
			logger.warning("‚ùå Ch∆∞a c√≥ d·ªØ li·ªáu ƒë·ªÉ t·∫°o bi·ªÉu ƒë·ªì EDA")
			return

		self.detect_types()
		self.visualizer.set_data(self.data)
		self.visualizer.set_target(target_col)

		logger.info("üìä ƒêang t·∫°o b√°o c√°o EDA...")
		
		# 1. Data Overview
		self.visualizer.plot_data_overview()
		
		# 2. Numeric distributions (grid)
		self.visualizer.plot_numeric_grid(cols=numeric_cols or self.numeric_cols)
		
		# 3. Categorical distributions (grid)
		self.visualizer.plot_categorical_grid(cols=categorical_cols or self.categorical_cols)
		
		# 4. Correlation heatmap
		corr_save_path = self.visualizer.output_dir / "04_correlation_heatmap.png"
		self.visualizer.plot_correlation_heatmap(method="spearman", annot=True, save_path=corr_save_path, show=False)
		
		# 5. Target analysis
		if target_col:
			self.visualizer.plot_target_analysis(top_n_corr=10)
		
		# 6. Outliers boxplot
		self.visualizer.plot_outliers_boxplot(cols=numeric_cols or self.numeric_cols)
		
		logger.info("‚úÖ ƒê√£ t·∫°o xong b√°o c√°o EDA (6 bi·ªÉu ƒë·ªì)")

	# ------------------------------------------------------------------
	# Pipeline orchestration
	# ------------------------------------------------------------------
	def mark_as_fitted(self) -> None:
		"""ƒê√°nh d·∫•u transformer ƒë√£ ƒë∆∞·ª£c fit."""
		if self.data is None:
			return
		self.feature_columns = self.data.columns.tolist()
		self._is_fitted = True

	def fit_transform(
		self,
		target_col: str,
		remove_outliers: bool = False,
		outlier_method: str = "iqr",
		outlier_threshold: float = 1.5,
		encoding_method: str = "onehot",
		drop_first_onehot: bool = True,
		scaling_method: str = "standard",
		exclude_scale_cols: Optional[List[str]] = None,
		interaction_pairs: Optional[List[Tuple[str, str]]] = None,
		generate_eda: bool = False,
	) -> pd.DataFrame:
		"""
		Pipeline ho√†n ch·ªânh ƒë·ªÉ fit v√† transform d·ªØ li·ªáu train.
		
		Th·ª© t·ª± x·ª≠ l√Ω: Outliers ‚Üí Fill Missing ‚Üí Encode ‚Üí Scale ‚Üí Interaction
		
		Args:
			target_col: T√™n c·ªôt target (s·∫Ω kh√¥ng scale)
			remove_outliers: C√≥ lo·∫°i b·ªè outliers kh√¥ng
			outlier_method: Ph∆∞∆°ng ph√°p detect outliers ('iqr', 'zscore')
			outlier_threshold: Ng∆∞·ª°ng detect outliers
			encoding_method: Ph∆∞∆°ng ph√°p encoding ('onehot', 'label')
			drop_first_onehot: C√≥ drop first column trong onehot kh√¥ng
			scaling_method: Ph∆∞∆°ng ph√°p scaling ('standard', 'minmax', 'robust')
			exclude_scale_cols: C√°c c·ªôt kh√¥ng scale (ngo√†i target)
			interaction_pairs: C√°c c·∫∑p c·ªôt ƒë·ªÉ t·∫°o interaction features
			generate_eda: C√≥ t·∫°o EDA report kh√¥ng
			
		Returns:
			DataFrame ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
		"""
		if self.data is None:
			raise ValueError("Ch∆∞a c√≥ d·ªØ li·ªáu ƒë·ªÉ x·ª≠ l√Ω")
		
		logger.info("=" * 70)
		logger.info("üîß B·∫ÆT ƒê·∫¶U FIT_TRANSFORM PIPELINE")
		logger.info("=" * 70)
		
		# EDA tr∆∞·ªõc khi x·ª≠ l√Ω
		if generate_eda:
			logger.info("üìä ƒêang t·∫°o EDA report...")
			self.generate_eda_report(target_col=target_col)
		
		# 1. Remove outliers
		if remove_outliers:
			logger.info("üî∏ B∆∞·ªõc 1: Lo·∫°i b·ªè outliers (%s)", outlier_method)
			self.remove_outliers(method=outlier_method, threshold=outlier_threshold)
			logger.info("   Shape sau outliers: %s", self.data.shape)
		
		# 2. Fill missing values
		logger.info("üî∏ B∆∞·ªõc 2: X·ª≠ l√Ω missing values")
		self.handle_missing(
			strategy='auto',
			numeric_strategy=self.missing_strategy,
			categorical_strategy=self.categorical_missing_strategy
		)
		
		# 3. Encode categorical
		logger.info("üî∏ B∆∞·ªõc 3: M√£ h√≥a bi·∫øn ph√¢n lo·∫°i (%s)", encoding_method)
		self._onehot_drop_first = drop_first_onehot
		self.encode_categorical(method=encoding_method, drop_first=drop_first_onehot)
		
		# 4. Scale features
		logger.info("üî∏ B∆∞·ªõc 4: Chu·∫©n h√≥a features (%s)", scaling_method)
		auto_exclude = [target_col] if target_col else []
		if exclude_scale_cols:
			auto_exclude.extend(exclude_scale_cols)
		# Th√™m c√°c c·ªôt binary v√†o exclude list
		for col in self.data.columns:
			if col not in auto_exclude and self.data[col].nunique() <= 2:
				if set(self.data[col].dropna().unique()).issubset({0, 1, 0.0, 1.0}):
					auto_exclude.append(col)
		
		self.scale_features(method=scaling_method, exclude_columns=auto_exclude)
		
		# 5. Interaction features
		if interaction_pairs:
			logger.info("üî∏ B∆∞·ªõc 5: T·∫°o interaction features")
			self.create_interaction_features(col_pairs=interaction_pairs, operations=['multiply'])
		
		# Mark as fitted
		self.mark_as_fitted()
		
		logger.info("‚úÖ FIT_TRANSFORM HO√ÄN T·∫§T - Shape: %s", self.data.shape)
		return self.data.copy()

	def transform_new_data(self, df: pd.DataFrame) -> pd.DataFrame:
		"""Transform d·ªØ li·ªáu m·ªõi (test/inference) d√πng params ƒë√£ fit t·ª´ train."""
		if not self._is_fitted:
			raise ValueError("Transformer ch∆∞a ƒë∆∞·ª£c fit tr√™n d·ªØ li·ªáu train")
		original_data = self.data
		original_numeric = self.numeric_cols.copy()
		original_categorical = self.categorical_cols.copy()
		try:
			self._capture_config = False
			self.data = df.copy()
			self.detect_types()
			
			# Apply constraints n·∫øu c√≥
			if self.constraint_rules:
				for col, rule in self.constraint_rules.items():
					if col not in self.data.columns or not isinstance(rule, dict):
						continue
					action = rule.get("action", "drop")
					min_val, max_val = rule.get("min"), rule.get("max")
					if action == "clip" and (min_val is not None or max_val is not None):
						self.data[col] = self.data[col].clip(lower=min_val, upper=max_val)
			
			# Unify values n·∫øu ƒë√£ √°p d·ª•ng
			if self._unify_applied or self._text_unify_rules:
				cat_cols = self.types_.get("categorical_data", [])
				for col in cat_cols:
					self.data[col] = self.data[col].astype(str).str.lower().str.strip()
					if self._text_unify_rules and col in self._text_unify_rules:
						self.data[col] = self.data[col].replace(self._text_unify_rules[col])
			
			# Datetime features
			if self._datetime_feature_configs:
				for config in self._datetime_feature_configs:
					self.create_datetime_features(
						datetime_col=config["datetime_col"],
						features=config["features"],
						drop_original=config["drop_original"],
					)
			
			# Feature engineering
			if self._feature_engineering_config:
				# √Åp d·ª•ng feature engineering ƒë∆°n gi·∫£n
				pass
			
			# Fill missing
			self.fill_missing(fit=False)
			
			# Encode
			encoding_method = self._encoding_method or "onehot"
			self.encode_categorical(method=encoding_method, fit=False)
			
			# Scale
			exclude_cols = self._scale_exclude
			self.scale_features(method=self._scaling_method or "standard", exclude_columns=exclude_cols, fit=False)
			
			# Interaction features
			if self._interaction_configs:
				for config in self._interaction_configs:
					self.create_interaction_features(
						col_pairs=[tuple(pair) for pair in config["col_pairs"]],
						operations=config["operations"],
					)
			
			# Reindex ƒë·ªÉ match columns v·ªõi train
			if self.feature_columns:
				self.data = self.data.reindex(columns=self.feature_columns, fill_value=0)
			
			return self.data.copy()
		finally:
			self._capture_config = True
			self.data = original_data
			self.numeric_cols = original_numeric
			self.categorical_cols = original_categorical
			if self.data is not None:
				self.detect_types()

	# ------------------------------------------------------------------
	# Persistence & reporting
	# ------------------------------------------------------------------
	def get_processed_data(self) -> pd.DataFrame:
		"""L·∫•y d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω."""
		if self.data is None:
			raise ValueError("Ch∆∞a c√≥ d·ªØ li·ªáu")
		return self.data.copy()

	def split_features_target(self, target_col: Optional[str] = None) -> Tuple[pd.DataFrame, pd.Series]:
		"""
		T√°ch features (X) v√† target (y) t·ª´ d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω.
		
		Args:
			target_col: T√™n c·ªôt target (m·∫∑c ƒë·ªãnh d√πng self.target_col)
			
		Returns:
			(X, y) tuple
		"""
		if self.data is None:
			raise ValueError("Ch∆∞a c√≥ d·ªØ li·ªáu")
		
		target = target_col or self.target_col
		if target is None:
			raise ValueError("Ch∆∞a x√°c ƒë·ªãnh target column")
		if target not in self.data.columns:
			raise ValueError(f"Target column '{target}' kh√¥ng t·ªìn t·∫°i trong data")
		
		X = self.data.drop(columns=[target])
		y = self.data[target]
		return X, y

	def get_correlated_features(
		self, 
		target_col: Optional[str] = None,
		threshold: float = 0.1,
		method: str = "spearman"
	) -> List[str]:
		"""
		L·∫•y danh s√°ch features c√≥ correlation v·ªõi target >= threshold.
		
		Args:
			target_col: T√™n c·ªôt target (m·∫∑c ƒë·ªãnh d√πng self.target_col)
			threshold: Ng∆∞·ª°ng |correlation| t·ªëi thi·ªÉu
			method: Ph∆∞∆°ng ph√°p t√≠nh correlation ('pearson', 'spearman', 'kendall')
			
		Returns:
			Danh s√°ch t√™n features th·ªèa m√£n ƒëi·ªÅu ki·ªán
		"""
		if self.data is None:
			raise ValueError("Ch∆∞a c√≥ d·ªØ li·ªáu")
		
		target = target_col or self.target_col
		if target is None:
			raise ValueError("Ch∆∞a x√°c ƒë·ªãnh target column")
		
		numeric_data = self.data.select_dtypes(include=['number'])
		if target not in numeric_data.columns:
			logger.warning(f"Target '{target}' kh√¥ng ph·∫£i numeric, kh√¥ng th·ªÉ t√≠nh correlation")
			return []
		
		corr_matrix = numeric_data.corr(method=method)
		if target not in corr_matrix.columns:
			return []
		
		corr_series = corr_matrix[target].drop(labels=[target], errors='ignore')
		selected = corr_series[abs(corr_series) >= threshold]
		
		features = selected.index.tolist()
		logger.info(f"üéØ {len(features)} features c√≥ |corr| >= {threshold}: {features}")
		return features

	def summary(self) -> Dict[str, Any]:
		"""T√≥m t·∫Øt th√¥ng tin d·ªØ li·ªáu."""
		if self.data is None:
			return {"status": "No data loaded"}
		return {
			"shape": self.data.shape,
			"n_numeric_cols": len(self.numeric_cols),
			"n_categorical_cols": len(self.categorical_cols),
			"total_missing": int(self.data.isnull().sum().sum()),
			"preprocessing_steps": self.preprocessing_steps,
			"memory_usage_mb": float(self.data.memory_usage(deep=True).sum() / 1024 ** 2),
			"is_fitted": self._is_fitted,
		}

	def print_summary(self) -> None:
		"""In t√≥m t·∫Øt ra log."""
		info = self.summary()
		logger.info("%s", "=" * 70)
		logger.info("üìä T√ìM T·∫ÆT D·ªÆ LI·ªÜU")
		logger.info("%s", info)
		logger.info("%s", "=" * 70)

	def save_state(self, path: Union[str, Path]) -> str:
		"""L∆∞u transformer state ƒë·ªÉ d√πng cho inference."""
		path = Path(path)
		path.parent.mkdir(parents=True, exist_ok=True)
		joblib.dump(self, path)
		logger.info("‚úÖ ƒê√£ l∆∞u transformer state t·∫°i %s", path)
		return str(path)

	@staticmethod
	def load_state(path: Union[str, Path]) -> "DataTransformer":
		"""Load transformer state ƒë√£ l∆∞u."""
		return joblib.load(Path(path))

	def __getstate__(self) -> Dict[str, Any]:
		state = self.__dict__.copy()
		state["visualizer"] = None
		state["_visualizer_state"] = self._visualizer_init_args
		return state

	def __setstate__(self, state: Dict[str, Any]) -> None:
		visualizer_state = state.pop("_visualizer_state", None)
		self.__dict__.update(state)
		viz_args = visualizer_state or {
			"target_col": None,
			"output_dir": EDA_RESULTS_DIR,
			"auto_save": True,
			"auto_show": False,
			"dpi": PLOT_DPI,
			"style": PLOT_STYLE,
			"figure_size": FIGURE_SIZE,
		}
		self.visualizer = DataVisualizer(data=self.data, **viz_args)
