"""
Module ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu cho d·ª± √°n Taxi Price Prediction.

Class DataPreprocessor cung c·∫•p c√°c ch·ª©c nƒÉng:
- ƒê·ªçc d·ªØ li·ªáu t·ª´ nhi·ªÅu ƒë·ªãnh d·∫°ng (csv, xlsx, json)
- X·ª≠ l√Ω gi√° tr·ªã thi·∫øu
- Ph√°t hi·ªán v√† x·ª≠ l√Ω outliers
- M√£ h√≥a bi·∫øn ph√¢n lo·∫°i
- Chu·∫©n h√≥a d·ªØ li·ªáu
- Feature engineering
- L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
"""

import os
import logging
from pathlib import Path
from typing import Dict, List, Optional, Union, Tuple, Any

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder
from sklearn.ensemble import IsolationForest


# C·∫•u h√¨nh logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class DataPreprocessor:
    """
    L·ªõp ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu cho b√†i to√°n Taxi Price Prediction.
    
    Cung c·∫•p ƒë·∫ßy ƒë·ªß c√°c ch·ª©c nƒÉng ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu bao g·ªìm:
    - ƒê·ªçc d·ªØ li·ªáu t·ª´ nhi·ªÅu ngu·ªìn
    - X·ª≠ l√Ω missing values
    - Ph√°t hi·ªán v√† x·ª≠ l√Ω outliers
    - Encoding bi·∫øn ph√¢n lo·∫°i
    - Scaling/Normalization
    - Feature engineering
    
    Attributes:
        data (pd.DataFrame): DataFrame ch·ª©a d·ªØ li·ªáu
        original_data (pd.DataFrame): B·∫£n sao d·ªØ li·ªáu g·ªëc
        numeric_cols (List[str]): Danh s√°ch c·ªôt s·ªë
        categorical_cols (List[str]): Danh s√°ch c·ªôt ph√¢n lo·∫°i
        scaler: Scaler ƒë√£ fit (StandardScaler ho·∫∑c MinMaxScaler)
        encoders (Dict): Dictionary l∆∞u c√°c encoder
    """
    
    def __init__(self, data: Optional[pd.DataFrame] = None):
        """
        Kh·ªüi t·∫°o DataPreprocessor.
        
        Args:
            data: DataFrame d·ªØ li·ªáu (optional)
        """
        self.data = data.copy() if data is not None else None
        self.original_data = data.copy() if data is not None else None
        self.numeric_cols = []
        self.categorical_cols = []
        self.scaler = None
        self.encoders = {}
        self.preprocessing_steps = []  # Track c√°c b∆∞·ªõc ƒë√£ th·ª±c hi·ªán
        
        if self.data is not None:
            self._identify_column_types()
            logger.info("‚úÖ DataPreprocessor kh·ªüi t·∫°o th√†nh c√¥ng")
            logger.info(f"   Shape: {self.data.shape}")
    
    def __repr__(self) -> str:
        """Representation c·ªßa DataPreprocessor."""
        if self.data is not None:
            return (f"DataPreprocessor(shape={self.data.shape}, "
                   f"numeric_cols={len(self.numeric_cols)}, "
                   f"categorical_cols={len(self.categorical_cols)})")
        return "DataPreprocessor(no data loaded)"
    
    # ========== LOAD DATA ==========
    @staticmethod
    def load_data(filepath: str, **kwargs) -> pd.DataFrame:
        """
        ƒê·ªçc d·ªØ li·ªáu t·ª´ file (csv, xlsx, json).
        
        Args:
            filepath: ƒê∆∞·ªùng d·∫´n file
            **kwargs: C√°c tham s·ªë b·ªï sung cho pandas read functions
            
        Returns:
            DataFrame ch·ª©a d·ªØ li·ªáu
            
        Raises:
            ValueError: N·∫øu ƒë·ªãnh d·∫°ng file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£
            FileNotFoundError: N·∫øu file kh√¥ng t·ªìn t·∫°i
        """
        filepath = Path(filepath)
        
        if not filepath.exists():
            raise FileNotFoundError(f"File kh√¥ng t·ªìn t·∫°i: {filepath}")
        
        file_ext = filepath.suffix.lower()
        
        try:
            if file_ext == '.csv':
                df = pd.read_csv(filepath, **kwargs)
            elif file_ext in ['.xlsx', '.xls']:
                df = pd.read_excel(filepath, **kwargs)
            elif file_ext == '.json':
                df = pd.read_json(filepath, **kwargs)
            else:
                raise ValueError(f"ƒê·ªãnh d·∫°ng file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£: {file_ext}")
            
            logger.info(f"‚úÖ ƒê·ªçc d·ªØ li·ªáu th√†nh c√¥ng t·ª´: {filepath}")
            logger.info(f"   Shape: {df.shape}")
            return df
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi ƒë·ªçc file: {e}")
            raise
    
    def load(self, filepath: str, **kwargs) -> 'DataPreprocessor':
        """
        Load d·ªØ li·ªáu v√†o instance.
        
        Args:
            filepath: ƒê∆∞·ªùng d·∫´n file
            **kwargs: Tham s·ªë cho load_data
            
        Returns:
            self cho method chaining
        """
        self.data = self.load_data(filepath, **kwargs)
        self.original_data = self.data.copy()
        self._identify_column_types()
        return self
    
    def _identify_column_types(self) -> None:
        """T·ª± ƒë·ªông nh·∫≠n di·ªán ki·ªÉu d·ªØ li·ªáu c√°c c·ªôt."""
        if self.data is None:
            return
        
        self.numeric_cols = self.data.select_dtypes(include=[np.number]).columns.tolist()
        self.categorical_cols = self.data.select_dtypes(include=['object', 'category']).columns.tolist()
        
        logger.info(f"üìä Ph√°t hi·ªán {len(self.numeric_cols)} c·ªôt s·ªë v√† {len(self.categorical_cols)} c·ªôt ph√¢n lo·∫°i")
    
    # ========== MISSING VALUES ==========
    def check_missing(self) -> pd.DataFrame:
        """
        Ki·ªÉm tra missing values.
        
        Returns:
            DataFrame ch·ª©a th·ªëng k√™ missing values
        """
        if self.data is None:
            logger.warning("‚ùå Ch∆∞a load d·ªØ li·ªáu")
            return None
        
        missing_count = self.data.isnull().sum()
        missing_percent = (missing_count / len(self.data)) * 100
        
        missing_df = pd.DataFrame({
            'Column': missing_count.index,
            'Missing_Count': missing_count.values,
            'Missing_Percent': missing_percent.values
        })
        
        missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values(
            'Missing_Count', ascending=False
        )
        
        if len(missing_df) > 0:
            logger.info(f"‚ö†Ô∏è  Ph√°t hi·ªán {len(missing_df)} c·ªôt c√≥ missing values")
        else:
            logger.info("‚úÖ Kh√¥ng c√≥ missing values")
        
        return missing_df
    
    def handle_missing(self, 
                      strategy: str = 'auto',
                      numeric_strategy: str = 'median',
                      categorical_strategy: str = 'mode',
                      fill_value: Optional[Any] = None) -> 'DataPreprocessor':
        """
        X·ª≠ l√Ω missing values.
        
        Args:
            strategy: Chi·∫øn l∆∞·ª£c x·ª≠ l√Ω ('auto', 'drop', 'fill')
            numeric_strategy: Chi·∫øn l∆∞·ª£c cho c·ªôt s·ªë ('mean', 'median', 'mode', 'forward_fill')
            categorical_strategy: Chi·∫øn l∆∞·ª£c cho c·ªôt ph√¢n lo·∫°i ('mode', 'constant')
            fill_value: Gi√° tr·ªã d√πng ƒë·ªÉ fill (n·∫øu strategy='fill' v√† constant)
            
        Returns:
            self cho method chaining
        """
        if self.data is None:
            logger.warning("‚ùå Ch∆∞a load d·ªØ li·ªáu")
            return self
        
        logger.info(f"\n{'='*70}")
        logger.info("üîß X·ª¨ L√ù MISSING VALUES")
        logger.info(f"{'='*70}")
        
        initial_missing = self.data.isnull().sum().sum()
        logger.info(f"T·ªïng missing tr∆∞·ªõc x·ª≠ l√Ω: {initial_missing}")
        
        if strategy == 'drop':
            self.data = self.data.dropna()
            logger.info(f"‚úÖ ƒê√£ x√≥a c√°c d√≤ng c√≥ missing values")
        
        elif strategy in ['auto', 'fill']:
            # X·ª≠ l√Ω c·ªôt s·ªë
            for col in self.numeric_cols:
                if self.data[col].isnull().any():
                    if numeric_strategy == 'mean':
                        fill_val = self.data[col].mean()
                    elif numeric_strategy == 'median':
                        fill_val = self.data[col].median()
                    elif numeric_strategy == 'mode':
                        fill_val = self.data[col].mode()[0]
                    elif numeric_strategy == 'forward_fill':
                        self.data[col] = self.data[col].fillna(method='ffill')
                        continue
                    else:
                        fill_val = fill_value if fill_value is not None else 0
                    
                    self.data[col] = self.data[col].fillna(fill_val)
                    logger.info(f"   Filled {col} ({numeric_strategy}): {fill_val:.2f}")
            
            # X·ª≠ l√Ω c·ªôt ph√¢n lo·∫°i
            for col in self.categorical_cols:
                if self.data[col].isnull().any():
                    if categorical_strategy == 'mode':
                        fill_val = self.data[col].mode()[0]
                    else:
                        fill_val = fill_value if fill_value is not None else 'Unknown'
                    
                    self.data[col] = self.data[col].fillna(fill_val)
                    logger.info(f"   Filled {col} ({categorical_strategy}): {fill_val}")
        
        final_missing = self.data.isnull().sum().sum()
        logger.info(f"‚úÖ T·ªïng missing sau x·ª≠ l√Ω: {final_missing}")
        
        self.preprocessing_steps.append('handle_missing')
        return self
    
    # ========== OUTLIERS ==========
    def detect_outliers_iqr(self, columns: Optional[List[str]] = None, 
                           threshold: float = 1.5) -> Dict[str, pd.Series]:
        """
        Ph√°t hi·ªán outliers b·∫±ng IQR method.
        
        Args:
            columns: Danh s√°ch c·ªôt c·∫ßn ki·ªÉm tra (None = t·∫•t c·∫£ c·ªôt s·ªë)
            threshold: IQR multiplier (m·∫∑c ƒë·ªãnh 1.5)
            
        Returns:
            Dictionary {column: outlier_indices}
        """
        if self.data is None:
            logger.warning("‚ùå Ch∆∞a load d·ªØ li·ªáu")
            return {}
        
        if columns is None:
            columns = self.numeric_cols
        
        outliers = {}
        
        for col in columns:
            Q1 = self.data[col].quantile(0.25)
            Q3 = self.data[col].quantile(0.75)
            IQR = Q3 - Q1
            
            lower_bound = Q1 - threshold * IQR
            upper_bound = Q3 + threshold * IQR
            
            outlier_mask = (self.data[col] < lower_bound) | (self.data[col] > upper_bound)
            outliers[col] = self.data[outlier_mask].index
            
            if len(outliers[col]) > 0:
                logger.info(f"   {col}: {len(outliers[col])} outliers detected")
        
        return outliers
    
    def detect_outliers_zscore(self, columns: Optional[List[str]] = None,
                              threshold: float = 3.0) -> Dict[str, pd.Series]:
        """
        Ph√°t hi·ªán outliers b·∫±ng Z-score method.
        
        Args:
            columns: Danh s√°ch c·ªôt c·∫ßn ki·ªÉm tra
            threshold: Z-score threshold (m·∫∑c ƒë·ªãnh 3.0)
            
        Returns:
            Dictionary {column: outlier_indices}
        """
        if self.data is None:
            logger.warning("‚ùå Ch∆∞a load d·ªØ li·ªáu")
            return {}
        
        if columns is None:
            columns = self.numeric_cols
        
        outliers = {}
        
        for col in columns:
            z_scores = np.abs((self.data[col] - self.data[col].mean()) / self.data[col].std())
            outlier_mask = z_scores > threshold
            outliers[col] = self.data[outlier_mask].index
            
            if len(outliers[col]) > 0:
                logger.info(f"   {col}: {len(outliers[col])} outliers detected (Z-score)")
        
        return outliers
    
    def detect_outliers_isolation_forest(self, 
                                        columns: Optional[List[str]] = None,
                                        contamination: float = 0.1) -> np.ndarray:
        """
        Ph√°t hi·ªán outliers b·∫±ng Isolation Forest.
        
        Args:
            columns: Danh s√°ch c·ªôt c·∫ßn ki·ªÉm tra
            contamination: T·ª∑ l·ªá outliers ∆∞·ªõc t√≠nh (0-0.5)
            
        Returns:
            Array indices c·ªßa outliers
        """
        if self.data is None:
            logger.warning("‚ùå Ch∆∞a load d·ªØ li·ªáu")
            return np.array([])
        
        if columns is None:
            columns = self.numeric_cols
        
        iso_forest = IsolationForest(contamination=contamination, random_state=42)
        predictions = iso_forest.fit_predict(self.data[columns])
        
        outlier_indices = np.where(predictions == -1)[0]
        logger.info(f"   Isolation Forest: {len(outlier_indices)} outliers detected")
        
        return outlier_indices
    
    def remove_outliers(self, method: str = 'iqr', **kwargs) -> 'DataPreprocessor':
        """
        Lo·∫°i b·ªè outliers.
        
        Args:
            method: Ph∆∞∆°ng ph√°p ('iqr', 'zscore', 'isolation_forest')
            **kwargs: Tham s·ªë cho detection method
            
        Returns:
            self cho method chaining
        """
        if self.data is None:
            logger.warning("‚ùå Ch∆∞a load d·ªØ li·ªáu")
            return self
        
        logger.info(f"\n{'='*70}")
        logger.info(f"üîç PH√ÅT HI·ªÜN V√Ä X√ìA OUTLIERS (method={method})")
        logger.info(f"{'='*70}")
        
        initial_shape = self.data.shape
        
        if method == 'iqr':
            outliers_dict = self.detect_outliers_iqr(**kwargs)
            # L·∫•y union c·ªßa t·∫•t c·∫£ outlier indices
            all_outlier_indices = set()
            for indices in outliers_dict.values():
                all_outlier_indices.update(indices)
            self.data = self.data.drop(list(all_outlier_indices))
        
        elif method == 'zscore':
            outliers_dict = self.detect_outliers_zscore(**kwargs)
            all_outlier_indices = set()
            for indices in outliers_dict.values():
                all_outlier_indices.update(indices)
            self.data = self.data.drop(list(all_outlier_indices))
        
        elif method == 'isolation_forest':
            outlier_indices = self.detect_outliers_isolation_forest(**kwargs)
            self.data = self.data.drop(self.data.index[outlier_indices])
        
        self.data = self.data.reset_index(drop=True)
        final_shape = self.data.shape
        
        logger.info(f"‚úÖ ƒê√£ x√≥a {initial_shape[0] - final_shape[0]} outliers")
        logger.info(f"   Shape: {initial_shape} ‚Üí {final_shape}")
        
        self.preprocessing_steps.append('remove_outliers')
        return self
    
    # ========== ENCODING ==========
    def encode_categorical(self, 
                          method: str = 'onehot',
                          columns: Optional[List[str]] = None,
                          drop_first: bool = True) -> 'DataPreprocessor':
        """
        M√£ h√≥a bi·∫øn ph√¢n lo·∫°i.
        
        Args:
            method: Ph∆∞∆°ng ph√°p ('onehot', 'label')
            columns: Danh s√°ch c·ªôt c·∫ßn encode (None = t·∫•t c·∫£ c·ªôt ph√¢n lo·∫°i)
            drop_first: C√≥ drop c·ªôt ƒë·∫ßu ti√™n khi onehot kh√¥ng (tr√°nh multicollinearity)
            
        Returns:
            self cho method chaining
        """
        if self.data is None:
            logger.warning("‚ùå Ch∆∞a load d·ªØ li·ªáu")
            return self
        
        logger.info(f"\n{'='*70}")
        logger.info(f"üî§ M√É H√ìA BI·∫æN PH√ÇN LO·∫†I (method={method})")
        logger.info(f"{'='*70}")
        
        if columns is None:
            columns = self.categorical_cols
        
        if method == 'onehot':
            self.data = pd.get_dummies(self.data, columns=columns, drop_first=drop_first)
            logger.info(f"‚úÖ OneHot Encoding ho√†n t·∫•t")
            logger.info(f"   Shape sau encoding: {self.data.shape}")
        
        elif method == 'label':
            for col in columns:
                le = LabelEncoder()
                self.data[col] = le.fit_transform(self.data[col])
                self.encoders[col] = le
                logger.info(f"   {col}: {len(le.classes_)} classes encoded")
            logger.info(f"‚úÖ Label Encoding ho√†n t·∫•t")
        
        # Update column types
        self._identify_column_types()
        
        self.preprocessing_steps.append('encode_categorical')
        return self
    
    # ========== SCALING ==========
    def scale_features(self, 
                      method: str = 'standard',
                      columns: Optional[List[str]] = None,
                      exclude_columns: Optional[List[str]] = None) -> 'DataPreprocessor':
        """
        Chu·∫©n h√≥a d·ªØ li·ªáu.
        
        Args:
            method: Ph∆∞∆°ng ph√°p ('standard', 'minmax')
            columns: Danh s√°ch c·ªôt c·∫ßn scale (None = t·∫•t c·∫£ c·ªôt s·ªë)
            exclude_columns: Danh s√°ch c·ªôt kh√¥ng scale
            
        Returns:
            self cho method chaining
        """
        if self.data is None:
            logger.warning("‚ùå Ch∆∞a load d·ªØ li·ªáu")
            return self
        
        logger.info(f"\n{'='*70}")
        logger.info(f"üìè CHU·∫®N H√ìA D·ªÆ LI·ªÜU (method={method})")
        logger.info(f"{'='*70}")
        
        if columns is None:
            columns = self.numeric_cols
        
        if exclude_columns:
            columns = [col for col in columns if col not in exclude_columns]
        
        if method == 'standard':
            self.scaler = StandardScaler()
        elif method == 'minmax':
            self.scaler = MinMaxScaler()
        else:
            raise ValueError(f"Method kh√¥ng h·ª£p l·ªá: {method}")
        
        self.data[columns] = self.scaler.fit_transform(self.data[columns])
        
        logger.info(f"‚úÖ Scaling ho√†n t·∫•t cho {len(columns)} c·ªôt")
        
        self.preprocessing_steps.append('scale_features')
        return self
    
    # ========== VISUALIZATION ==========
    def plot_correlation_heatmap(self,
                                 target_col: Optional[str] = None,
                                 method: str = 'pearson',
                                 save_path: Optional[Union[str, Path]] = None,
                                 figsize: Tuple[int, int] = (12, 10),
                                 annot: bool = False,
                                 show: bool = False) -> Optional[pd.DataFrame]:
        """V·∫Ω heatmap t∆∞∆°ng quan ƒë·ªÉ h·ªó tr·ª£ ch·ªçn feature."""
        if self.data is None:
            logger.warning("‚ùå Ch∆∞a c√≥ d·ªØ li·ªáu ƒë·ªÉ v·∫Ω heatmap")
            return None
        
        numeric_cols = self.data.select_dtypes(include=[np.number]).columns.tolist()
        if not numeric_cols:
            logger.warning("‚ùå Kh√¥ng c√≥ c·ªôt s·ªë ƒë·ªÉ t√≠nh t∆∞∆°ng quan")
            return None
        
        corr_df = self.data[numeric_cols].corr(method=method)
        if target_col and target_col in corr_df.columns:
            ordered_cols = [target_col] + [col for col in corr_df.columns if col != target_col]
            corr_df = corr_df.loc[ordered_cols, ordered_cols]
        
        plt.figure(figsize=figsize)
        sns.heatmap(
            corr_df,
            cmap='RdYlBu_r',
            annot=annot,
            fmt='.2f',
            square=True,
            cbar=True
        )
        plt.title(f'Feature Correlation Heatmap ({method.title()})', fontweight='bold')
        plt.tight_layout()
        
        if save_path:
            save_path = Path(save_path)
            save_path.parent.mkdir(parents=True, exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"‚úÖ ƒê√£ l∆∞u heatmap t∆∞∆°ng quan: {save_path}")
        
        if show:
            plt.show()
        else:
            plt.close()
        
        return corr_df
    
    # ========== FEATURE ENGINEERING ==========
    def create_datetime_features(self, 
                                 datetime_col: str,
                                 features: List[str] = ['hour', 'day', 'month', 'dayofweek'],
                                 drop_original: bool = False) -> 'DataPreprocessor':
        """
        T·∫°o features t·ª´ c·ªôt datetime.
        
        Args:
            datetime_col: T√™n c·ªôt datetime
            features: Danh s√°ch features c·∫ßn t·∫°o
            drop_original: C√≥ x√≥a c·ªôt g·ªëc kh√¥ng
            
        Returns:
            self cho method chaining
        """
        if self.data is None:
            logger.warning("‚ùå Ch∆∞a load d·ªØ li·ªáu")
            return self
        
        logger.info(f"\n{'='*70}")
        logger.info("üìÖ T·∫†O DATETIME FEATURES")
        logger.info(f"{'='*70}")
        
        # Convert to datetime n·∫øu ch∆∞a
        if not pd.api.types.is_datetime64_any_dtype(self.data[datetime_col]):
            self.data[datetime_col] = pd.to_datetime(self.data[datetime_col])
        
        created_features = []
        
        if 'hour' in features:
            self.data[f'{datetime_col}_hour'] = self.data[datetime_col].dt.hour
            created_features.append('hour')
        
        if 'day' in features:
            self.data[f'{datetime_col}_day'] = self.data[datetime_col].dt.day
            created_features.append('day')
        
        if 'month' in features:
            self.data[f'{datetime_col}_month'] = self.data[datetime_col].dt.month
            created_features.append('month')
        
        if 'year' in features:
            self.data[f'{datetime_col}_year'] = self.data[datetime_col].dt.year
            created_features.append('year')
        
        if 'dayofweek' in features:
            self.data[f'{datetime_col}_dayofweek'] = self.data[datetime_col].dt.dayofweek
            created_features.append('dayofweek')
        
        if 'quarter' in features:
            self.data[f'{datetime_col}_quarter'] = self.data[datetime_col].dt.quarter
            created_features.append('quarter')
        
        if drop_original:
            self.data = self.data.drop(datetime_col, axis=1)
            logger.info(f"   ƒê√£ x√≥a c·ªôt g·ªëc: {datetime_col}")
        
        logger.info(f"‚úÖ ƒê√£ t·∫°o {len(created_features)} features: {created_features}")
        
        # Update column types
        self._identify_column_types()
        
        self.preprocessing_steps.append('create_datetime_features')
        return self
    
    def create_interaction_features(self, 
                                   col_pairs: List[Tuple[str, str]],
                                   operations: List[str] = ['multiply']) -> 'DataPreprocessor':
        """
        T·∫°o interaction features t·ª´ c√°c c·∫∑p c·ªôt.
        
        Args:
            col_pairs: List c√°c tuple (col1, col2)
            operations: C√°c ph√©p to√°n ('multiply', 'add', 'subtract', 'divide')
            
        Returns:
            self cho method chaining
        """
        if self.data is None:
            logger.warning("‚ùå Ch∆∞a load d·ªØ li·ªáu")
            return self
        
        logger.info(f"\n{'='*70}")
        logger.info("üîó T·∫†O INTERACTION FEATURES")
        logger.info(f"{'='*70}")
        
        created_count = 0
        
        for col1, col2 in col_pairs:
            if col1 not in self.data.columns or col2 not in self.data.columns:
                logger.warning(f"   B·ªè qua: {col1}, {col2} kh√¥ng t·ªìn t·∫°i")
                continue
            
            for op in operations:
                if op == 'multiply':
                    feature_name = f'{col1}_x_{col2}'
                    self.data[feature_name] = self.data[col1] * self.data[col2]
                elif op == 'add':
                    feature_name = f'{col1}_plus_{col2}'
                    self.data[feature_name] = self.data[col1] + self.data[col2]
                elif op == 'subtract':
                    feature_name = f'{col1}_minus_{col2}'
                    self.data[feature_name] = self.data[col1] - self.data[col2]
                elif op == 'divide':
                    feature_name = f'{col1}_div_{col2}'
                    # Tr√°nh chia cho 0
                    self.data[feature_name] = self.data[col1] / (self.data[col2] + 1e-6)
                
                logger.info(f"   Created: {feature_name}")
                created_count += 1
        
        logger.info(f"‚úÖ ƒê√£ t·∫°o {created_count} interaction features")
        
        # Update column types
        self._identify_column_types()
        
        self.preprocessing_steps.append('create_interaction_features')
        return self
    
    # ========== SAVE & EXPORT ==========
    def save_data(self, filepath: str, index: bool = False, **kwargs) -> None:
        """
        L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω ra file.
        
        Args:
            filepath: ƒê∆∞·ªùng d·∫´n file output
            index: C√≥ l∆∞u index kh√¥ng
            **kwargs: Tham s·ªë cho pandas to_csv/to_excel/to_json
        """
        if self.data is None:
            logger.warning("‚ùå Ch∆∞a c√≥ d·ªØ li·ªáu ƒë·ªÉ l∆∞u")
            return
        
        filepath = Path(filepath)
        file_ext = filepath.suffix.lower()
        
        try:
            if file_ext == '.csv':
                self.data.to_csv(filepath, index=index, **kwargs)
            elif file_ext in ['.xlsx', '.xls']:
                self.data.to_excel(filepath, index=index, **kwargs)
            elif file_ext == '.json':
                self.data.to_json(filepath, **kwargs)
            else:
                raise ValueError(f"ƒê·ªãnh d·∫°ng file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£: {file_ext}")
            
            logger.info(f"‚úÖ ƒê√£ l∆∞u d·ªØ li·ªáu v√†o: {filepath}")
            
        except Exception as e:
            logger.error(f"‚ùå L·ªói khi l∆∞u file: {e}")
            raise
    
    def get_processed_data(self) -> pd.DataFrame:
        """
        L·∫•y DataFrame ƒë√£ x·ª≠ l√Ω.
        
        Returns:
            DataFrame ƒë√£ x·ª≠ l√Ω
        """
        return self.data.copy()
    
    def summary(self) -> Dict[str, Any]:
        """
        Tr·∫£ v·ªÅ th·ªëng k√™ t·ªïng quan v·ªÅ d·ªØ li·ªáu.
        
        Returns:
            Dictionary ch·ª©a th√¥ng tin t·ªïng quan
        """
        if self.data is None:
            return {'status': 'No data loaded'}
        
        return {
            'shape': self.data.shape,
            'n_numeric_cols': len(self.numeric_cols),
            'n_categorical_cols': len(self.categorical_cols),
            'total_missing': self.data.isnull().sum().sum(),
            'preprocessing_steps': self.preprocessing_steps,
            'memory_usage_mb': self.data.memory_usage(deep=True).sum() / 1024**2
        }
    
    def print_summary(self) -> None:
        """In ra t√≥m t·∫Øt d·ªØ li·ªáu."""
        summary = self.summary()
        
        logger.info(f"\n{'='*70}")
        logger.info("üìä T√ìM T·∫ÆT D·ªÆ LI·ªÜU")
        logger.info(f"{'='*70}")
        logger.info(f"Shape: {summary.get('shape', 'N/A')}")
        logger.info(f"Numeric columns: {summary.get('n_numeric_cols', 0)}")
        logger.info(f"Categorical columns: {summary.get('n_categorical_cols', 0)}")
        logger.info(f"Total missing: {summary.get('total_missing', 0)}")
        logger.info(f"Memory usage: {summary.get('memory_usage_mb', 0):.2f} MB")
        logger.info(f"Preprocessing steps: {', '.join(summary.get('preprocessing_steps', []))}")
        logger.info(f"{'='*70}\n")
