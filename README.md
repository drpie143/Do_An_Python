# Äá»’ ÃN CUá»I Ká»²: Dá»° ÄOÃN GIÃ TAXI

**MÃ´n:** Python cho Khoa há»c Dá»¯ liá»‡u - K23

## ğŸ“‹ Má»¥c lá»¥c

- [Giá»›i thiá»‡u](#giá»›i-thiá»‡u)
- [Cáº¥u trÃºc dá»± Ã¡n](#cáº¥u-trÃºc-dá»±-Ã¡n)
- [CÃ i Ä‘áº·t](#cÃ i-Ä‘áº·t)
- [HÆ°á»›ng dáº«n sá»­ dá»¥ng](#hÆ°á»›ng-dáº«n-sá»­-dá»¥ng)
- [Chi tiáº¿t ká»¹ thuáº­t](#chi-tiáº¿t-ká»¹-thuáº­t)
- [Káº¿t quáº£](#káº¿t-quáº£)

## ğŸ¯ Giá»›i thiá»‡u

Dá»± Ã¡n xÃ¢y dá»±ng pipeline há»c mÃ¡y hoÃ n chá»‰nh Ä‘á»ƒ **dá»± Ä‘oÃ¡n giÃ¡ cÆ°á»›c taxi** dá»±a trÃªn cÃ¡c yáº¿u tá»‘ nhÆ°:

- Khoáº£ng cÃ¡ch di chuyá»ƒn
- Thá»i gian di chuyá»ƒn
- Sá»‘ hÃ nh khÃ¡ch
- Äiá»u kiá»‡n giao thÃ´ng
- Thá»i tiáº¿t
- Thá»i Ä‘iá»ƒm trong ngÃ y

### MÃ´ hÃ¬nh sá»­ dá»¥ng:

1. **Polynomial Regression** - MÃ´ hÃ¬nh tuyáº¿n tÃ­nh vá»›i polynomial features
2. **Random Forest Regressor** - Ensemble learning
3. **Extra Trees Regressor** - Extremely Randomized Trees
4. **XGBoost Regressor** - Gradient boosting

### Tá»‘i Æ°u hyperparameters:

- Sá»­ dá»¥ng **Optuna** Ä‘á»ƒ tá»± Ä‘á»™ng tÃ¬m kiáº¿m hyperparameters tá»‘i Æ°u
- So sÃ¡nh hiá»‡u suáº¥t dá»±a trÃªn RMSE, MAE, RÂ²

## ğŸ“ Cáº¥u trÃºc dá»± Ã¡n

```
Do_An_Python/
â”‚
â”œâ”€â”€ src/                              # Source code chÃ­nh
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ preprocessing/                # Module tiá»n xá»­ lÃ½ dá»¯ liá»‡u
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ data_loader.py            # Class DataLoader (pre-split processing)
â”‚   â”‚   â””â”€â”€ data_transformer.py       # Class DataTransformer (post-split processing)
â”‚   â”‚
â”‚   â”œâ”€â”€ modeling/                     # Module training mÃ´ hÃ¬nh
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ base_trainer.py           # Class BaseTrainer (abstract base)
â”‚   â”‚   â”œâ”€â”€ model_registry.py         # CÃ¡c trainer cá»¥ thá»ƒ (Polynomial, RF, ET, XGB)
â”‚   â”‚   â””â”€â”€ model_trainer.py          # Class ModelTrainer (orchestrator)
â”‚   â”‚
â”‚   â””â”€â”€ visualization/                # Module trá»±c quan hÃ³a
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ data_visualizer.py        # Class DataVisualizer
â”‚
â”œâ”€â”€ data/                             # Dá»¯ liá»‡u
â”‚   â”œâ”€â”€ taxi_price.csv                # Dá»¯ liá»‡u gá»‘c
â”‚   â””â”€â”€ taxi_price_processed.csv      # Dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
â”‚
â”œâ”€â”€ models/                           # MÃ´ hÃ¬nh Ä‘Ã£ train
â”‚   â”œâ”€â”€ polynomial.joblib
â”‚   â”œâ”€â”€ random_forest.joblib
â”‚   â”œâ”€â”€ extra_trees.joblib
â”‚   â”œâ”€â”€ xgboost.joblib
â”‚   â””â”€â”€ data_transformer.joblib       # Transformer state cho inference
â”‚
â”œâ”€â”€ results/                          # Káº¿t quáº£ vÃ  biá»ƒu Ä‘á»“
â”‚   â”œâ”€â”€ eda/                          # Biá»ƒu Ä‘á»“ EDA (6 files)
â”‚   â”‚   â”œâ”€â”€ 01_data_overview.png
â”‚   â”‚   â”œâ”€â”€ 02_numeric_distributions.png
â”‚   â”‚   â”œâ”€â”€ 03_categorical_distributions.png
â”‚   â”‚   â”œâ”€â”€ 04_correlation_heatmap.png
â”‚   â”‚   â”œâ”€â”€ 05_target_analysis.png
â”‚   â”‚   â””â”€â”€ 06_outliers_boxplot.png
â”‚   â”œâ”€â”€ model/                        # Biá»ƒu Ä‘á»“ model (3 files)
â”‚   â”‚   â”œâ”€â”€ metrics_summary.png
â”‚   â”‚   â”œâ”€â”€ predictions_combined.png
â”‚   â”‚   â””â”€â”€ feature_importance_comparison.png
â”‚   â”œâ”€â”€ model_results.json
â”‚   â””â”€â”€ pipeline_state.json
â”‚
â”œâ”€â”€ notebooks/                        # Jupyter notebooks
â”‚   â””â”€â”€ do_an_py_modeling.ipynb
â”‚
â”œâ”€â”€ config.py                         # File cáº¥u hÃ¬nh
â”œâ”€â”€ main.py                           # Script chÃ­nh Ä‘á»ƒ cháº¡y pipeline
â”œâ”€â”€ predict.py                        # Script dá»± Ä‘oÃ¡n vá»›i model Ä‘Ã£ train
â”œâ”€â”€ requirements.txt                  # Dependencies
â”œâ”€â”€ README.md                         # File nÃ y
â””â”€â”€ yeu_cau_do_an.txt                 # YÃªu cáº§u Ä‘á»“ Ã¡n
```

## ğŸ”§ CÃ i Ä‘áº·t

### YÃªu cáº§u há»‡ thá»‘ng:

- Python 3.8+
- pip

### BÆ°á»›c 1: Clone hoáº·c download project

```bash
cd Do_An_Python
```

### BÆ°á»›c 2: CÃ i Ä‘áº·t dependencies

```bash
pip install -r requirements.txt
```

### BÆ°á»›c 3: Kiá»ƒm tra cÃ i Ä‘áº·t

```bash
python -c "import pandas, sklearn, xgboost, optuna; print('Cai dat thanh cong!')"
```

## ğŸš€ HÆ°á»›ng dáº«n sá»­ dá»¥ng

### CÃ¡ch 1: Cháº¡y toÃ n bá»™ pipeline (Khuyáº¿n nghá»‹)

```bash
# Cháº¡y vá»›i hyperparameters máº·c Ä‘á»‹nh (nhanh ~2 giÃ¢y)
python main.py

# Cháº¡y vá»›i optimization (cháº­m hÆ¡n nhÆ°ng káº¿t quáº£ tá»‘t hÆ¡n)
python main.py --optimize

# Cháº¡y khÃ´ng váº½ biá»ƒu Ä‘á»“
python main.py --no-viz

# Bá» qua download dá»¯ liá»‡u (náº¿u Ä‘Ã£ cÃ³)
python main.py --skip-download
```

Pipeline sáº½ tá»± Ä‘á»™ng:

1. Download dá»¯ liá»‡u tá»« Google Drive (náº¿u chÆ°a cÃ³)
2. Tiá»n xá»­ lÃ½ dá»¯ liá»‡u
3. Train 4 mÃ´ hÃ¬nh
4. ÄÃ¡nh giÃ¡ vÃ  so sÃ¡nh
5. LÆ°u mÃ´ hÃ¬nh vÃ  káº¿t quáº£

### CÃ¡ch 2: Dá»± Ä‘oÃ¡n vá»›i model Ä‘Ã£ train

```bash
# Dá»± Ä‘oÃ¡n tá»« file CSV
python predict.py --input data/new_data.csv --output predictions.csv

# Dá»± Ä‘oÃ¡n vá»›i model cá»¥ thá»ƒ
python predict.py --input data/new_data.csv --model xgboost

# Dá»± Ä‘oÃ¡n interactive (nháº­p tá»«ng giÃ¡ trá»‹)
python predict.py --interactive
```

### CÃ¡ch 3: Sá»­ dá»¥ng tá»«ng module riÃªng láº»

#### Tiá»n xá»­ lÃ½ dá»¯ liá»‡u:

```python
from src.preprocessing import DataLoader, DataTransformer

# PHASE 1: Load vÃ  clean dá»¯ liá»‡u (trÆ°á»›c khi split)
loader = DataLoader.from_file('data/taxi_price.csv')
loader.drop_duplicates()
loader.unify_values()
loader.apply_constraints()
raw_df = loader.get_data()

# PHASE 2: Chia train/test
from sklearn.model_selection import train_test_split
train_df, test_df = train_test_split(raw_df, test_size=0.2, random_state=42)

# PHASE 3: Transform dá»¯ liá»‡u (sau khi split)
transformer = DataTransformer(data=train_df)
train_processed = transformer.fit_transform(target_col='Trip_Price')

# PHASE 4: Transform test set (dÃ¹ng transformer Ä‘Ã£ fit)
test_processed = transformer.transform_new_data(test_df)

# LÆ°u transformer Ä‘á»ƒ dÃ¹ng cho inference
transformer.save_state('models/data_transformer.joblib')
```

#### Training mÃ´ hÃ¬nh:

```python
from src.modeling import ModelTrainer

# Khá»Ÿi táº¡o trainer
trainer = ModelTrainer(X_train, X_test, y_train, y_test)

# Train táº¥t cáº£ models
trainer.train_all(optimize=False)

# Hoáº·c train tá»«ng model
trainer.train_polynomial(degree=3, alpha=1.0)
trainer.train_rf(n_estimators=100, max_depth=10)
trainer.train_extra_trees(n_estimators=200, max_depth=12)
trainer.train_xgb(max_depth=6, learning_rate=0.1)

# ÄÃ¡nh giÃ¡
trainer.summary()

# LÆ°u mÃ´ hÃ¬nh
trainer.save_all_models()
```

#### Visualization:

```python
# Biá»ƒu Ä‘á»“ metrics tá»•ng há»£p
trainer.plot_metrics_summary(save=True)

# Biá»ƒu Ä‘á»“ predictions tá»•ng há»£p
trainer.plot_combined_predictions(save=True)

# So sÃ¡nh feature importance
trainer.compare_feature_importance(top_n=10, save=True)
```

## ğŸ”¬ Chi tiáº¿t ká»¹ thuáº­t

### 1. Module Preprocessing

#### DataLoader (pre-split processing)

| Method                  | MÃ´ táº£                             |
| ----------------------- | --------------------------------- |
| `from_file()`           | Load dá»¯ liá»‡u tá»« CSV, Excel, JSON  |
| `drop_duplicates()`     | XÃ³a dÃ²ng trÃ¹ng láº·p                |
| `unify_values()`        | Chuáº©n hÃ³a text (lowercase, strip) |
| `apply_constraints()`   | Ãp dá»¥ng rÃ ng buá»™c dá»¯ liá»‡u         |
| `generate_eda_report()` | Táº¡o 6 biá»ƒu Ä‘á»“ EDA                 |
| `get_data()`            | Láº¥y DataFrame Ä‘Ã£ xá»­ lÃ½            |

#### DataTransformer (post-split processing)

| Method                          | MÃ´ táº£                                  |
| ------------------------------- | -------------------------------------- |
| `fit_transform()`               | Fit vÃ  transform trÃªn train set        |
| `transform_new_data()`          | Transform dá»¯ liá»‡u má»›i (test/inference) |
| `fill_missing()`                | Xá»­ lÃ½ missing values                   |
| `encode()`                      | MÃ£ hÃ³a biáº¿n phÃ¢n loáº¡i (OneHot, Label)  |
| `scale()`                       | Chuáº©n hÃ³a features (Standard, MinMax)  |
| `remove_outliers()`             | Loáº¡i bá» outliers (IQR, Z-score)        |
| `save_state()` / `load_state()` | LÆ°u/load transformer state             |

### 2. Module Modeling

#### ModelTrainer (orchestrator)

| Method                | MÃ´ táº£                             |
| --------------------- | --------------------------------- |
| `train_all()`         | Train táº¥t cáº£ 4 models             |
| `train_polynomial()`  | Train Polynomial Regression       |
| `train_rf()`          | Train Random Forest               |
| `train_extra_trees()` | Train Extra Trees                 |
| `train_xgb()`         | Train XGBoost                     |
| `optimize_*()`        | Tá»‘i Æ°u hyperparameters vá»›i Optuna |
| `save_all_models()`   | LÆ°u táº¥t cáº£ models                 |
| `predict()`           | Dá»± Ä‘oÃ¡n vá»›i model                 |
| `summary()`           | In tÃ³m táº¯t káº¿t quáº£                |

#### Trainer cá»¥ thá»ƒ (trong model_registry.py)

- `PolynomialTrainer` - Polynomial Regression vá»›i Ridge
- `RandomForestTrainer` - Random Forest Regressor
- `ExtraTreesTrainer` - Extra Trees Regressor
- `XGBoostTrainer` - XGBoost Regressor

### 3. Optuna Optimization

**Hyperparameters Ä‘Æ°á»£c tá»‘i Æ°u:**

| Model         | Parameters                                                         |
| ------------- | ------------------------------------------------------------------ |
| Polynomial    | degree [2-5], alpha [1e-3, 10]                                     |
| Random Forest | n_estimators [50-300], max_depth [5-20]                            |
| Extra Trees   | n_estimators [50-300], max_depth [5-20]                            |
| XGBoost       | max_depth [4-10], learning_rate [0.01-0.3], n_estimators [100-500] |

### 4. Metrics Ä‘Ã¡nh giÃ¡

| Metric   | Ã nghÄ©a                                               |
| -------- | ----------------------------------------------------- |
| **RMSE** | Root Mean Squared Error - Äá»™ lá»—i trung bÃ¬nh           |
| **MAE**  | Mean Absolute Error - Äá»™ lá»—i tuyá»‡t Ä‘á»‘i trung bÃ¬nh     |
| **RÂ²**   | Coefficient of Determination (0-1, cÃ ng cao cÃ ng tá»‘t) |

## ğŸ“Š Káº¿t quáº£

### So sÃ¡nh hiá»‡u suáº¥t cÃ¡c mÃ´ hÃ¬nh:

| Model              | Train RMSE | Test RMSE | Test MAE | Test RÂ²   |
| ------------------ | ---------- | --------- | -------- | --------- |
| Polynomial         | 11.20      | 13.92     | 8.64     | 0.917     |
| Random Forest      | 5.09       | 11.95     | 6.27     | 0.939     |
| **Extra Trees** â­ | 1.52       | **10.48** | **5.72** | **0.953** |
| XGBoost            | 6.63       | 16.27     | 6.47     | 0.887     |

### MÃ´ hÃ¬nh tá»‘t nháº¥t: **Extra Trees**

- Test RÂ²: **0.953** (giáº£i thÃ­ch 95.3% phÆ°Æ¡ng sai)
- Test RMSE: **10.48**
- Test MAE: **5.72**

### Biá»ƒu Ä‘á»“ EDA (6 files trong results/eda/):

1. `01_data_overview.png` - Tá»•ng quan dá»¯ liá»‡u
2. `02_numeric_distributions.png` - PhÃ¢n phá»‘i biáº¿n sá»‘
3. `03_categorical_distributions.png` - PhÃ¢n phá»‘i biáº¿n phÃ¢n loáº¡i
4. `04_correlation_heatmap.png` - Ma tráº­n tÆ°Æ¡ng quan
5. `05_target_analysis.png` - PhÃ¢n tÃ­ch biáº¿n má»¥c tiÃªu
6. `06_outliers_boxplot.png` - Boxplot phÃ¡t hiá»‡n outliers

### Biá»ƒu Ä‘á»“ Model (3 files trong results/model/):

1. `metrics_summary.png` - So sÃ¡nh RÂ², RMSE, MAE
2. `predictions_combined.png` - Actual vs Predicted cho táº¥t cáº£ models
3. `feature_importance_comparison.png` - So sÃ¡nh feature importance

## ğŸ“ Cáº¥u hÃ¬nh

Chá»‰nh sá»­a file `config.py` Ä‘á»ƒ thay Ä‘á»•i cÃ¡c tham sá»‘:

```python
# Training
TEST_SIZE = 0.2
RANDOM_SEED = 42

# Preprocessing
MISSING_STRATEGY = {'numeric': 'median', 'categorical': 'mode'}
ENCODING_METHOD = 'onehot'
SCALING_METHOD = 'standard'

# Optuna
OPTUNA_N_TRIALS = {'polynomial': 10, 'random_forest': 20, ...}
```

## ğŸ› Troubleshooting

### Lá»—i download dá»¯ liá»‡u:

Download thá»§ cÃ´ng file `taxi_price.csv` vÃ  Ä‘áº·t vÃ o thÆ° má»¥c `data/`

### Lá»—i thiáº¿u thÆ° viá»‡n:

```bash
pip install -r requirements.txt --upgrade
```

### Lá»—i emoji trÃªn Windows:

Emoji cÃ³ thá»ƒ hiá»ƒn thá»‹ sai trÃªn PowerShell, nhÆ°ng file log (`training.log`) váº«n hiá»ƒn thá»‹ Ä‘Ãºng.

## ğŸ‘¥ ThÃ nh viÃªn nhÃ³m

Mai Quang DÅ©ng - 23280049
NgÃ´ Anh Khoa - 23280065

## ğŸ“„ License

Dá»± Ã¡n Ä‘Æ°á»£c táº¡o cho má»¥c Ä‘Ã­ch há»c táº­p - MÃ´n Python cho Khoa há»c Dá»¯ liá»‡u K23.

---

**NgÃ y hoÃ n thÃ nh:** 05/12/2025
