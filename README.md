# ƒê·ªí √ÅN CU·ªêI K·ª≤: D·ª∞ ƒêO√ÅN GI√Å TAXI

**M√¥n:** Python cho Khoa h·ªçc D·ªØ li·ªáu - K23

## üìã M·ª•c l·ª•c

- [Gi·ªõi thi·ªáu](#gi·ªõi-thi·ªáu)
- [C·∫•u tr√∫c d·ª± √°n](#c·∫•u-tr√∫c-d·ª±-√°n)
- [C√†i ƒë·∫∑t](#c√†i-ƒë·∫∑t)
- [H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng](#h∆∞·ªõng-d·∫´n-s·ª≠-d·ª•ng)
- [Chi ti·∫øt k·ªπ thu·∫≠t](#chi-ti·∫øt-k·ªπ-thu·∫≠t)
- [K·∫øt qu·∫£](#k·∫øt-qu·∫£)

## üéØ Gi·ªõi thi·ªáu

D·ª± √°n x√¢y d·ª±ng pipeline h·ªçc m√°y ho√†n ch·ªânh ƒë·ªÉ **d·ª± ƒëo√°n gi√° c∆∞·ªõc taxi** d·ª±a tr√™n c√°c y·∫øu t·ªë nh∆∞:

- Kho·∫£ng c√°ch di chuy·ªÉn
- Th·ªùi gian di chuy·ªÉn
- S·ªë h√†nh kh√°ch
- ƒêi·ªÅu ki·ªán giao th√¥ng
- Th·ªùi ti·∫øt
- Th·ªùi ƒëi·ªÉm trong ng√†y

### M√¥ h√¨nh s·ª≠ d·ª•ng:

1. **Polynomial Regression** - M√¥ h√¨nh tuy·∫øn t√≠nh v·ªõi polynomial features
2. **Random Forest Regressor** - Ensemble learning
3. **XGBoost Regressor** - Gradient boosting (m√¥ h√¨nh t·ªët nh·∫•t)

### T·ªëi ∆∞u hyperparameters:

- S·ª≠ d·ª•ng **Optuna** ƒë·ªÉ t·ª± ƒë·ªông t√¨m ki·∫øm hyperparameters t·ªëi ∆∞u
- So s√°nh hi·ªáu su·∫•t d·ª±a tr√™n RMSE, MAE, R¬≤

## üìÅ C·∫•u tr√∫c d·ª± √°n

```
Do_An_ver2/
‚îÇ
‚îú‚îÄ‚îÄ src/                          # Source code ch√≠nh
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing/            # Module ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_preprocessor.py  # Class DataPreprocessor
‚îÇ   ‚îî‚îÄ‚îÄ modeling/                 # Module training m√¥ h√¨nh
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ model_trainer.py      # Class ModelTrainer
‚îÇ
‚îú‚îÄ‚îÄ data/                         # D·ªØ li·ªáu
‚îÇ   ‚îú‚îÄ‚îÄ taxi_price.csv           # D·ªØ li·ªáu g·ªëc
‚îÇ   ‚îî‚îÄ‚îÄ taxi_price_processed.csv # D·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
‚îÇ
‚îú‚îÄ‚îÄ models/                       # M√¥ h√¨nh ƒë√£ train
‚îÇ   ‚îú‚îÄ‚îÄ polynomial_*.joblib
‚îÇ   ‚îú‚îÄ‚îÄ random_forest_*.joblib
‚îÇ   ‚îî‚îÄ‚îÄ xgboost_*.joblib
‚îÇ
‚îú‚îÄ‚îÄ results/                      # K·∫øt qu·∫£ v√† bi·ªÉu ƒë·ªì
‚îÇ   ‚îú‚îÄ‚îÄ model_results.json
‚îÇ   ‚îú‚îÄ‚îÄ comparison_*.png
‚îÇ   ‚îî‚îÄ‚îÄ predictions_*.png
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                    # Jupyter notebooks
‚îÇ   ‚îî‚îÄ‚îÄ do_an_py_modeling.ipynb  # Notebook ph√¢n t√≠ch ban ƒë·∫ßu
‚îÇ
‚îú‚îÄ‚îÄ config.py                     # File c·∫•u h√¨nh
‚îú‚îÄ‚îÄ main.py                       # Script ch√≠nh ƒë·ªÉ ch·∫°y pipeline
‚îú‚îÄ‚îÄ requirements.txt              # Dependencies
‚îú‚îÄ‚îÄ README.md                     # File n√†y
‚îî‚îÄ‚îÄ yeu_cau_do_an.txt            # Y√™u c·∫ßu ƒë·ªì √°n

```

## üîß C√†i ƒë·∫∑t

### Y√™u c·∫ßu h·ªá th·ªëng:

- Python 3.8+
- pip

### B∆∞·ªõc 1: Clone ho·∫∑c download project

```bash
cd Do_An_ver2
```

### B∆∞·ªõc 2: C√†i ƒë·∫∑t dependencies

#### C√°ch 1: S·ª≠ d·ª•ng m√¥i tr∆∞·ªùng Python hi·ªán t·∫°i (Khuy·∫øn ngh·ªã n·∫øu ƒë√£ c√≥ s·∫µn)

```bash
# C√†i ƒë·∫∑t tr·ª±c ti·∫øp v√†o m√¥i tr∆∞·ªùng hi·ªán t·∫°i
pip install -r requirements.txt
```

#### C√°ch 2: T·∫°o m√¥i tr∆∞·ªùng ·∫£o m·ªõi (Optional)

```bash
# T·∫°o m√¥i tr∆∞·ªùng ·∫£o
python -m venv venv

# K√≠ch ho·∫°t m√¥i tr∆∞·ªùng
# Windows PowerShell:
.\venv\Scripts\Activate.ps1
# Windows CMD:
.\venv\Scripts\activate.bat
# Linux/Mac:
source venv/bin/activate

# C√†i ƒë·∫∑t dependencies
pip install -r requirements.txt
```

#### C√°ch 3: S·ª≠ d·ª•ng Conda environment (Optional)

```bash
# T·∫°o conda environment
conda create -n taxi_price python=3.9 -y
conda activate taxi_price

# C√†i ƒë·∫∑t dependencies
pip install -r requirements.txt
```

### B∆∞·ªõc 3: Ki·ªÉm tra c√†i ƒë·∫∑t

```bash
python -c "import pandas, sklearn, xgboost, optuna; print('‚úÖ C√†i ƒë·∫∑t th√†nh c√¥ng!')"
```

> **üí° L∆∞u √Ω:** N·∫øu b·∫°n ƒë√£ c√≥ m√¥i tr∆∞·ªùng Python v·ªõi c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt, b·∫°n c√≥ th·ªÉ s·ª≠ d·ª•ng tr·ª±c ti·∫øp m√† kh√¥ng c·∫ßn t·∫°o m√¥i tr∆∞·ªùng m·ªõi. Ch·ªâ c·∫ßn ƒë·∫£m b·∫£o c√°c th∆∞ vi·ªán trong `requirements.txt` ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t.

## üöÄ H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng

### C√°ch 1: Ch·∫°y to√†n b·ªô pipeline (Khuy·∫øn ngh·ªã)

```bash
# Ch·∫°y v·ªõi hyperparameters m·∫∑c ƒë·ªãnh (nhanh)
python main.py

# Ch·∫°y v·ªõi optimization (ch·∫≠m h∆°n nh∆∞ng k·∫øt qu·∫£ t·ªët h∆°n)
python main.py --optimize

# Ch·∫°y kh√¥ng v·∫Ω bi·ªÉu ƒë·ªì (ti·∫øt ki·ªám th·ªùi gian)
python main.py --no-viz
```

Pipeline s·∫Ω t·ª± ƒë·ªông:

1. Download d·ªØ li·ªáu t·ª´ Google Drive (n·∫øu ch∆∞a c√≥)
2. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu
3. Train 3 m√¥ h√¨nh
4. ƒê√°nh gi√° v√† so s√°nh
5. L∆∞u m√¥ h√¨nh v√† k·∫øt qu·∫£

### C√°ch 2: S·ª≠ d·ª•ng t·ª´ng module ri√™ng l·∫ª

#### Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu:

```python
from src.preprocessing.data_preprocessor import DataPreprocessor

# Kh·ªüi t·∫°o v√† load data
preprocessor = DataPreprocessor()
preprocessor.load('data/taxi_price.csv')

# X·ª≠ l√Ω missing values
preprocessor.handle_missing(strategy='auto')

# Encoding bi·∫øn ph√¢n lo·∫°i
preprocessor.encode_categorical(method='onehot', drop_first=True)

# L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
preprocessor.save_data('data/taxi_price_processed.csv')
df = preprocessor.get_processed_data()
```

#### Training m√¥ h√¨nh:

```python
from src.modeling.model_trainer import ModelTrainer
import pandas as pd

# Load d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
df = pd.read_csv('data/taxi_price_processed.csv')

# Chu·∫©n b·ªã d·ªØ li·ªáu
X_train, X_test, y_train, y_test = ModelTrainer.prepare_data(
    df, target_col='Trip_Price', test_size=0.2, random_state=42
)

# Kh·ªüi t·∫°o trainer
trainer = ModelTrainer(X_train, X_test, y_train, y_test)

# Train XGBoost
trainer.train_xgb(max_depth=6, learning_rate=0.1, n_estimators=100)

# ƒê√°nh gi√°
trainer.summary()

# L∆∞u m√¥ h√¨nh
trainer.save_model('xgboost', format='joblib')
```

#### S·ª≠ d·ª•ng m√¥ h√¨nh ƒë√£ train ƒë·ªÉ d·ª± ƒëo√°n:

```python
from src.modeling.model_trainer import ModelTrainer
import pandas as pd

# T·∫°o trainer instance
trainer = ModelTrainer(X_train, X_test, y_train, y_test)

# Load m√¥ h√¨nh ƒë√£ l∆∞u
trainer.load_model('models/xgboost_20231126_120000.joblib', model_name='xgboost')

# D·ª± ƒëo√°n
predictions = trainer.predict(X_test, model_name='xgboost')

# ƒê√°nh gi√°
from sklearn.metrics import r2_score, mean_squared_error
print(f"R¬≤ Score: {r2_score(y_test, predictions):.4f}")
print(f"RMSE: {mean_squared_error(y_test, predictions, squared=False):.4f}")
```

#### Ph√¢n t√≠ch Feature Importance:

```python
# L·∫•y feature importance
importance_df = trainer.get_feature_importance('xgboost', top_n=10)
print(importance_df)

# V·∫Ω bi·ªÉu ƒë·ªì feature importance
trainer.plot_feature_importance('xgboost', top_n=15, save=True)

# So s√°nh feature importance gi·ªØa Random Forest v√† XGBoost
trainer.compare_feature_importance(top_n=10, save=True)
```

### C√°ch 3: Demo nhanh v·ªõi d·ªØ li·ªáu m·∫´u

```bash
# Ch·∫°y demo ƒë·ªÉ xem feature importance (nhanh, kh√¥ng c·∫ßn download data)
python demo_feature_importance.py
```

Script n√†y s·∫Ω:

- T·∫°o d·ªØ li·ªáu m·∫´u
- Train Random Forest v√† XGBoost
- Ph√¢n t√≠ch v√† v·∫Ω bi·ªÉu ƒë·ªì feature importance
- T·∫°o file `training.log` v·ªõi ƒë·∫ßy ƒë·ªß th√¥ng tin

## üî¨ Chi ti·∫øt k·ªπ thu·∫≠t

### 1. Class DataPreprocessor

**V·ªã tr√≠:** `src/preprocessing/data_preprocessor.py`

**Ch·ª©c nƒÉng ch√≠nh:**

| Method                          | M√¥ t·∫£                                                                   |
| ------------------------------- | ----------------------------------------------------------------------- |
| `load_data()`                   | ƒê·ªçc d·ªØ li·ªáu t·ª´ CSV, Excel, JSON                                         |
| `eda_overview()`                | B√°o c√°o t·ªïng quan (shape, missing %, skew, rare categories, correlations) |
| `apply_constraints()`           | √Åp d·ª•ng r√†ng bu·ªôc ki·ªÉu/mi·ªÅn gi√° tr·ªã d·ª±a tr√™n `config.CONSTRAINT_RULES`  |
| `handle_missing()`              | X·ª≠ l√Ω missing values (mean, median, mode, forward-fill)                 |
| `detect_outliers_*()`           | Ph√°t hi·ªán outliers (IQR, Z-score, Isolation Forest)                     |
| `remove_outliers()`             | Lo·∫°i b·ªè outliers                                                        |
| `encode_categorical()`          | M√£ h√≥a bi·∫øn ph√¢n lo·∫°i (OneHot, Label Encoding)                          |
| `scale_features()`              | Chu·∫©n h√≥a d·ªØ li·ªáu (StandardScaler, MinMaxScaler, h·ªó tr·ª£ exclude target) |
| `create_datetime_features()`    | T·∫°o features t·ª´ datetime                                                |
| `create_interaction_features()` | T·∫°o interaction features                                                |
| `save_data()`                   | L∆∞u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω                                                    |

> üìå **C·∫•u h√¨nh r√†ng bu·ªôc**: s·ª≠a `config.CONSTRAINT_RULES` ƒë·ªÉ quy ƒë·ªãnh dtype, min/max v√† h√†nh ƒë·ªông (clip/drop/mean) cho t·ª´ng c·ªôt. `main.py` s·∫Ω t·ª± ƒë·ªông g√°n c√°c rule n√†y cho `DataPreprocessor.apply_constraints()` tr∆∞·ªõc khi x·ª≠ l√Ω missing/outlier.

**V√≠ d·ª• s·ª≠ d·ª•ng:**

```python
preprocessor = DataPreprocessor()
preprocessor.load('data.csv') \
            .handle_missing(strategy='auto') \
            .encode_categorical(method='onehot') \
            .scale_features(method='standard', exclude_columns=['Trip_Price']) \
            .save_data('processed.csv')

‚ö†Ô∏è **L∆∞u √Ω:** lu√¥n lo·∫°i c·ªôt target (`Trip_Price`) kh·ªèi b∆∞·ªõc scale ƒë·ªÉ gi·ªØ nguy√™n distribution c·ªßa bi·∫øn m·ª•c ti√™u.
```

### 2. Class ModelTrainer

**V·ªã tr√≠:** `src/modeling/model_trainer.py`

**Ch·ª©c nƒÉng ch√≠nh:**

| Method                         | M√¥ t·∫£                                      |
| ------------------------------ | ------------------------------------------ |
| `prepare_data()`               | Chia v√† chu·∫©n h√≥a d·ªØ li·ªáu (static method)  |
| `optimize_polynomial()`        | T·ªëi ∆∞u Polynomial Regression b·∫±ng Optuna   |
| `train_polynomial()`           | Train Polynomial Regression                |
| `optimize_rf()`                | T·ªëi ∆∞u Random Forest b·∫±ng Optuna           |
| `train_rf()`                   | Train Random Forest                        |
| `optimize_xgb()`               | T·ªëi ∆∞u XGBoost b·∫±ng Optuna                 |
| `train_xgb()`                  | Train XGBoost                              |
| `save_model()`                 | L∆∞u m√¥ h√¨nh (joblib/pickle)                |
| `load_model()`                 | Load m√¥ h√¨nh                               |
| `get_best_model()`             | T√¨m m√¥ h√¨nh t·ªët nh·∫•t                       |
| `save_results()`               | L∆∞u k·∫øt qu·∫£ ƒë√°nh gi√° (JSON)                |
| `plot_comparison()`            | V·∫Ω bi·ªÉu ƒë·ªì so s√°nh                         |
| `plot_predictions()`           | V·∫Ω bi·ªÉu ƒë·ªì actual vs predicted             |
| `get_feature_importance()`     | **L·∫•y feature importance**                 |
| `plot_feature_importance()`    | **V·∫Ω bi·ªÉu ƒë·ªì feature importance**          |
| `compare_feature_importance()` | **So s√°nh feature importance gi·ªØa models** |
| `summary()`                    | In t√≥m t·∫Øt k·∫øt qu·∫£                         |
| `predict()`                    | D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh                        |

**V√≠ d·ª• workflow:**

```python
# Chu·∫©n b·ªã d·ªØ li·ªáu
X_train, X_test, y_train, y_test = ModelTrainer.prepare_data(df)

# Kh·ªüi t·∫°o
trainer = ModelTrainer(X_train, X_test, y_train, y_test)

# T·ªëi ∆∞u v√† train
best_params = trainer.optimize_xgb(n_trials=30)
trainer.train_xgb(**best_params)

# ƒê√°nh gi√°
trainer.summary()
trainer.plot_comparison()
trainer.save_all_models()
```

### 3. Optuna Optimization

D·ª± √°n s·ª≠ d·ª•ng **Optuna** ƒë·ªÉ t·ªëi ∆∞u hyperparameters:

- **TPESampler**: Tree-structured Parzen Estimator
- **MedianPruner**: D·ª´ng s·ªõm c√°c trial kh√¥ng hi·ªáu qu·∫£
- **Objective function**: Minimize RMSE

**Hyperparameters ƒë∆∞·ª£c t·ªëi ∆∞u:**

**Polynomial Regression:**

- degree: [2, 5]

**Random Forest:**

- n_estimators: [50, 300]
- max_depth: [5, 20]
- min_samples_split: [2, 10]
- min_samples_leaf: [1, 5]

**XGBoost:**

- max_depth: [3, 10]
- learning_rate: [0.01, 0.3]
- n_estimators: [50, 300]
- subsample: [0.5, 1.0]
- colsample_bytree: [0.5, 1.0]
- min_child_weight: [1, 5]
- lambda: [0.0, 1.0]
- alpha: [0.0, 1.0]

### 4. Metrics ƒë√°nh gi√°

| Metric   | √ù nghƒ©a                                                         | C√°ch t√≠nh                  |
| -------- | --------------------------------------------------------------- | -------------------------- |
| **RMSE** | Root Mean Squared Error - ƒê·ªô l·ªói trung b√¨nh                     | ‚àö(Œ£(y_true - y_pred)¬≤ / n) |
| **MAE**  | Mean Absolute Error - ƒê·ªô l·ªói tuy·ªát ƒë·ªëi trung b√¨nh               | Œ£\|y_true - y_pred\| / n   |
| **R¬≤**   | Coefficient of Determination - T·ª∑ l·ªá ph∆∞∆°ng sai ƒë∆∞·ª£c gi·∫£i th√≠ch | 1 - (SS_res / SS_tot)      |

**R¬≤ Score:**

- R¬≤ = 1.0: M√¥ h√¨nh ho√†n h·∫£o
- R¬≤ = 0.8-1.0: R·∫•t t·ªët
- R¬≤ = 0.6-0.8: T·ªët
- R¬≤ < 0.6: C·∫ßn c·∫£i thi·ªán

### 5. Feature Importance Analysis

D·ª± √°n ph√¢n t√≠ch **ƒë·∫∑c tr∆∞ng quan tr·ªçng** (Feature Importance) ƒë·ªÉ hi·ªÉu:

- Features n√†o ·∫£nh h∆∞·ªüng nhi·ªÅu nh·∫•t ƒë·∫øn gi√° taxi
- So s√°nh quan ƒëi·ªÉm c·ªßa Random Forest vs XGBoost
- Lo·∫°i b·ªè features kh√¥ng quan tr·ªçng ƒë·ªÉ t·ªëi ∆∞u m√¥ h√¨nh

**C√°ch s·ª≠ d·ª•ng:**

```python
# L·∫•y top 10 features quan tr·ªçng nh·∫•t
importance_df = trainer.get_feature_importance('xgboost', top_n=10)
print(importance_df)

# V·∫Ω bi·ªÉu ƒë·ªì
trainer.plot_feature_importance('random_forest', top_n=15)

# So s√°nh gi·ªØa c√°c m√¥ h√¨nh
trainer.compare_feature_importance(top_n=10)
```

**Output:**

- `results/feature_importance_random_forest.png` - Bi·ªÉu ƒë·ªì RF
- `results/feature_importance_xgboost.png` - Bi·ªÉu ƒë·ªì XGBoost
- `results/feature_importance_comparison.png` - So s√°nh c·∫£ 2 models

**V√≠ d·ª• k·∫øt qu·∫£:**

Top features th∆∞·ªùng quan tr·ªçng nh·∫•t:

1. `Trip_Distance_km` - Kho·∫£ng c√°ch
2. `Trip_Duration_Minutes` - Th·ªùi gian
3. `Per_Km_Rate` - Gi√° theo km
4. `Per_Minute_Rate` - Gi√° theo ph√∫t
5. `Base_Fare` - Gi√° kh·ªüi ƒëi·ªÉm

## üìä K·∫øt qu·∫£

### So s√°nh hi·ªáu su·∫•t c√°c m√¥ h√¨nh:

| Model                     | Train RMSE | Test RMSE | Test MAE | Test R¬≤   |
| ------------------------- | ---------- | --------- | -------- | --------- |
| **Polynomial Regression** | ~14.2      | ~15.9     | ~6.3     | ~0.79     |
| **Random Forest**         | ~3.5       | ~7.2      | ~5.0     | ~0.96     |
| **XGBoost** ‚≠ê            | ~2.1       | ~6.0      | ~3.4     | **~0.97** |

### M√¥ h√¨nh t·ªët nh·∫•t: **XGBoost**

- Test R¬≤: **0.971** (gi·∫£i th√≠ch 97.1% ph∆∞∆°ng sai)
- Test RMSE: **6.018** (sai l·ªách trung b√¨nh ~$6)
- Test MAE: **3.377** (sai l·ªách tuy·ªát ƒë·ªëi ~$3.38)

### Nh·∫≠n x√©t:

1. **XGBoost** cho k·∫øt qu·∫£ t·ªët nh·∫•t v·ªõi R¬≤ = 0.971
2. **Random Forest** c≈©ng r·∫•t t·ªët v·ªõi R¬≤ = 0.958
3. **Polynomial Regression** k√©m h∆°n ƒë√°ng k·ªÉ v·ªõi R¬≤ = 0.794

### Visualization:

Sau khi ch·∫°y pipeline, c√°c bi·ªÉu ƒë·ªì ƒë∆∞·ª£c l∆∞u trong `results/`:

**Bi·ªÉu ƒë·ªì so s√°nh:**

- `comparison_test_r2.png` - So s√°nh R¬≤ score
- `comparison_test_rmse.png` - So s√°nh RMSE
- `comparison_test_mae.png` - So s√°nh MAE

**Bi·ªÉu ƒë·ªì predictions:**

- `predictions_polynomial.png` - Actual vs Predicted (Polynomial)
- `predictions_random_forest.png` - Actual vs Predicted (Random Forest)
- `predictions_xgboost.png` - Actual vs Predicted (XGBoost)

**Bi·ªÉu ƒë·ªì Feature Importance:** ‚≠ê

- `feature_importance_random_forest.png` - Top features (RF)
- `feature_importance_xgboost.png` - Top features (XGBoost)
- `feature_importance_comparison.png` - So s√°nh RF vs XGBoost

**Log file:**

- `training.log` - Chi ti·∫øt qu√° tr√¨nh training, optimization, v√† ƒë√°nh gi√°

## üìù C·∫•u h√¨nh

Ch·ªânh s·ª≠a file `config.py` ƒë·ªÉ thay ƒë·ªïi:

```python
# Tham s·ªë training
TEST_SIZE = 0.2          # T·ª∑ l·ªá test set
RANDOM_SEED = 42         # Random seed

# Optuna optimization
OPTUNA_N_TRIALS = {
    'polynomial': 10,
    'random_forest': 20,
    'xgboost': 30
}

# X·ª≠ l√Ω missing values
MISSING_STRATEGY = {
    'numeric': 'median',
    'categorical': 'mode'
}

# Encoding
ENCODING_METHOD = 'onehot'
```

## üêõ Troubleshooting

### L·ªói import module:

```bash
# Th√™m project v√†o PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:$(pwd)"  # Linux/Mac
set PYTHONPATH=%PYTHONPATH%;%CD%          # Windows
```

### L·ªói download d·ªØ li·ªáu:

N·∫øu `gdown` kh√¥ng ho·∫°t ƒë·ªông, download th·ªß c√¥ng:

1. Truy c·∫≠p Google Drive
2. Download file `taxi_price.csv`
3. ƒê·∫∑t v√†o th∆∞ m·ª•c `data/`

### L·ªói thi·∫øu th∆∞ vi·ªán:

```bash
pip install -r requirements.txt --upgrade
```

## üë• Th√†nh vi√™n nh√≥m

(ƒêi·ªÅn t√™n v√† vai tr√≤ c√°c th√†nh vi√™n)

## üìö T√†i li·ªáu tham kh·∫£o

- [Scikit-learn Documentation](https://scikit-learn.org/)
- [XGBoost Documentation](https://xgboost.readthedocs.io/)
- [Optuna Documentation](https://optuna.readthedocs.io/)
- [Pandas Documentation](https://pandas.pydata.org/)

## üìÑ License

D·ª± √°n ƒë∆∞·ª£c t·∫°o cho m·ª•c ƒë√≠ch h·ªçc t·∫≠p - M√¥n Python cho Khoa h·ªçc D·ªØ li·ªáu K23.

---

**Ng√†y ho√†n th√†nh:** 26/11/2025
**L·ªõp:** Python cho Khoa h·ªçc D·ªØ li·ªáu - K23
